{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few-shot learning - Omniglot - Fellowship.ai\n",
    "This notebook is my personal project to the few-shot learning challenge from [Fellowship.ai](https://fellowship.ai/challenge/) with the following goal:\n",
    "> Omniglot, the “transpose” of MNIST, with 1623 character classes, each with 20 examples.  Build a few-shot classifier with a target of <35% error.\n",
    "\n",
    "I managed to reach an error rate of 6.5% on a 20-way classification corresponding to the current state of the art model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "\n",
    "\n",
    "### The Omniglot dataset\n",
    "*Dataset reference:* [Link](https://github.com/brendenlake/omniglot)\n",
    "> Lake, B. M., Salakhutdinov, R., and Tenenbaum, J. B. (2015). Human-level concept learning through probabilistic program induction. Science, 350(6266), 1332-1338.\n",
    "\n",
    "The Omniglot dataset is often considered as the transpose of the MNIST dataset. While the latter contains only 10 classes with a training set of 60000 examples, Omniglot contains an important number of classes (1623 different handwritten characters from 50 different alphabets) with only a low number of examples (20) for each, making it an ideal dataset for few-shot learning problems.\n",
    "\n",
    "### Few-shot learning\n",
    "Whereas, lots of deep learning projects are based on a huge number of training examples to be trained, few-shot learning is  based only on a few one. This approach is much closer to the one experienced by humans. We are able to memorize and recognize objects we have never seen before from a few number of examples. Then for each new encounter with these types of object we can classify them in an accurate and easy way.\n",
    "\n",
    "### Approach\n",
    "The approach I used for this challenge is essentially based on a triplet-loss model.\n",
    "For each image one embedding of size 64 was created using a ResNet-like architecture (current state of the art CNN architecture) combined to a triplet-loss  model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content\n",
    "* A/ Libraries and Images Path\n",
    "\n",
    "* B/ Exploratory Data Analysis\n",
    "\n",
    "* C/ Preprocessing\n",
    "\n",
    "* D/ Triplet loss Neural Network Architecture\n",
    "\n",
    "* E/ Training Procedure\n",
    "\n",
    "* F/ Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A/ Libraries and Images Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import useful libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, Activation, BatchNormalization, Flatten, Conv2D, concatenate,AveragePooling2D, Lambda, Add\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras import regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from skimage.transform import resize\n",
    "import h5py\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We indicate the PATH of the training and evaluation examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=\"images_background/\" #Training images path\n",
    "\n",
    "PATH_TEST = \"images_evaluation/\" #Validation and test images path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pictures are divided into two folders :\n",
    "The evaluation folder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B/ Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a look at the list of all alphabets contained in the training set, and their total number.\n",
    "### 0.Alphabets in Training and Evaluation set\n",
    "#### a) Traning set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of the different alphabets:\n",
      " ['Alphabet_of_the_Magi' 'Anglo-Saxon_Futhorc' 'Arcadian' 'Armenian'\n",
      " 'Asomtavruli_(Georgian)' 'Balinese' 'Bengali'\n",
      " 'Blackfoot_(Canadian_Aboriginal_Syllabics)' 'Braille' 'Burmese_(Myanmar)'\n",
      " 'Cyrillic' 'Early_Aramaic' 'Futurama' 'Grantha' 'Greek' 'Gujarati'\n",
      " 'Hebrew' 'Inuktitut_(Canadian_Aboriginal_Syllabics)'\n",
      " 'Japanese_(hiragana)' 'Japanese_(katakana)' 'Korean' 'Latin'\n",
      " 'Malay_(Jawi_-_Arabic)' 'Mkhedruli_(Georgian)' 'N_Ko'\n",
      " 'Ojibwe_(Canadian_Aboriginal_Syllabics)' 'Sanskrit' 'Syriac_(Estrangelo)'\n",
      " 'Tagalog' 'Tifinagh']\n",
      "\n",
      "Number of different alphabets: 30\n"
     ]
    }
   ],
   "source": [
    "alph_type = np.array(os.listdir(PATH)) #Give the different types of alphabet in our training data\n",
    "print(f\"List of the different alphabets:\\n {alph_type}\")\n",
    "print(f\"\\nNumber of different alphabets: {len(alph_type)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Evaluation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of the different alphabets:\n",
      " ['Angelic' 'Atemayar_Qelisayer' 'Atlantean' 'Aurek-Besh' 'Avesta' 'Ge_ez'\n",
      " 'Glagolitic' 'Gurmukhi' 'Kannada' 'Keble' 'Malayalam' 'Manipuri'\n",
      " 'Mongolian' 'Old_Church_Slavonic_(Cyrillic)' 'Oriya' 'Sylheti'\n",
      " 'Syriac_(Serto)' 'Tengwar' 'Tibetan' 'ULOG']\n",
      "\n",
      "Number of different alphabets: 20\n"
     ]
    }
   ],
   "source": [
    "alph_type_test = np.array(os.listdir(PATH_TEST)) #Give the different types of alphabet in our evaluation data\n",
    "print(f\"List of the different alphabets:\\n {alph_type_test}\")\n",
    "print(f\"\\nNumber of different alphabets: {len(alph_type_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Number of characters per alphabets\n",
    "We then check the number of character for each alphabets.\n",
    "#### a) Traning set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAE8CAYAAADQaEpSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJztnXfcJEXx/9+fC3AESXIgchyHcIiKJI98ioL4U4IoURQERNCvCRQDYkKCKAYkKUHCiUgGJSkgHFkO78hRchAEJJ4gKFC/P6rnntnZ2d3ZfdI9Q71fr3ntzkxNd0+q6a6urpaZEQRBEIx8Rg13AYIgCIKBIRR6EARBTQiFHgRBUBNCoQdBENSEUOhBEAQ1IRR6EARBTQiFXkMknSjpgGHKW5JOkPSspOu7PPZBSR8crLIFzUgySSsMtGzJsXFvh4BQ6ENAepifkLRAbttnJV0+jMUaLKYCGwMTzGyt4S5ML0i6XNJnh7scgRMfg+qEQh86xgB7DHchukXS6C4PWRZ40MxeHIzyVCG1Eobt2e7hmg1UvmOGI99g7iEU+tDxU+DrkhYp7pA0KTVnx+S2zaklStpZ0jWSDpH0nKT7Ja2Xtj8i6UlJOxWSXVzSJZJmS7pC0rK5tFdK+56RdLekbXP7TpT0a0kXSnoR+EBJed8q6dx0/L2SdkvbdwV+A6wr6d+Sflh2ISTtJunOVLY7JK2R272apFskPS/pNEnj0jGLSjpf0lPJnHO+pAmF63WgpGuAl4C3Sdoll8/9kj5XKMcWkm6S9IKk+yR9WNKBwHuBI9I5HNHLNZO0STq32ZL+IenrLa5Fdm8PT+d8l6SNcvsXlnScpMdTOgdkH4zCc/EMsG9J+mtJ+mt6bh6XdISkeVqU5URJR7V6bhIflHRPugdHSlI6dnlJl0l6WtK/JJ1c8qyvma7Js3Kz3Lhc3pule/GcpGslrZK2nwRMBM5L9+ObZWUPEmYWyyAvwIPAB4GzgQPSts8Cl6f/kwADxuSOuRz4bPq/M/AqsAswGjgAeBg4EpgX+BAwG1gwyZ+Y1t+X9h8KXJ32LQA8ktIaA6wB/At4V+7Y54H18Q/+uJLzuQL4FTAOWA14CtgoV9ar21yLbYB/AGsCAlYAls1dp+uBtwKLAXcCn0/73gxsBcwPvAk4A/hD4Xo9DLwrnddYYFNg+ZTPBriiXyPJr5XOc+N0nksDKxWvfa/XDHgceG/av2iWb8n1yO7tV1OZt0tpLZb2/wE4OpVhiXR9Plc49supXPOVpP8eYJ20f1K6pnvm9huwQqfnJid7PrAIrmSfAj6c9q2QruW8wHjgSuCXhXfgNmCZdG+voe9dWAN4Elgbf753SvLz5t+f4X6PR8Iy7AV4Iyz0KfSV08s6nu4V+j25fe9O8kvmtj0NrJb+nwicmtu3IPBaepm2A64qlO9o4Ae5Y3/b5lyWSWm9KbftIODEXFnbKfSLgD3aXKcdcusHA0e1kF0NeLZwvfbrcB/+kOWdzvmQFnJzrn1a7/qa4R+XzwELdSjTzsBjgHLbrgd2BJYEXiGnqIHtgem5Yx/u8lncEzgnt15U6KXPTU52am7/6cDeLfL5GHBj4d5+Pre+CXBf+v9rYP/C8XcDG+Tfn17evTfaEiaXIcTMbsNrOHv3cPgTuf//SekVty2YW38kl++/gWfwmu+ywNqpafucpOeATwFvKTu2hLcCz5jZ7Ny2h/AabhWWAe5rs/+fuf8vkc5J0vySjpb0kKQX8BrgImq0VzeUW9JHJF2XzCTP4Upk8YrlyNPLNdsq5fdQMl2s2yb9f1jSXImH6LtXY4HHc/kejdfUW+XbgKQVk3nqn+m6/Yi+a1BGq+cmo9X9WULSqcks9ALwu5J88mXNzpF0nnsVru8yhXyDCoRCH3p+AOxGowLMOhDnz23LK4teWCb7I2lBvJn7GP5SXWFmi+SWBc3s/3LHtgvB+RiwmKQ35bZNxM0oVXgEN4N0y17A24G1zWwh3CwAbk7JmFNuSfMCZwE/w1syiwAX5uTblaN4/l1fMzP7m5ltgSvfP+C12VYsndmiExPpu1evAIvn8l3IzN7VpqxFfg3cBUxO120fGq9ZkVbPTScOSmVZJeWzQ0k+y+T+Z+cIfp4HFq7v/GZ2StofIWErEgp9iDGze4HTgK/ktj2FK8QdJI2W9Bl6U3p5NpE0NXWA7Q/MMLNH8BbCipJ2lDQ2LWtKekfF8j8CXAscJGlc6rzaFTi5Yrl+g3cOv0fOCiUdb2W8CW+FPCdpMfzD2I55cHvuU8Crkj6C9zVkHAfsImkjSaMkLS1ppbTvCeBtOdmurpmkeSR9StLCZvY/4AXcdNGKJYCvpHS3Ad4BXGhmjwMXAz+XtFAq5/KSNuhw7nnelPL/dzq//+sg3+q5qZLPv/H7szTwjRKZL0qakO7fPvh7AHAs8HlJa6dnYgFJm+YqDcX7EbQgFPrwsB/eyZVnN/wleBrv2Lu2n3n8Hld6z+AdY58CSKaSDwGfwGtI/wR+giu/qmyP2/0fA87BbcmXVDnQzM4ADkzlm43XXhercOgvgfnwzsjrgD93yGc2/tE8HXgW+CRwbm7/9Xgn5yF4v8YVeNMfvDNw6+SNcViP12xH4MFkfvg8XmNtxQxgcjq3A4GtzezptO/T+MfpjnQeZwJLtTv3Al/Hz302rjhPay9e/txU4Id45+bzwAW4A0BZ2hcD96flAAAzm4k//0fg53gv3j+QcRDw3WSOKfUWChw1mu6CIBhKJO2Md8BOnQvKciLwqJl9d7jLEvRG1NCDIAhqQij0IAiCmhAmlyAIgpoQNfQgCIKaMKTBfBZffHGbNGnSUGYZBEEw4pk1a9a/zGx8J7khVeiTJk1i5syZQ5llEATBiEfSQ1XkwuQSBEFQE0KhB0EQ1IRQ6EEQBDUhFHoQBEFNCIUeBEFQE0KhB0EQ1IRQ6EEQBDUhFHoQBEFNCIUeBEFQE4Z0pGgQBEEdmbT3BW33P/jjTYekHFFDD4IgqAmh0IMgCGpCKPQgCIKaEAo9CIKgJoRCD4IgqAmh0IMgCGpCKPQgCIKaEAo9CIKgJoRCD4IgqAmh0IMgCGpCDP0PgrmAuWXoeDCyiRp6EARBTQiFHgRBUBNCoQdBENSESjZ0SQ8Cs4HXgFfNbIqkxYDTgEnAg8C2Zvbs4BQzCIIg6EQ3NfQPmNlqZjYlre8NXGpmk4FL03oQBEEwTPTH5LIFMC39nwZ8rP/FCYIgCHqlqkI34GJJsyTtnrYtaWaPA6TfJQajgEEQBEE1qvqhr29mj0laArhE0l1VM0gfgN0BJk6c2EMRgyAIgipUqqGb2WPp90ngHGAt4AlJSwGk3ydbHHuMmU0xsynjx48fmFIHQRAETXRU6JIWkPSm7D/wIeA24FxgpyS2E/DHwSpkEARB0JkqJpclgXMkZfK/N7M/S/obcLqkXYGHgW0Gr5hBEARBJzoqdDO7H1i1ZPvTwEaDUaggCIKge2KkaBAEQU0IhR4EQVATInxuECQihG0w0okaehAEQU0IhR4EQVATQqEHQRDUhLChB8EIIuz8QTuihh4EQVATQqEHQRDUhFDoQRAENSFs6HMxYS8NgqAbooYeBEFQE0KhB0EQ1IRQ6EEQBDUhFHoQBEFNCIUeBEFQE0KhB0EQ1IRQ6EEQBDUhFHoQBEFNCIUeBEFQE0KhB0EQ1IQY+h+MWCI0QhA0EjX0IAiCmhAKPQiCoCaEQg+CIKgJYUMPgqA2vNH7VaKGHgRBUBNCoQdBENSEMLlU4I3ejAuCboj3ZfioXEOXNFrSjZLOT+vLSZoh6R5Jp0maZ/CKGQRBEHSiG5PLHsCdufWfAIeY2WTgWWDXgSxYEARB0B2VFLqkCcCmwG/SuoANgTOTyDTgY4NRwCAIgqAaVW3ovwS+Cbwprb8ZeM7MXk3rjwJLlx0oaXdgd4CJEyf2XtIRQNgOg7mFwXgW4/me++lYQ5e0GfCkmc3Kby4RtbLjzewYM5tiZlPGjx/fYzGDIAiCTlSpoa8PfFTSJsA4YCG8xr6IpDGplj4BeGzwihkEQRB0omMN3cy+bWYTzGwS8AngMjP7FDAd2DqJ7QT8cdBKGQRBEHSkPwOLvgV8TdK9uE39uIEpUhAEQdALXQ0sMrPLgcvT//uBtQa+SEEQBEEvxND/IAiCmhAKPQiCoCaEQg+CIKgJodCDIAhqQij0IAiCmhAKPQiCoCaEQg+CIKgJodCDIAhqQij0IAiCmhAKPQiCoCaEQg+CIKgJodCDIAhqQij0IAiCmhAKPQiCoCaEQg+CIKgJodCDIAhqQij0IAiCmtDVjEXBG4dJe1/Qdv+DP950iEoSBEFVooYeBEFQE0KhB0EQ1IRQ6EEQBDUhbOhBvwhbe9Ar8ewMPFFDD4IgqAmh0IMgCGpCKPQgCIKaEAo9CIKgJoRCD4IgqAmh0IMgCGpCKPQgCIKa0FGhSxon6XpJN0u6XdIP0/blJM2QdI+k0yTNM/jFDYIgCFpRpYb+CrChma0KrAZ8WNI6wE+AQ8xsMvAssOvgFTMIgiDoREeFbs6/0+rYtBiwIXBm2j4N+NiglDAIgiCoRCUbuqTRkm4CngQuAe4DnjOzV5PIo8DSLY7dXdJMSTOfeuqpgShzEARBUEIlhW5mr5nZasAEYC3gHWViLY49xsymmNmU8ePH917SIAiCoC1debmY2XPA5cA6wCKSsuBeE4DHBrZoQRAEQTdU8XIZL2mR9H8+4IPAncB0YOskthPwx8EqZBAEQdCZKuFzlwKmSRqNfwBON7PzJd0BnCrpAOBG4LhBLGdlIiRnEARvVDoqdDO7BVi9ZPv9uD09CIIgmAuIkaJBEAQ1IRR6EARBTQiFHgRBUBNCoQdBENSEUOhBEAQ1IRR6EARBTQiFHgRBUBNCoQdBENSEUOhBEAQ1ocrQ/1rSKUQA1DNMwEgIjTASylgn4nrXh6ihB0EQ1IRQ6EEQBDUhFHoQBEFNCIUeBEFQE0KhB0EQ1IRQ6EEQBDXhDeu2OJyEm9jQEtc7eKMQNfQgCIKaEAo9CIKgJoRCD4IgqAlhQw+C4A1HXftVooYeBEFQE0KhB0EQ1IRQ6EEQBDUhbOjBkFCncMV1OpegXkQNPQiCoCaEQg+CIKgJodCDIAhqQkeFLmkZSdMl3Snpdkl7pO2LSbpE0j3pd9HBL24QBEHQiio19FeBvczsHcA6wBclvRPYG7jUzCYDl6b1IAiCYJjoqNDN7HEzuyH9nw3cCSwNbAFMS2LTgI8NViGDIAiCznTltihpErA6MANY0sweB1f6kpZocczuwO4AEydO7LmgdR2qGwRBMFBU7hSVtCBwFrCnmb1Q9TgzO8bMppjZlPHjx/dSxiAIgqAClRS6pLG4Mj/ZzM5Om5+QtFTavxTw5OAUMQiCIKhCFS8XAccBd5rZL3K7zgV2Sv93Av448MULgiAIqlLFhr4+sCNwq6Sb0rZ9gB8Dp0vaFXgY2GZwihgEQRBUoaNCN7OrAbXYvdHAFicIgiDolRgpGgRBUBNCoQdBENSEUOhBEAQ1IRR6EARBTQiFHgRBUBNCoQdBENSEmIIuCIKgBSMthlTU0IMgCGpCKPQgCIKaEAo9CIKgJoQNPQgGkZFmgw1GNlFDD4IgqAmh0IMgCGpCKPQgCIKaEAo9CIKgJoRCD4IgqAmh0IMgCGpCuC3WhHCPC+pKp2cb4vnOiBp6EARBTQiFHgRBUBNCoQdBENSEUOhBEAQ1IRR6EARBTQiFHgRBUBNCoQdBENSEUOhBEAQ1IRR6EARBTQiFHgRBUBNCoQdBENSEjgpd0vGSnpR0W27bYpIukXRP+l10cIsZBEEQdKJKDf1E4MOFbXsDl5rZZODStB4EQRAMIx0VupldCTxT2LwFMC39nwZ8bIDLFQRBEHRJrzb0Jc3scYD0u0QrQUm7S5opaeZTTz3VY3ZBEARBJwa9U9TMjjGzKWY2Zfz48YOdXRAEwRuWXhX6E5KWAki/Tw5ckYIgCIJe6FWhnwvslP7vBPxxYIoTBEEQ9EoVt8VTgL8Cb5f0qKRdgR8DG0u6B9g4rQdBEATDSMc5Rc1s+xa7NhrgsgRBEAT9IEaKBkEQ1IRQ6EEQBDUhFHoQBEFNCIUeBEFQE0KhB0EQ1IRQ6EEQBDUhFHoQBEFNCIUeBEFQE0KhB0EQ1IRQ6EEQBDUhFHoQBEFNCIUeBEFQE0KhB0EQ1IRQ6EEQBDUhFHoQBEFNCIUeBEFQE0KhB0EQ1IRQ6EEQBDUhFHoQBEFNCIUeBEFQE0KhB0EQ1IRQ6EEQBDUhFHoQBEFNCIUeBEFQE0KhB0EQ1IRQ6EEQBDUhFHoQBEFNCIUeBEFQE0KhB0EQ1IR+KXRJH5Z0t6R7Je09UIUKgiAIuqdnhS5pNHAk8BHgncD2kt45UAULgiAIuqM/NfS1gHvN7H4z+y9wKrDFwBQrCIIg6BaZWW8HSlsDHzazz6b1HYG1zexLBbndgd3T6tuBu3svbgOLA/8aQLnBSHO45IYz7zfiuQxn3nEuc2fe3ZSxCsua2fiOUmbW0wJsA/wmt74jcHiv6fWQ/8yBlBuMNIdLbiSUsU7nMhLKGOcyd8oN9NIfk8ujwDK59QnAY/1ILwiCIOgH/VHofwMmS1pO0jzAJ4BzB6ZYQRAEQbeM6fVAM3tV0peAi4DRwPFmdvuAlawzxwyw3GCkOVxyw5n3G/FchjPvOJe5M+9uyjhg9NwpGgRBEMxdxEjRIAiCmhAKPQiCoCaEQg+CIKgJI06hS5pH0sppGTvc5ekvkhYYwrwOH6q8gmpImiRp4zb7JWmCpEkd0pGkCQNdvpJ83jZA6Xx7INIpSXebKtu6SO9ZSc8UlgcknZG/J5JWlHSspIslXZYtvebbc3lHUqeopPcD04AHAeF+8DuZ2ZU5mYWBfYH3pk1XAPuZ2fMl6c0LbAVMIufxY2b7FeTGA7uVyH0m7V+sXbnN7JmSvNcDfgMsaGYTJa0KfM7MvpD2/9LM9pR0HtB0k8zso+3yLEPSDWa2Rvq/LDDZzP4iaT5gjJnNzslWPidJywGPm9nLaX0+YEkze7BFORbF713+Wt5QIrcp8C5gXE5uv9z+ycBBeCyhvEyT0pH0ETP7U2Hb583sqPR/JTO7S9IaLc63rHxXAVcCVwHX5K9fVSSdAXwQ2AOYBTyVzmUF4APARsAP8NHWo4A/tpMzs0sK6a9H83P725JyrI+/N8smWblo47WUdCWwNO62fCVwlZnd2sN5z3kW0/qq9L2zV5nZzd2mWZZuq21p+/rATWb2oqQdgDWAQ83soZzMfsATwO/xa/IJYDxwL/BZM/tAkrsZOAq/N69lx5vZrF7Oo1dGmkKfBXzSzO5O6ysCp5jZe3IyZwG34YoffATrqma2ZUl6fwaep/km/Lwgdy3+0hblzkr7H8CVrkqK3fRSpGNmAFsD55rZ6mnbbWa2cvr/HjObJWmDsmthZleUbW9H9mBL2g1XEIuZ2fJJMR5lZhvlZF/HB4+9mm1qdU6SZgLrmcf0IY1LuMbM1iwpw/7AzsB99H2ozMw2LMgdBcyPK6vf4NfqejPbNSdzNa7sDgE2B3bBn+kflOR7LfBdM7ssrX8LeL+ZfSStH2Nmu0uaXnLpmsqXjnkbMBVXROsAr+DK6KslabRE0p3A2cD6wFLAS8CdwIXAmbkP5TuBT3WSy6V7ErA8cBN9z62Z2VdKynAX8FWan/GnS2TnAdYE3g98Dq+UtK0AlKRxY+653wOvMJ2ddn8cOMbMDs/JrwMcDrwDmAd3lX7RzBZK+z8CbAJsC5yWy2oh4J1mtlZJGW4BVgVWAU4CjgO2NLMNcjLXmdk6heOuM7N1JN1sZqumbbPyemjYGI7hqb0uwC2dtuFf3KJM07a0/baK+ZYe389zmZF+b8xtu3mQr98N2fngL0U+71sLsocCNwO/whWWurk+rc4Fj+UzT9V7nftdELi4IDOrWHZcoZaltzhwXTqXA4GzgLElcuOqbMvtWwqvtR0J3AH8udf70kFm44ppbZz7f2e7+1b2PFaQmwp8G/+IXJuej+37c87ALcACufUFSt7rmXhr5EZcme8CHJjbvyqwE/BQ+s2WLYFF25UB+D6wa9m9SM/Mlrn1LfPvLrBYWvYFvpCeh2zbYt1el/4uPQ8sGiZmSjoO/5qC11aKTZr/SJpqZlfDnGbVf1qkd62kd1vnJuP5kjYxswvbCUl6X9l2y5mEcjySmsOWajxfwV/ALK1bKTG15NJcpUOZS4uYfl8xs/9KyvIaU8zLzPaQC7yfFKdH0sXAr83sgUK6T0n6qJmdm9LbgtaBiW4DFgGe7FDW7J69JOmtwNPAcgWZlyWNAu6RD3L7B7BEWWJm9i9JHwX+gj8zW1t6Kwtcize9O21D0n34ef4er9192cxe73BevfIT4JKOUo1ytwFvAR6vcNx0ST/Fa8mvZBut2dR0Ba5cDwIutNQq6wEV/r+WW3+tsD8ry72SRpvZa8AJqdWV7bsZuFnSyWb2avHYFsxOtvwdgPfJQ4IX++V2wJ/936T1GcCOkuYH9sSfpXzr/Bv5IgMD0udQlZGm0P8P+CKu/ITb8H5VIjMt2dIBnsWb+GVMBXZOJpNX6LMbFpXlHsA+kl4B/peTW6ggl7+Z4/AQw7OApuY68Hm8Frw0btq4OJ1bxmYtytwSSduY2Rltth2afq+QtA8wn7xD7gvAecX0ksKbLulGvBa6P3APcGzJuZws6Qj82jwCfLpFMQ8CbpR0G42Ko9gncL6kRYCfAjfgL8dvCjJ74maZr6SybYjXyvLnP5vGj9U8+Eu2taQ591DSW/B7MZ+k1el7QRdKeZRxGP4MbQ+sjl/XK83svkIZ5jWzV9pse7BF+g2HVJApyi0O3CHpetpfa4C10++U3Daj+dl9M27yeR/wlWSa+6uZfa9i+TLyz+kJwAxJ56T1jwHHF+RfShWfmyQdjH+k5jgUSDrdzLbFn62yPqeyCtB2wCfx2vk/JU3En7f8cfficz6UcQXNlYxhZUTZ0Dsh6Swz2yr9XwjAzF5oI79s2XbLdYr0szzLAAeb2fYDkV6F/Cp1CKVa7a7Ah3AFcBEeOdNyMgvg8e23wzuBzgZOM7NH2uS/IP5MtewclHQ7cDRwKzCnNmtt+gTkndfjrKRje6CQtBP+4Z+C10AzZgMnmtnZZcelYxfETQBfByaY2ejC/soddW3yqCSvxo7vAet/KeTxDmAD3Hy1HvCwJbuzpG+a2cFyj6oyxdpkv0/HrYF/HLOK2u352n96V5/AP8hfBRYGfpUULpKWMrPHq7zTSh3g6X/Dx1bSOmZ2XW79rXhFaGradCXwVTNrCEQoqamPDu+fu9XMOrVGB4y6KfQb8Wbml7KXP93g4y3X4Vdy3BI0ekk8XCKzKDC5IFdmSskfI9wW+O6SfQcDB+CmhT/jNsA9zex3Bbm2nUFJpusOoU5IehGvjZ+C9+gXTTJnS9rBzH4n6WtlaZjZL0rSvcJynU5t8h8NbEqzh8YvcjIr4q2iZQsyZR2Ywk10y5nZ/ulju5SZXV+Q28pSZ3eFMv4cf9EXBP6Kd5xfZWb3p/1Zrf93eE0wX+s/ysxWqpJPSqtrhV4x3a7uYTIz3Q1cjZ/vjILi3dzMzksfyLL0puVkv2dm+5eUaWHgj2b2frmH2Xgzu6MgszLwhJk9lds2GrjIzD7Y4ZzzH72it01x/SLgTCDzDNoR2MbM/l8hzQuAdYGsU/39uP19RdzL7iSGgJFmcumE4Q/ajPSALo2/8HuVCSeb6s+Bt+I23WVxO/a7CnKfxc0uE/AOxXXwF7jomZGvlYwCVsM7Fsv4kJl9U9LHcZPLNvjD8LuC3BG4ueMMvPb4abxzKM9jeK3yozT2KczGazNZ+bqxy5+RZFdKS4MoXmPPmrxvapVmCbMkHYRH5mxnqz0PeJlCTb7AGbir2LE02mDL+FVKZ0PcPPNvvCOzwRPHzM5SB3fJHNfhLbAnWuT5//Ba/wQgrxhnA/t0KG+RB7uVq1IZoP09LHtWJrfrJzCz89LvtFYyOd4r6UAz+06uzG/BW4xZi+hw4Nclxy6NX8NP5vJ+TdJLkhbu0Jor2u9b7QN3v82bGH+T+muKvA68I3sWJC2Zyr02XqsfEoU+pD2wg73Q12s9Fbd1Pw68pY38zbhN8Ma0/gHcXaoodyv+ct+U1lfCzQ9FuXzv+qeA9dvkfXv6PRaf+QlKPENIgfLJ9foD17ZIcyz+4q6clrGF/cu2W4boHk0vWS4rkWvyaCqRmdXDs9HWqwj/QPwW7wf4Qbr3x7VIcxTeafa9tD4RWKtEbqsK5Zsf+B5wbFqfDGzWq1z27NDsGfKjFrJNz2qLbePwvp5f4Xbu4/EWcLb/PPxjXbqUpHU+8IvcudyLj8doeE9alLnJSw04HXgY76Q+LFvKnoXi/xbrl+EVKqVluxbPa9FLTFn58s/cYC91q6FLPhXe9/Ca7CrAhZJ2sfKBCv8zs6cljZI0ysymS/pJidzLZvaypMzmdpektxcyHo27jO1QsaznyX1//wN8ITUtXy6Ra9sZVGA9XBk9iD9Qy0jayZJpyLroG5APtPi9taiNSVoe+DFtPCisxF5qaSBGBf4k6UNmdnFJ3pnP83mSvgCcQ2Ntv2kgF/C/dI8spTGe8pr/ema2iqRbzOyHyazSyn5+JI21/tm4O2TR/35lSe8qbMMaa/0n4K2rddP6o3gL5PzCYVXlsjxaeoYUOJxmT56ybScBd+Gtj/3wisuduf0/a5F+WdleTi3UUyWdip/TnmZ2Tk6s3Wjwsn0XpKUdEyQdhr8j2X/S+tIF2c/gH68j8WfnOrz/qchVks6nr7N3K+DK1Bf1XIfyDBgjSqFL2sPMDm2z7Vu4x8ZU846IU1LP+TTc/FHkudShdSXupfEkfQNp8jwq97j4A3CJpGcpzM5k3twbL2keq+DKZWZ7p4/HC+nYlyifZHtHvHb1Jdx8sgz+sJTxC9yU0zDwCnhPWi96fGSUee28GfcYmEXzyMQNcHe9GXhHVVdUNGlcB5yTOnCLnkW9uIodhiv+JSUdiA9U+m6JXPZRbecumbG2+UCtG9M5PJs+vkX+nfs/DvdgurMgs7yZbSdp+5TWf5Ldv0g2ftPoAAAgAElEQVRVuewc2lYGJK2LVwTGF+zoC+HPXZEVzGwbSVuY2TRJv8dNJKTyVO5wzeV3PfBN3Ca/XLbd3H5/j0pchlO/0f0lyd5mhdGZkjYvyOSfl5mFfQ3r5qOdN+l8NnwRfy/Xx5/L3wJnmVfRq1Zi+s2IUui4KePQwrads22pNtdQozOz6yW16hTcAn+Bv4rXNBbGax0NmNnH09995SMJF8Y7Mos8CFwj6VzgxdzxZZ2D8+MPwUR81OZb8Um0G2pauVr1f4AftjiPjLGZMk/H/l25eDdmVtnWbWaHyt0QN8Qf0lVSGe4EdrSSjuMqqMUI0BLRn+M1tlvTS5Ev23IprXHWPDpyHCWY2cnp47QR/sJ9zMyKShW81l90lyy6aWZUqvVb88jjn9E8u9d/5SETsrSWJ9fq6EEOvDIwivaVgXnwTt0xNNrRX8DvTZH/pd/nUsfkP/GO6wbUN3q6AWscNZ3P77CSbaRyny9pW/r6h6bgz0aZa++xqVV6ayrH9rh76xy3XKtg35d0SFn5c2l8rbBueOfpmZ3SHkxGhJdLuimfxG3jV+V2vQl4zXK92umF3pXmGuBnesh3ITN7QS3imhSb9pJ+0EKuSRFLOg1/QD9tZiunl/SvZrZaQa6sI/N5vCZxgOWGZks6PsnmB16NMbNdejmfTsjjzCyP2z3L0mvyd06mjFVyvwsCZ5vZhwpyFwEfaWXySTJduQNKmop36p2QlO+ClhsklVoD65jZtWm9rbukpE/hNtU18Fbg1nh4gTPK5HPHLYqHMZic27Yx3mJ4J14pWR/Y2cwuLxxbSS4nPx8wMf+hbyG3bBWTnNxB4Cz8A38C/jH4vqWYODm5N+dWx+Gd/ouZ2fc75VGS53fxQWMrp0234+bAJhOlPBzDmfizPxU3vW6Wv4eSjsHt6reVHL8Afk/XwVugpZjZcYXjqnRADzojRaEvizd7DwL2zu2ajXeevZqTPQO38X2SnI3PzPbIyVxtZlNLTBANpgdJ55vZZiqP1WLWHLhodTO7seI5zTSzKWqMaTEnNkRO7mDcg+P3aVPWQfM8blraPCc7L17rz/vz/sqSn22351PhHDbAa6+7le0va35Lut7M1pJ0HT6M+mm8mTy5IHcibjr5E4328V+oB3fA9LGdArzdzFZM5pQzzGz9gtxfzWzd4vGtkLQSfbX+S8tq/YWP8mjcr39/y8UqSXJvxhWJgOvMrHS0bRdym+M27XnMbDlJq+EudGUf2vG42aNYESobFNcT2XvX5THCO79bumJKPkIst74ibh59BG+J/acgvxruIfNu3M05MydOxp+h4/HnqFXLp6wMMynxRrOcB89QMCJMLqnm8BCwrpqjBM6HK/aMtja+lN7U9NvWBGFmm6XfqqPBfiFpKfymnmrt51it2nRev6B0bpV0jZmtL++4zJf3lWQmuSSle7eZ/S+3v9vzaYuZXSHpxW7splQ3aTyQlnnSkqcXd8CP46M5b0hlf0xS2f2/WNJWeKuhZW0n1eZvMQ+mdlcruUTeNPAq7j/d0Fejvsh/F6T7uo+khsh/3cgl9sVHK1+ezvkmtQ7DezI+hmEzfOTvTriiy/It9VNPvIIHW7s4a1GpMWrlKFzJdePemjEdt+9PzJv55H0DU1M5p0vai8bK2WL4x3OGJCznkmtmNwHbptbhFDz+yn/wil9DS0bS4rjbc/FD19CiTNuqdkAPGiNCoWcoFyUQb+pPwN3M8oOGOtr4WpkcMspMD5KWpnkAy5WF4z6Qao/bAsfIR6ueZmYHlGTzA9wOv4ykk0lN5xK5BSWtbWYzUjnWwpu5UOjAVUl4YeW8XAqyXQ+UKknjdMBamIUaXqIkPwqvxT4HnCX3Cig1aZSZqXL7puHhHSoPAgL+a2amNCxcrePQfw3vOHxV0suUdxhjZq9LurmoaFpwgJntmN8g6aTCtl8Dq8rDyH4DryX+Fu+Apgc5gFfN7Hm17DNt4M1mdpzcyeAKPIxB/kPdThkvir+Dn8GfffA+kDnlwD/O29I9Hwb+jjs4LId7jMyHfyQuBg5JH6rpbdIoxcz+TfrYlSGP3LoA3pn+cbz1uxOuU4p04402eNgQ+UcOxEK1KIGfxR+wDfBe8CfJ+bUmmQfSvgdwc8a/8Kb/a8ADJfn+BFeSF+KdK+dR8KktOebduC37v21k3oyPhtwMWLyFzJq4L/QDqQy34LWuBYBtC7KzcJNCtr4iJb7a6Rrdise5mY7XTpp8ayvcj6VwH+fKfu14P0GVtMfjtfgLcV/gy4plBObFTS774BHzvo/bc8vS+zoecuB+3ET0VzyYVn+ex8vwVsGltPC1TnJF3+YxwB1lMrSJ/NeNXNp+XLo+t+Af78NxU0KZ7HXp96L0TK4O3Nfl9cgiY44CtuvPtS2km40TGZueuUVKZBZNv8sD86b/78fj/DTJV82Xvoie2bkJuLxEdlm8crQQXln7BW4tGJBrULnMQ51hP29sQ8jZ9GJ0HIDSJr2jgE1y6x8Bfl4id3f2kHRI7x14M/c2PHDP/wFLtJFfGncZe1+2tJFduNODWXYtWmyrNFCq4jX8DvCXLuR/iHtatA3rite+dsW9ajbAa6I/Kcj8GTcTfBNvFu8F7NUmzY3xj8TPaBOOFq8QrNXpvqRyNS25/d/GFf6ruNfIC2n9aeCgQlpXJPl78AiJoylUVrqRS7Lz46GC/5aWA2gRChivVCyMdzxOxysHm+f2f5cWYWjT/g0L8lf28jy1SHufCjL50NBjcPfa+/BY+Rf2mO8N9H3oLsZNfe+myw/dUC4jolM0IzVlnsM7HL6M+5zfYWbfSR1At1iyJUr6Pq44HgL2sOaQr6gkKH3WWVnY9ic8fsO/aUPq6DsF72x7rIPsT/De9Nvpc3UzK3RYqeKsSkm2rZdLTu5vZrampJtwX+pXJN1kBQ+bJLsi3sxf0twbZxXgo5YzI8ndNHe0CsGzUkf0AriSa2nSyO6NkjdM2tYQB0a5CUE65FkpxkeSLQ3zYC06B5OJbS38uv/NzJqa45IOMrO2U66ldD6Z0rhKHvnv/VaYXagLudHAj80s73PdFZL2NLNfpv9b4B/Ol3FFl+9IXA0PS/wjS7FVJH0Pb/mdRqMLb36mq3YeL2YlcV7alPVGM1tdfZO4fBP4j5kdrpzjQTdIugGvoF2B18CPxGvgP7RCsDY1z/iUncSQhs8daQq9ZZRA+ewj65jZS5I2w5s8WVjTpmA6Kb2LcDfI3+Ev5A54bawYeOcsPHjWpTR6XJRGjqt4LncDq1iHnnRVnFUpybb1csnJnYMPA98Tr1k9i/uwNw2gSHbUbwBHW8nMSmn9dFzxXULjy9uf65PNCnMR7qP8GD4rz/I5mWOAw63CFGhVPzqpP2BNvGa2mtyL5Ydmtl2J7Gdx08dl+PXeAPciOT7t72paOzV2+M8PjLaSyJVdyF3W6kNUBUkPm9nEwrbJ9M2YlI1LuNKaPUmaKlAUPKlSR2aR+XGT4JvNbMGS/a3KminyGcAv8Zbj5mb2QNUPf0malT8E6mLGp8FkRHWKmndETcP9QzMvDuvbbS+l/1vi8Tdm4cGgvtAiye1xe1c21PjKtK1IZh9ti7qY4xK35Y6l9aCQjAlm9uEO+V5qHk1yPzP7Fo2eH01Y9YFSAPObD87KbyuOpq0y3Lqs3Mvjrl7bl7xwB8ij7u2F234XIhdoLFE1nj2kQF+SOn10OoZ5yPENYPXspZW7E15LXyzvr+Gd+E0fX2iMNV7S4b80zR3+leUSN6YP2RmFc24ZCrhA2SQT9+DmnvIDpMPN7MtWwZMqXymRexztgXesnkr5NavCLriXzoFJmS9Hc8C7LM8F8Fp85pkzCjdJZXrkW5LK3qXn8RhL+Wf+eSvMWTscjCiFLh8yfhRuGxOwnKTPpQspuRvSS/jDnZ/4otXowWfwh6gt5u6PVQZonEDfHJcfIM1x2UL2JbxHvFOtv8qsSkvJfcI/Ko+J0ZBnviaoRnc7rLPL4b+S4s28Q7amOX7L07idsuNsPXK3zk/gH85V8A/g9gWZ0XgN9Hz85Wk1dLrVxANl5D86WSWg7N50DPOQl6XRZXY27vvsmZjtnpXTOo9o/SJuupmRjr1HHta5SFU5cKX/NI1RQY3WsWmK9NJ837Sdx0mJqWIx/MP3KdxDaw0ze7aHfJXSvwPvCM3yewCPOVTGpfgE3ZkpdX7cVr5eOvZiSdvgFbRsBOiWeB/ZFyRtaGZZK6PqjE+Di80FhvyqC+7vu0JufXngrvT/M/iIxRvIzeuIm1wubZFeR0+KJLc53jH6QFpfjXJvhm7muNypbCmRuwP4b8r/FrxDszjf4tb4AJzZVItkeDL+capyzd+G20dfwkfrXQ1MKsj8Dv/IHoyHEC1LZ7d0ff+Od86tQolHUU5+ehfPxRJ4CIWJxfPCwzt8Mbd+PX1eTtt0SHcDPCTxPIXtX0vLb3FPiH3xD/kNlHiR0MZbJbdeqcO/qlwX1242fR22+WU27vbYbXpP4xWbC3BT3llpeQb37c/L/jQ9N9/CR+12SnsqsEv6Px5YLrdvsfS7Pm76+zt9nmz3t0iv4/zD6Zkdm1sfi38IxpCLBFny3pW+e4O9jKgaOvCkpRlKEplbImZ2fLK3LkFjDPJ/4jXlMtoOpsixL80DNMqalN3McVklXjRUqIWa2ZnAmSqZMEAqdUJeCrhdPjVZvineNILQfLKGD6bm6SgrsdWa2Q5yn/vt8QEVhr/Up+Tkj8RdBT9pZjNT2drVAK+VD5IqdqrlWxtV4tl/E28RZMyDBytbMJXxjJRWWViErFW0gKTXzAeMQJ9P9n1pyfhj/gTU3bR2V6jCtIBdyCFpAm6uWh/mzBWwh5k9mslYF/F9KvKQme0iH2PwTjN7PJVlKfwZyLMXXpv9LvCd3KPa1FGu3Ehf/L6NxSsS66fzyDpbj6PElt2CFyWtkT1Tkt5D8/zDS+N+79n4lvmApc3sVfmUlKT8hywAVztGhEJX3/ROt0u6EI95bHh8iL9lcmb2D1yJZsfta2b7tkm602CKjLIBGmXKqDjH5QcozK2pNPehKg7GMbOHVBKDpMX5NIQTTR+Xk/DmbJ5OQb7yaeyBv0Cz8cBHawB7WyGsbVKGZ+EP/J74QIxvSDrMfIj7W/H79Qt58P/Ti+UtsF76zXvzNNid8Wu8Du42ubqkD9DcBzKPNU6bd3V6+Z9R4+Ci3+Mf9mIkx4wFJR1rZvtYYdCTpAXM7EWayY9o/XkuzRdoHtG6N97hfyvwObzVWJxDtRs58Pv2e/y6g3f6n4C7bw4W2TlOypR54gl8XMQczGxUF+lWHenbjS17T+AMSZlJbSnc8yzPL+gzjQr3bf9penYuVw+zdg0qQ90k6GXBH8JWy/FtjisdcJHbX2kwBRUHaFDShC9uw6c9g4qDcfCm/HnA39P6W4FrWpzPicC30/958Y7cfTtcg8Vp4xNOmgQCV07n4t4+RXPB5njH8i14R+ESafv8eI2tmOYEfKDPLLxGXTrpQoXnIpv842a89QAe9Covc2+b4yv7E+P+3ncWtq2Lm8QeTuur4l5FxWN3KNm22EC9H23K3NGk0EOaK3fYv3P6PSK9VzvjLd8/4R5JveZ7ffrN/M0XoNwk9WPclLMuHjRtDdwu3yrdsbjv/bspTAhTeF63wk2byxT27Z5+f1C2DPY9birrUGc4pCfXYaYQygdTfLRELj9AY2b63zRAo6joWm3r8hxuwmsG+dGxpTbTJPd7fODJxfhktvn96+Bmo7Pxj9dtuEnqSdKsSSVpZiPkDgU+nr+u+OCNqbgt+X25Y96Lx+0G2KjD+b09/+CTBvyQG/lJi1GguG1/QfwDe0oq47UFmZOB3Ury/RxuEipuf1/Z0qLsM/CQtPl7UzaLzgX4eIBs/S0URvBS0fZbVS53fXbAP0aj0//S/qQunser8X6IL9B5oNvHcQeBQ7Jnpx/5VhrpS5e2bPzd3xZvSX8aj35alFkY/zCsly25fT9Jv237Y4ZqGWl+6F2FxpUao7ANYrkqT9Ks7iaZQH3RCTM/2wXwgS6r5GTyfs5j8Qf/GrxlgfXZCGfiTf2FgWNw74vr5L7Wp1iJz62kE3A74nJ4DXQ0PvT5PclOuo+Z3VI4ZgqupIsTC3Qkd555H+U5k0Lk73XmdoYPNc/i2Z9sjSGFl8A9Vl4hNddxG/q8eCS+hgk65CGB8/muhSvfsomnZ5jZ2uocMXM3vAW4Ff4BOBf4uuXMVqrox1xVLslOxGvK6+LP3LXAV6zHWPa5dCfjTgjb4Mr9BDO7pERuWSr4y3eR78bkxqCU5dllej/ATSjvxE1XH8FNclvnZD6D2/qXxs1c2RiF96f9t+LKfoZ1MTn3oDHcX5RuFrwDa3+8I2onvBZ6aEFmAt78fwq3252F+3KXpTeNXC0DH/LdZMLBO2POxhXCLdmS279qKs9DNHqtbEmb4dIVz7ljzYTyWklT7YRcc5tm80FpawZXlmtk1wmPP7NK+t9UG80dVzocvcL5tirHvPhLnK2PpruQAxvio4u/DGzYxXHLUFKTT/vOxGtsN+CdrV/Ho2yWyX4RN53dSq6Gl9s/o2J5Osq1et7Tvs2r5FMhj9H4B+ofuNnsLmDL3P7d8BbtfWl9Mv1oHeAfkMkVZTfFO8M7xfe5NT3fmVlxSeC8Epn56AuT8a7884Cbd56nMbxD5iX0wkBc666u01Bn2M+HKGvqZ2aAsTQHbLoE92oZk5adgUvapVdh2924+9pytLd3l9rgOpxTS5e7nEylGCQV8qo8OW5uu/Cm+vfT+pyJkGlvn265r2oZC9sXBe4pbDsXWHiQnznROlbK4rhJ5wncbPU7vKM92/+13LIXbj47KdtWSKuS7beKXHpeJ5Ucuwv9jEOCu5segpt8jszyxvt2HsrJdQyk12W+++EuhPfhHepfBlYrketmku/MLj8Lb02LwqTUeIiFOeeT/pfpiD8O5nNYdRkRXi45qkx/Nd7MTsitnyhpzxbpjZK0qKWBDMllreyaPGVmHUeKAv9P0v70xXMoNaOkvKq43IEncAk+yGVx3M+3FEk/Ag42D0+LPETuXmaWzZ25qqQXUrnmS/9J66WDr/ABWtlEyPvROBHy3yTtZmYN8cwl7UrfdGE9oRaTQhTEqo7+7Cbfw3P5jsL7GsomGMd8YomiB1GeohfGOS22A6ydfvNxhIxGr56qcl/Fn5dNzEd2IunbeMf+Bm3KW4Uj8Pj1+1huuL+510l+jtZXzOy/mWeYpDH0NlApS//7KZ358Nr/N/Ah/sV5T7uZ5HumfBDZsfjz+m/SdIiSxpjHrH88yZwHXCTpGUrm0TWzsvmAh5yRZkPvOP2VpL/g3h6npE3b44MRmoZGS/o03oF4Jv6wbYsPGT6pILdRSqc4qrM46u1e3MzSNA9mSd434y9hg8udpdGF8imtfowPyNgfr9ktjiuZT5tZ01B9lcSeUJsp2aqQs2k32YmT++E5+MCn/HyP8+CdYGVxozvld7aZbZnsrxmtJoXYKbeaXW9ZdR//svyzNC3l+6ClKelyMnml30R/PigDRXpmjwY+hsdGWROfiq2XUZi95N8ykF6P6X0X7xBeEB/MdTU+aO/xglzWr5HNiPUM/j5OLqZZOG4SsJCl/qCy9yZd04WBC6xvFrDi7GfK/5ZV5gaTEVVDN7PM3/YKymd2B7e1HYE3C7OOoNJOUzP7beoo3BC/AVuaDx0usgseYnYsuciINH/5H8HtylW+kv8zs6cljZI0ysymyyMwZhxBXwfmZRQ6MCmPvTJaHn8ke9jmw23P/aHlRMjmHYrrpY9RFovlAjO7rFVi6XqfgM8J2aRczCwbc9ByUgh55L8JZnZk2n49XoM3fNRh17RLU9I3zQdvZeRnhv8h3rRvl/Z0ysccbFiQ25TmDv+yqJod5czsUkk7415N1+LeRk1zcHaLqscr6sZfvgpb4h/YC/D3/7oW53N+qlEfTF8loyFftQmapr6BRmVxbC4tyW+BtG+gB2j1xIhS6KoQSta8B79pxGNJWvmYJmVKPM+qZvbuCkX8JnChfHBSwzyYJbLPyWPPXAmcLOlJGoNejbHkBSFpPzO7LqV1l1rPQPM74NLkmWL4h6zn2mriMLwWvoSkA0kTIecFzCzrgK3CJ/AP5N9yyv3iko9gg+kpNdmzUMeVRn92Sac05yj0fAtAHmK20zX+eu7/OPwZLrY2jsLdYz+AK6CtSc3/buUKtcV58dhGT8ofnP7WGk+gQrwi87g+x1I+vWDXpFbim3A32Y3xQW5PWJpOUtKawCOWRkqnd+tWvLP2kEJyVYKmjVebafdy7/RcZeIYUQodH1qdhZIthoTtqhls3U0hdp2kd7aovec5ELfDjaN5HswiW+Aud1+lz+UuX8vKB7oqDkcuPU8zOzjZnrNJi/c3s4vKZKtiZidLmpVL82NWMhFyF+ndiw/z/h7uing88Lo8lvuheG0uG9qe2fvBzTrHpP9VR392Q69pdnyhzaN+5rlGzSOSq9p+O8oNcm1xvlT7l/ncA/tKuopCK0XN8cGzj0lP8cFTn9l78T6AKXhr+KqcyNF4oC0kvQ83V34Zj7t0DP7hAy/E7qlC910zu6ZFlqPxj3mn+fuWqKj4h4SRptDbhZKd2WJ7O8pimlhJB8dUYCd1DtO6mJVMHluG9Q0Vf13SBcDThVpqLx2YmA97Hugwnk/gL8+YVJY58S96QT5Jxi647/5ZuKfIVNxjaTXgILWfFGLR/IqZfSm3Or7HYg1GmsCczvaMUXjN/y0Fseyj/ZKkt+Kd32XxgqrKDRZV4xV1E1OlCj/BW7OH4Z4n/yvsH2198Vy2A44xn2/2LPlELg2kCt3PcG+hMh4vM3eVUFXxDwkjTaG3DCXbY0dYPiaHcKVSFg+9bTzyHH+R9CErxDnJ066zU9Kczk4zK/bedySlfTg+Fd48+MP2Yn+a2HKvnZ1xd7Hsg1PmfVE1vVl4Z9lxeEyYrKU1I9XqMv6UaloNmE9kPUPl3jWfo8RMUZHKaapxcNj8hY9tmUkjHx8mmzB514JMZvv9Ke7XbpSbK6rKDRbFeEUb4mMuigxofHAz21Q+CfNKwEqS7jaz/+ZERqvPM2Uj3KSS0UrPXSxpKzwKZLGlVVVBV1X8Q8KI8HJRnwvbGHyAwv20qCnLR/oVT+p5vAZ/dLEjRdJquDvXtviLdrZ5MKlsf0P88A7lzKZX+y99LpYNL7h6GK1ZlZT2J3Ab8hTcw2AF69GzIKV5N/DuwsvTM5LeZh7BsZNcyxGb6nL0Z8VyDXiavZL6isZZ59mVKskNJbmOxm3xCsWAxAeXtAluVpkzFwI++fuf0v7v4C2+f+FjJdYwM5O0AjDNzNYvSbPldIiSFsvV+NuVq6fp7QaLkaLQl223P9nyMtlD8SZy5ra4He6vPh/ulrSjfJ7MbJKFp/Hh+l83s9J8JJ2MB73q15DplNacuTsl3Wlm78jt69fDoTQfqhrn4bzWzNbrdGybNM8C/s/Mnuw1jZROSzsjdLY1SloG97HfPrdtQ/o6T2+3Nt41XZRzwNNM6a5Hc2f+b3P7x+GufVPpC3X765IKSCW5gaZFRWkOlkIvq83kFi7W25R48pAHm6U+GOSTrlxgZivlZNbBzagXZybN9K4v2B8TYYdyVVL8Q8WIMLkUFPYa9D3M15TcqNXNLN9UP0/SlWb2Pkm3p2134TbhzXMPSHF6szxVbe3ZgKEs/8vNZ93J03VnZxe8lJqlN8n9gB8nuVX1g4Pwqcxuo7Gm1dGTqEB/O+oepc81MitDNinJgDEYaUo6CZ+M5Sb67MmGj2jM+C0+aCtrHW6Pm+O2oZGqcgPNz6oIWYoLLmlcycfozf3Iv+VcCLm8ryspz99bJSjpTLxT/s9WYbatMuYmZQ4jRKFnyGcJ34a+Xv0TJJ1huRnocXejOZ4r8gBFi6d9mdlgK7yGPl0+CXPTtG0FKtnaJf0YH8Bxctq0h6SpZrZ3Tqynzs6K7Ig3c7+Ed0gtg59rf5iGd0jdSuPHqCuSR8ZoPDhU0Y2sCTWP2FyNFiM2RwBT8CBt7T7Yb7fGoF7T5YPPepUbUKzzVIVFzpK0RbJpI5/s4wL6XE8roYpzIfTIUXjn/OGSzgBONLO7+pnmsDIiTC4Zku7Ea+Avp/X58NgfebPFJhTmHcWbqJfjYVR/mZNdAB9Jtz3euTMNOKesU7OTrT3J3ILHl8gmnR2Nx30om7R4RCDpCjPr73DxfHrTrcLsLmocBZqN2GzlYjZXk5TFV6wwqrEgcyIeY/+6tL42PiXhF3qRGyxUcWCRKkSYrJjfCW12m7WItNplHgvjOuA7uDvkscDvrNmTZq5npCn0P+HD47NYJYvgF36zgty8eG+48DlHO9oX5a5l2wDbZXa+HmzttwDvz5phKc3LB1uhq8XsRxn9yV8+6/kr+As5EJ1bB+IdwqVTy6nauIARRbIrr4Z7y5SarVJl5e1Adu4T8dg+r5Pr+K8qN4jncjV9A4s2Jw0sMrOm0bKSvoh7iE3COzCvLcoMN8kMtAPeun2MPhfad1sKkTuSGGkK/Q+4SeMSXIFtjHcKZfOKfiXJte2A6iK/13Fb+645W/v9xdpITn573CVxOv4xeR/emXpqt3l3Wc7KncY9pF3WydWfzq226SkXQ0PSWWbWX5PRsCOptIWTN2NUvYeDea+rIGmWeSz8Wy2NnpZ0lZm9N/3Pd34LV5S34vFXuh5oIw+7cLBaDBy0/gViOxuv+J2Ex3T/Z27fTDOb0vLguZQRZUPHh6Cfk1u/vChQsQOqKl3Z2s3sFEmX4x8dAd+yHgJUdUvZS6wUmbGD3bYtcpfNX5vZ6f0pX54K5pb89e1pVOHcRhX7c05hL0GjKePhXuQGkU4Di7qJMFmFbFRyLwMHS5GHCXgUOMLMLkvmvaMlPYRP2fjMSFTmMMJq6FVITdJOHVDdptEDSeAAAASNSURBVFnJ1i4fGHOTmb0oaQc8VvWhQ1Br6joyYxdpX1nwGuo3ahNcqlBD71ekyOFGXcxOpRbhlM2sGNOmktxgkZThncAi+LO2MO5O2uRhMrci6Qbgg2b2jHzw2qn0hQl4h+VmLBppjAiF3sFGbPle/yodUP0sS5OtPbfvFnz2olXwFsHxeATHAetUbFGmwRys9D3cvbJo8+7JXUstgkuZ2a5p/2spH+FjB17KDqX/gaXmWtQhnHK3csOFpLbzBliX7q6Sfmlme6rcD97wSszR3XxQlJsmUNKR+HwH+6b1OeNERiIjxeSyWck24dPN7VPYvjhwh9xnPOuAMhugAPRJkR2dliKvmpnJQ7EeZmbHFbw1BoteIjNWJfMi+GJum9G7OaRtcCnrIeRBTegUTrlbuQGlC0W9Lu4pcgo+iXZ/H8BsboJf4ZOSv457sGVjOBbHK07v7CLNXsIEjAhGROGtcWBR0X3wrIL4vrn/7eKzDAaz5TPD7Ai8N7ktDsU1HrTBSmY20IGfhju41NxKFk75KsrDKXcrN9BUVdRvwZ0Vtsff0wvwVuLtLeQ7cYt8kNxncM+erCJ3Ij5r0v8kdRuW4hTgCkn/wp/HqwDkYQLmmjAKvTBSTC7dug929BkfpHK+JeV7vZldnexzJ5jZ8oOcbzszxTgzG9uPtD9dtr0Xr6GU3vfwUY4b4XNSGvAbM/ter2WsA5Lmpy+eyA74HJcnF01bVeUGoXyj6VPUq1BBUSf34e3xQGL79fIOSjoE71D9qpnNTtsWwkeuvmRmraaX7JTukIcJGApGikLv6D7YrdIfxLIOy8dksEjuYhnjcEV8w0B0HGkuDC411LToOM1qvy/j5oXv4IHDOspZ+aw6A0onRZ32b5pkJuFjGI43s3/0kNc9wIpFJ4f0gbnLOkwt90ZjRJhcqOY+2G18lgGjxcdEFVz05nrM7Mv5dfmoupNaiLck8ydO/7cxszPMQ+e+IulHZlbsC3lDYG0mo0hKa2W8Bl5JjkK8m4GkRFEfRmFyDUnTUhn+BPzQzG7rZ7ZW5rFmZq9Jmvtro0ONmY2YBQ809SngfNys8GvgQ2nfx3FFmg3d3Qh4YIjK9To+z+EKuW33D/f1GqRzHYu7yXV73A1l/8vWY2m6dp8bSLkeyzANj+t+ALByG7nX8eBhs4EXcsts4IUe8v0D7npb3L4DcO5w35u5bRkRJpcyWrkPVvUZH+CyfByvoa+HT958Km4XHvGdfQV3sVG4N8Hp1hhwrEo6c0IDqxAmuLgezH0ks+ecSKP5XQyiO6mkpfFWwH/omyhkTbyv6OPWgxmnzoxYhV6Fdj7jg5TfkH9MBhs1Dlt/FXjIzB7tIZ2WA4ZG+gCiYPBRX5x64XHqB72vYCRSa4U+nAz1x2Qo6E84gcH0xAmCwAmFHpQymOEEgiAYHEKhB6UMZjiBIAgGh1HDXYBgrmWMmV1sZmcA/7RcOIFhLlcQBC0IhR60YjDnPg2CYBAIk0tQSnRiBsHIIxR6EARBTQiTSxAEQU0IhR4EQVATQqEHQRDUhFDoQRAENeH/A/0IPZ1bWQaWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The maximum number of different character for one alphabet is 55\n",
      "The minimum number of different character for one alphabet is 14\n",
      "The total number of different character is 964\n"
     ]
    }
   ],
   "source": [
    "alph_num_char ={alphabet:len(os.listdir(f'{PATH}{alphabet}')) for alphabet in alph_type}\n",
    "num_of_char = alph_num_char.values()\n",
    "\n",
    "plt.bar(range(len(alph_type)),num_of_char)\n",
    "plt.xticks(range(len(alph_type)), [alph[:10] for alph in alph_type], rotation=90)\n",
    "plt.title('Number of characters per alphabet')\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nThe maximum number of different character for one alphabet is {max(num_of_char)}')\n",
    "print(f'The minimum number of different character for one alphabet is {min(num_of_char)}')\n",
    "total_char = sum(num_of_char)\n",
    "print(f'The total number of different character is {total_char}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Evaluation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The maximum number of different character for one alphabet is 47\n",
      "The minimum number of different character for one alphabet is 20\n",
      "The total number of different character is 659\n"
     ]
    }
   ],
   "source": [
    "alph_num_char_test ={alphabet:len(os.listdir(f'{PATH_TEST}{alphabet}')) for alphabet in alph_type_test}\n",
    "num_of_char_test = alph_num_char_test.values()\n",
    "\n",
    "plt.bar(range(len(alph_type_test)),num_of_char_test)\n",
    "plt.xticks(range(len(alph_type_test)), [alph[:10] for alph in alph_type_test], rotation=90)\n",
    "plt.title('Number of characters per alphabet')\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nThe maximum number of different character for one alphabet is {max(num_of_char_test)}')\n",
    "print(f'The minimum number of different character for one alphabet is {min(num_of_char_test)}')\n",
    "total_char_test = sum(num_of_char_test)\n",
    "print(f'The total number of different character is {total_char_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Number of pictures per character\n",
    "#### a) Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 20\n",
      "The total number of different pictures is 19280\n"
     ]
    }
   ],
   "source": [
    "alph_num_char_ex={}\n",
    "for alphabet in alph_type:\n",
    "    char_list=os.listdir(f'{PATH}{alphabet}')\n",
    "    for char in char_list:\n",
    "        alph_num_char_ex[(alphabet,char)]= len(os.listdir(f'{PATH}{alphabet}/{char}'))\n",
    "\n",
    "num_of_example = alph_num_char_ex.values()\n",
    "\n",
    "plt.bar(range(len(alph_num_char_ex)),num_of_example)\n",
    "plt.title('Number of example pictures per character')\n",
    "plt.show()\n",
    "\n",
    "total_example = sum(num_of_example) \n",
    "print(min(num_of_example),max(num_of_example))\n",
    "print(f'The total number of different pictures is {total_example}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Evaluation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 20\n",
      "The total number of different pictures is 13180\n"
     ]
    }
   ],
   "source": [
    "alph_num_char_ex_test={}\n",
    "for alphabet in alph_type_test:\n",
    "    char_list_test=os.listdir(f'{PATH_TEST}{alphabet}')\n",
    "    for char in char_list_test:\n",
    "        alph_num_char_ex_test[(alphabet,char)]= len(os.listdir(f'{PATH_TEST}{alphabet}/{char}'))\n",
    "\n",
    "num_of_example_test = alph_num_char_ex_test.values()\n",
    "\n",
    "plt.bar(range(len(alph_num_char_ex_test)),num_of_example_test)\n",
    "plt.title('Number of example pictures per character')\n",
    "plt.show()\n",
    "\n",
    "total_example = sum(num_of_example_test) \n",
    "print(min(num_of_example_test),max(num_of_example_test))\n",
    "print(f'The total number of different pictures is {total_example}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that each character have 20 examples(pictures) and that our training and evluation set are well balanced. For our training we will consider that each character is an independent class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C/ Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Add a label for each character\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each picture in our training set we set a corresponding integer which allow us to determine the corresponding character. Here an integer is sufficient as we are not really interested in knowing from which alphabet an image is coming from and as we don't have need to know the character name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19280,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train = np.array([i//20+1 for i in range(total_example)])\n",
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reshape, our data to have the number of channel including (here is 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19280, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train=Y_train.reshape(*Y_train.shape,1)\n",
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Convert images to datafile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first retrieve the path for each picture in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['images_background/Syriac_(Estrangelo)/character17/0289_20.png',\n",
       " 'images_background/Syriac_(Estrangelo)/character17/0289_08.png',\n",
       " 'images_background/Syriac_(Estrangelo)/character17/0289_12.png',\n",
       " 'images_background/Syriac_(Estrangelo)/character17/0289_17.png',\n",
       " 'images_background/Syriac_(Estrangelo)/character17/0289_05.png']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train set\n",
    "imagePath = glob.glob(f\"{PATH}*/*/*.png\")\n",
    "\n",
    "#Test set\n",
    "testPath = glob.glob(f\"{PATH_TEST}*/*/*.png\")\n",
    "\n",
    "imagePath[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A few examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print some random images of the dataset, convert them to arrays and resize them.\n",
    "We decide to resize to the size of 64x64 pixels, which will help the model to be easier to train. Moreover, 64x64 seems a reasonable size still allowing us and the machine to recognize the character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SZ=64 #Dimension of the output image expected\n",
    "\n",
    "#Dimensions of the grill of sample pictures\n",
    "columns = 4\n",
    "rows = 5\n",
    "\n",
    "fig=plt.figure(figsize=(5, 5))\n",
    "\n",
    "#Print 20 random examples of images\n",
    "list_example = np.random.randint(total_example, size = columns*rows)\n",
    "pos=0\n",
    "for i in list_example:\n",
    "    pos+=1\n",
    "    img = mpimg.imread(imagePath[i])\n",
    "    img = resize(img, (SZ,SZ), mode='reflect')\n",
    "    fig.add_subplot(rows, columns, pos)\n",
    "    plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All images seems clear and well centered. We can also observe that they only have one channel and no RGB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image conversion\n",
    "We convert all training images into arrays and resize them all to the 64x64 format. We concatenate all arrays into the variable X_train\n",
    "\n",
    "#### a) Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'imagePath' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-1693b4e76caa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Train set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#Transform in array and resize all 19280 images\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmpimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSZ\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mSZ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'nearest'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mimagePath\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'imagePath' is not defined"
     ]
    }
   ],
   "source": [
    "#Train set\n",
    "#Transform in array and resize all 19280 images \n",
    "X_train = np.array([resize(mpimg.imread(i), (SZ,SZ), mode='nearest') for i in imagePath] )\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our CNN blocks later we will need to define a number of channel. Here pictures only have one channel. So we will reshape the pictures to take in account that they have one channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19280, 64, 64, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channel_sz = 1 #number of channel\n",
    "X_train= X_train.reshape(*X_train.shape, channel_sz)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Evaluation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same process is done with the evaluation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13180, 64, 64, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_array = np.array([resize(mpimg.imread(i), (SZ,SZ), mode='nearest') for i in testPath] )\n",
    "test_array= test_array.reshape(*test_array.shape, channel_sz)\n",
    "test_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13180, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_char_test = np.array([i//20+1 for i in range(test_array.shape[0])])\n",
    "class_char_test = class_char_test.reshape(*class_char_test.shape,1)\n",
    "class_char_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Validation/Test set split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the evaluation set into a train and a validation sets of pictures (Test : 70%, Validation : 30%)\n",
    "We will use the validation set to select our model and the test set to check the performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3954, 64, 64, 1), (9226, 64, 64, 1), (3954, 1), (9226, 1))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val, X_test, Y_val, Y_test = train_test_split(test_array, class_char_test, test_size=0.7, stratify= class_char_test)\n",
    "X_val.shape, X_test.shape, Y_val.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Triplet loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our loss function is determined by the following triplet loss function.<br/>\n",
    "The basic idea here is to take the embeddings of a triplet of pictures (Anchor, Positive, Negative) where :\n",
    "* Anchor :  are the embeddings corresponding to our base image\n",
    "* Positive : are the embeddings of another image corresponding to the same class as our Anchor \n",
    "* Negative : are the embeddings of an image which correspond to an image from a different class than our Anchor\n",
    "\n",
    "The goal of the triplet loss is to minimize the distance between the embeddings of the Anchor and the Positive image (that we call the pos_dist), while it should maximize the distance between the embeddings of the Anchor and the Negative Image(neg_dist).\n",
    "Thus triplet loss should maximize the difference between the pos_dis and neg_dis. If this difference if below 0, our model is accurate, while if the difference is above 0, we obtain a value for our loss to optimize.\n",
    "\n",
    "\n",
    "Due to the way we define our triplet_loss we need to add an alpha term to insure that the difference between pos_dist and neg_dist is not zero. This also prevent the model from choosing the trivial values of zero embeddings for our 3 images.\n",
    "\n",
    "The initial triplet loss is define as the sum of the difference between pos_dist and neg_dist over a mini-batch. Therefore it makes the loss value dependant of the mini batc size. Instead we choose to use the mean over a minibatch to keep a constant loss independent of mini batch size.\n",
    "We also decide to divide by alpha to obtain a value non dependent of alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(y_true, y_pred, alpha = 0.2):\n",
    "    \"\"\"\n",
    "    Implementation of the triplet loss\n",
    "    \n",
    "    Arguments:\n",
    "    y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.\n",
    "    y_pred -- python list containing three objects:\n",
    "            anchor -- the encodings for the anchor images, of shape (None, 64)\n",
    "            positive -- the encodings for the positive images, of shape (None, 64)\n",
    "            negative -- the encodings for the negative images, of shape (None, 64)\n",
    "    \n",
    "    Returns:\n",
    "    loss -- real number, value of the loss\n",
    "    \"\"\"\n",
    "    \n",
    "    anchor, positive, negative = y_pred[:,0:64], y_pred[:,64:128], y_pred[:,128:196]\n",
    "    \n",
    "    #Compute the (encoding) distance between the anchor and the positive, you will need to sum over axis=-1\n",
    "    pos_dist = tf.sqrt(tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), axis = -1))\n",
    "    #Compute the (encoding) distance between the anchor and the negative, you will need to sum over axis=-1\n",
    "    neg_dist = tf.sqrt(tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), axis = -1))\n",
    "    #Subtract the two previous distances and add alpha.\n",
    "    basic_loss = tf.add(tf.subtract(pos_dist, neg_dist), alpha)\n",
    "    # Count number of positive triplets (where triplet_loss > 0)\n",
    "    valid_triplets = tf.to_float(tf.greater(basic_loss,0))\n",
    "    num_positive_triplets = tf.reduce_sum(valid_triplets)\n",
    "    #Take the maximum of basic_loss and 0.0. Sum over the training examples.\n",
    "    loss = tf.reduce_mean(tf.maximum(basic_loss, 0))/ alpha * 100\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our error rate by calculating the number of hard triplets over the total number of triplets. For the accuracy we simply take 1-error percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_acc(y_true, y_pred, alpha = 0.2):\n",
    "    \"\"\"\n",
    "    Implementation of the triplet accuracy\n",
    "    \n",
    "    Arguments:\n",
    "    y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.\n",
    "    y_pred -- python list containing three objects:\n",
    "            anchor -- the encodings for the anchor images, of shape (None, 64)\n",
    "            positive -- the encodings for the positive images, of shape (None, 64)\n",
    "            negative -- the encodings for the negative images, of shape (None, 64)\n",
    "    \n",
    "    Returns:\n",
    "    loss -- real number, value of the accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    anchor, positive, negative = y_pred[:,0:64], y_pred[:,64:128], y_pred[:,128:196]\n",
    "    \n",
    "    #Compute the (encoding) distance between the anchor and the positive, you will need to sum over axis=-1\n",
    "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), axis = -1)\n",
    "    #Compute the (encoding) distance between the anchor and the negative, you will need to sum over axis=-1\n",
    "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), axis = -1)\n",
    "    #Subtract the two previous distances and add alpha.\n",
    "    basic_loss = tf.add(tf.subtract(pos_dist, neg_dist), alpha)\n",
    "    # Count number of positive triplets (where triplet_loss > 0)\n",
    "    hard_triplets = tf.to_float(tf.greater(basic_loss,alpha))\n",
    "    num_hard_triplets = tf.reduce_sum(hard_triplets)\n",
    "    #Count number of triplets\n",
    "    all_triplets = tf.reduce_sum(tf.to_float(tf.greater(basic_loss,-10**10)))\n",
    "    \n",
    "    #Accuracy\n",
    "    acc = 1 - num_hard_triplets/all_triplets\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.Creation of valid triplets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The possible number of valid triplets in our training set is huge:\n",
    "20*19*19XXX\n",
    "\n",
    "To be able to feed our model with only valid triplets, we will later build a generator for our training set based on a select. Therefore for the validation set, to be able to compare more accurately our models, we create a fixed number of valid triplets. There is no need here to create a generator as all the triplets can easily fit in memory.\n",
    "For each (anchor, positive) couple of images we add a random negative anchor image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_triplets(X, Y, num=1):\n",
    "    \"\"\"\n",
    "    Create a list of valid triplets for each valid (anchor, positive) couple\n",
    "    \n",
    "    Arguments:\n",
    "    X -- array of images\n",
    "    Y -- array of classes corresponding to each image\n",
    "    num -- number of negative images for each valid anchor and positive images - must be positive\n",
    "           if num = 0, all possible valid couples are created\n",
    "            For example : for one valid (A,P) couple we can select 'num' random N images. \n",
    "                          Thus 'num' triplets are created for this (A,P) couple\n",
    "    \n",
    "    Returns:\n",
    "    (A,P,N) -- python tuple containing 3 arrays : \n",
    "            A -- the array for the anchor images, of shape (None, 64)\n",
    "            P -- the array for the positive images, of shape (None, 64)\n",
    "            N -- the array for the negative images, of shape (None, 64)\n",
    "    \"\"\"\n",
    "\n",
    "    Y = Y.reshape(Y.shape[0],)\n",
    "    A = []\n",
    "    P = []\n",
    "    N = []\n",
    "    \n",
    "    #We loop over all possible valid (A,P)\n",
    "    for i in range(X.shape[0]):  \n",
    "        list_pos = X[Y==Y[i]]\n",
    "        for j in list_pos:\n",
    "            #We provide a number 'num' of triplets for each valid (A,P)\n",
    "            if num >=1:\n",
    "                for k in range(num):\n",
    "                    rand_num = np.random.randint(X.shape[0])\n",
    "                    if np.array_equal(X[i],j) == False:\n",
    "                        A.append(X[i])\n",
    "                        P.append(j)\n",
    "                        while np.array_equal(Y[rand_num], Y[i]):\n",
    "                            rand_num = np.random.randint(X.shape[0])\n",
    "                        N.append(X[rand_num])\n",
    "            if num == 0:\n",
    "                for k in range(X.shape[0]):\n",
    "                    if np.array_equal(X[i],j) == False:\n",
    "                        if np.array_equal(Y[i],Y[k]) == False:\n",
    "                            A.append(X[i])\n",
    "                            P.append(j)\n",
    "                            N.append(X[k])\n",
    "    \n",
    "    A = np.array(A)\n",
    "    P = np.array(P)\n",
    "    N = np.array(N)\n",
    "    \n",
    "    return (A, P, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(19770, 64, 64, 1), (19770, 64, 64, 1), (19770, 64, 64, 1)]\n"
     ]
    }
   ],
   "source": [
    "triplets_list_val = create_triplets(X_val, Y_val)\n",
    "print([i.shape for i in triplets_list_val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a number of 19770 valid triplets for the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D/ Triplet loss Neural Network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Triplet loss Neural Network is composed of 2 parts :\n",
    "* a Convolutional Neural Network(CNN) which transforms an input image into an embedding(vector of features describing the whale). The same CNN, with the same weights are used one valid triplet of images.\n",
    "* a global model which will concatenante all the embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Embedding Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding model that we use is based on the version 2 of ResNet model. Therefore we won't use the original model because our input images here are only of size 64x64 instead of (229x229) and moreover as our problem is easier with less channels and features we will create a small variant of this model.\n",
    "\n",
    "The embedding Model is composed of 4 blocks :\n",
    "* Block 1 - 64x64\n",
    "* Block 2 - 32x32\n",
    "* Block 3 - 16x16\n",
    "* Block 4 - 8x8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EmbModel(input_shape, l2):\n",
    "    regul  = regularizers.l2(l2)\n",
    "    kwargs = {'padding':'same', 'kernel_regularizer':regul,'kernel_initializer':'he_normal'}\n",
    "\n",
    "    inp = Input(input_shape) # 64x64x1\n",
    "    x   = Conv2D(32, (3,3), strides=1, **kwargs)(inp) #32x32x64\n",
    "    x   = BatchNormalization()(x)\n",
    "    x   = Activation('relu')(x)\n",
    "    \n",
    "    #Stage 0 / resblock 0\n",
    "    y   = Conv2D(32, (1,1), strides=1, **kwargs)(x)\n",
    "    \n",
    "    y   = BatchNormalization()(y)\n",
    "    y   = Activation('relu')(y)\n",
    "    y   = Conv2D(32, (3,3), strides=1, **kwargs)(y)\n",
    "    \n",
    "    y   = BatchNormalization()(y)\n",
    "    y   = Activation('relu')(y)\n",
    "    y   = Conv2D(64, (1,1), strides=1, **kwargs)(y)\n",
    "    \n",
    "    x   = Conv2D(64, (1,1), strides=1, **kwargs) (x)\n",
    "    x   = Add()([x,y])\n",
    "    \n",
    "    #Stage 0 / resblock 1\n",
    "    x   = BatchNormalization()(x)\n",
    "    x   = Activation('relu')(x)\n",
    "    y   = Conv2D(32, (1,1), strides=1, **kwargs)(x)\n",
    "    \n",
    "    y   = BatchNormalization()(y)\n",
    "    y   = Activation('relu')(y)\n",
    "    y   = Conv2D(32, (3,3), strides=1, **kwargs)(y)\n",
    "    \n",
    "    y   = BatchNormalization()(y)\n",
    "    y   = Activation('relu')(y)\n",
    "    y   = Conv2D(64, (1,1), strides=1, **kwargs)(y)\n",
    "    \n",
    "    x   = Add()([x,y])\n",
    "    \n",
    "    #Stage 1 / resblock 0\n",
    "    y   = Conv2D(64, (1,1), strides=2, **kwargs)(x)\n",
    "    \n",
    "    y   = BatchNormalization()(y)\n",
    "    y   = Activation('relu')(y)\n",
    "    y   = Conv2D(64, (3,3), strides=1, **kwargs)(y)\n",
    "    \n",
    "    y   = BatchNormalization()(y)\n",
    "    y   = Activation('relu')(y)\n",
    "    y   = Conv2D(128, (1,1), strides=1, **kwargs)(y)\n",
    "    \n",
    "    x   = Conv2D(128, (1,1), strides=2, **kwargs) (x)\n",
    "    x   = Add()([x,y])\n",
    "    \n",
    "    #Stage 1 / resblock 1\n",
    "    x   = BatchNormalization()(x)\n",
    "    x   = Activation('relu')(x)\n",
    "    y   = Conv2D(64, (1,1), strides=1, **kwargs)(x)\n",
    "    \n",
    "    y   = BatchNormalization()(y)\n",
    "    y   = Activation('relu')(y)\n",
    "    y   = Conv2D(64, (3,3), strides=1, **kwargs)(y)\n",
    "    \n",
    "    y   = BatchNormalization()(y)\n",
    "    y   = Activation('relu')(y)\n",
    "    y   = Conv2D(128, (1,1), strides=1, **kwargs)(y)\n",
    "    \n",
    "    x   = Add()([x,y])\n",
    "    \n",
    "    #Stage 2 / resblock 0\n",
    "    y   = Conv2D(128, (1,1), strides=2, **kwargs)(x)\n",
    "    \n",
    "    y   = BatchNormalization()(y)\n",
    "    y   = Activation('relu')(y)\n",
    "    y   = Conv2D(128, (3,3), strides=1, **kwargs)(y)\n",
    "    \n",
    "    y   = BatchNormalization()(y)\n",
    "    y   = Activation('relu')(y)\n",
    "    y   = Conv2D(256, (1,1), strides=1, **kwargs)(y)\n",
    "    \n",
    "    x   = Conv2D(256, (1,1), strides=2, **kwargs) (x)\n",
    "    x   = Add()([x,y])\n",
    "    \n",
    "    #Stage 2 / resblock 1\n",
    "    x   = BatchNormalization()(x)\n",
    "    x   = Activation('relu')(x)\n",
    "    y   = Conv2D(128, (1,1), strides=1, **kwargs)(x)\n",
    "    \n",
    "    y   = BatchNormalization()(y)\n",
    "    y   = Activation('relu')(y)\n",
    "    y   = Conv2D(128, (3,3), strides=1, **kwargs)(y)\n",
    "    \n",
    "    y   = BatchNormalization()(y)\n",
    "    y   = Activation('relu')(y)\n",
    "    y   = Conv2D(256, (1,1), strides=1, **kwargs)(y)\n",
    "    \n",
    "    x   = Add()([x,y])\n",
    "    \n",
    "    #Stage 3 / resblock 0\n",
    "    y   = Conv2D(256, (1,1), strides=2, **kwargs)(x)\n",
    "    \n",
    "    y   = BatchNormalization()(y)\n",
    "    y   = Activation('relu')(y)\n",
    "    y   = Conv2D(256, (3,3), strides=1, **kwargs)(y)\n",
    "    \n",
    "    y   = BatchNormalization()(y)\n",
    "    y   = Activation('relu')(y)\n",
    "    y   = Conv2D(512, (1,1), strides=1, **kwargs)(y)\n",
    "    \n",
    "    x   = Conv2D(512, (1,1), strides=2, **kwargs) (x)\n",
    "    x   = Add()([x,y])\n",
    "    \n",
    "    #Stage 2 / resblock 1\n",
    "    x   = BatchNormalization()(x)\n",
    "    x   = Activation('relu')(x)\n",
    "    y   = Conv2D(256, (1,1), strides=1, **kwargs)(x)\n",
    "    \n",
    "    y   = BatchNormalization()(y)\n",
    "    y   = Activation('relu')(y)\n",
    "    y   = Conv2D(256, (3,3), strides=1, **kwargs)(y)\n",
    "    \n",
    "    y   = BatchNormalization()(y)\n",
    "    y   = Activation('relu')(y)\n",
    "    y   = Conv2D(512, (1,1), strides=1, **kwargs)(y)\n",
    "    \n",
    "    x   = Add()([x,y])\n",
    "    \n",
    "    #Final\n",
    "    x   = BatchNormalization()(x)\n",
    "    x   = Activation('relu')(x)\n",
    "    x   = AveragePooling2D(pool_size=8)(x)\n",
    "    x   = Flatten()(x)\n",
    "    x = Dense (64, activation ='tanh',kernel_initializer='he_normal') (x)\n",
    "\n",
    "    \n",
    "    ##Create model\n",
    "    model = Model(inputs = inp, outputs = x, name='EmbModel')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Global model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We define our global model\n",
    "def global_model(size, channel_size=1, l2=1e-4):\n",
    "    input_size = (size, size, channel_sz)                     \n",
    "\n",
    "    A = Input(input_size)\n",
    "    P = Input(input_size)\n",
    "    N = Input(input_size)\n",
    "\n",
    "    emb_model= EmbModel(input_size, l2)\n",
    "\n",
    "    out_A = emb_model(A)\n",
    "    out_P = emb_model(P)\n",
    "    out_N = emb_model(N)\n",
    "\n",
    "    y_pred = concatenate([out_A, out_P, out_N], axis =-1)\n",
    "\n",
    "    full_model = Model(inputs = [A, P, N], outputs = y_pred)\n",
    "    \n",
    "    return full_model, emb_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an instance of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model, emb_model = global_model(SZ,channel_sz, l2=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 64, 64, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 64, 64, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 64, 64, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "EmbModel (Model)                (None, 64)           2401728     input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 192)          0           EmbModel[1][0]                   \n",
      "                                                                 EmbModel[2][0]                   \n",
      "                                                                 EmbModel[3][0]                   \n",
      "==================================================================================================\n",
      "Total params: 2,401,728\n",
      "Trainable params: 2,394,880\n",
      "Non-trainable params: 6,848\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 64, 64, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 64, 64, 32)   320         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 64, 64, 32)   128         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 64, 64, 32)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 32)   1056        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64, 64, 32)   128         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 64, 64, 32)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 64, 32)   9248        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 32)   128         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 64, 64, 32)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 64, 64, 64)   2112        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 64, 64, 64)   2112        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 64, 64, 64)   0           conv2d_5[0][0]                   \n",
      "                                                                 conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 64, 64, 64)   256         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 64, 64, 64)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 64, 64, 32)   2080        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 64, 64, 32)   128         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 64, 64, 32)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 64, 64, 32)   9248        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 64, 64, 32)   128         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 64, 64, 32)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 64, 64, 64)   2112        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 64, 64, 64)   0           activation_4[0][0]               \n",
      "                                                                 conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 32, 32, 64)   4160        add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 64)   256         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 64)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 64)   36928       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 32, 32, 64)   256         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 32, 32, 64)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 128)  8320        add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 128)  8320        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 32, 32, 128)  0           conv2d_12[0][0]                  \n",
      "                                                                 conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 32, 32, 128)  512         add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 32, 32, 128)  0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 32, 64)   8256        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32, 32, 64)   256         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 32, 32, 64)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 32, 32, 64)   36928       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32, 32, 64)   256         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 32, 32, 64)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 32, 32, 128)  8320        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 32, 32, 128)  0           activation_9[0][0]               \n",
      "                                                                 conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 16, 16, 128)  16512       add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 128)  512         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 128)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 16, 16, 128)  147584      activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 16, 128)  512         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 16, 128)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 16, 16, 256)  33024       add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 16, 16, 256)  33024       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 16, 16, 256)  0           conv2d_19[0][0]                  \n",
      "                                                                 conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 16, 16, 256)  1024        add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 16, 16, 256)  0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 16, 16, 128)  32896       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 16, 16, 128)  512         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 16, 16, 128)  0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 16, 16, 128)  147584      activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 16, 16, 128)  512         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 16, 16, 128)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 16, 16, 256)  33024       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 16, 16, 256)  0           activation_14[0][0]              \n",
      "                                                                 conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 8, 8, 256)    65792       add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 8, 8, 256)    1024        conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 8, 8, 256)    0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 8, 8, 256)    590080      activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 8, 8, 256)    1024        conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 8, 8, 256)    0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 8, 8, 512)    131584      add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 8, 8, 512)    131584      activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 8, 8, 512)    0           conv2d_26[0][0]                  \n",
      "                                                                 conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 8, 8, 512)    2048        add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 8, 8, 512)    0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 8, 8, 256)    131328      activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 8, 8, 256)    1024        conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 8, 8, 256)    0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 8, 8, 256)    590080      activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 8, 8, 256)    1024        conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 8, 8, 256)    0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 8, 8, 512)    131584      activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 8, 8, 512)    0           activation_19[0][0]              \n",
      "                                                                 conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 8, 8, 512)    2048        add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 8, 8, 512)    0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 1, 1, 512)    0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 512)          0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           32832       flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,401,728\n",
      "Trainable params: 2,394,880\n",
      "Non-trainable params: 6,848\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classification_model.summary()\n",
    "emb_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Batch Generator\n",
    "Before creating our Batch Generator we will create an Image augmentation Pipeline to create more randomness in our data and to reduce overfitting.\n",
    "#### a) Image Augmentation Pipeline\n",
    "In our Augmentation Pipeline, we will use the following 4 transformations :\n",
    "1. random_rotation with a range of 30 degrees\n",
    "2. random_shear with an intensity of 0.2\n",
    "3. random_soom with a range of [0.85,1.15]\n",
    "4. random_shift with a range of 0.15 horizontally as vertically\n",
    "All these range are selected to be in a 'human range'. That means that they give some approximations of the variation that a human can have when writing these characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import random_rotation, random_shift, random_shear, random_zoom\n",
    "\n",
    "def augmentation_pipeline(img_arr):\n",
    "    img_arr = random_rotation(img_arr, 30, row_axis=0, col_axis=1, channel_axis=2, fill_mode='nearest')\n",
    "    img_arr = random_shear(img_arr, intensity=0.2, row_axis=0, col_axis=1, channel_axis=2, fill_mode='nearest')\n",
    "    img_arr = random_zoom(img_arr, zoom_range=(0.85, 1.15), row_axis=0, col_axis=1, channel_axis=2, fill_mode='nearest')\n",
    "    img_arr = random_shift(img_arr, wrg=0.15, hrg=0.15,fill_mode='nearest')\n",
    "    return img_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b)Batch generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a batch generator that will take 'batch_size' valid triplets from an input of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X,Y, bs=32,hardmode = False):\n",
    "    \"\"\"\n",
    "    Create a mini-batch generator\n",
    "    \n",
    "    Arguments:\n",
    "    X -- array of images\n",
    "    Y -- array of classes corresponding to each image\n",
    "    bs -- size of the minibatch\n",
    "\n",
    "    \n",
    "    Returns:\n",
    "    [A_batch, P_batch, N_batch], y_dummie) -- a mini-batch of size bs\n",
    "    \"\"\"\n",
    "    Y = Y.reshape(Y.shape[0],)\n",
    "    while True:\n",
    "        #0. Initialize Anchor,Postive, Negative\n",
    "        A_batch = []\n",
    "        P_batch = []\n",
    "        N_batch = []\n",
    "        for i in range(bs) :\n",
    "            #1.Choose a random Anchor Image\n",
    "            rand_A_num = np.random.randint(X.shape[0])    \n",
    "\n",
    "            #2.Choose a random Positive Image\n",
    "            list_pos = X[Y==Y[rand_A_num]]                            #List of positive images\n",
    "            rand_P_num = np.random.randint(len(list_pos))\n",
    "            while np.array_equal(X[rand_A_num],list_pos[rand_P_num]):\n",
    "                rand_P_num = np.random.randint(len(list_pos))\n",
    "                \n",
    "\n",
    "            #3.Choose a random Negative Image\n",
    "            rand_N_num = np.random.randint(X.shape[0])\n",
    "            while np.array_equal(Y[rand_N_num], Y[rand_A_num]):\n",
    "                rand_A_num = np.random.randint(X.shape[0])\n",
    "                \n",
    "            A_augment = augmentation_pipeline(X[rand_A_num])\n",
    "            P_augment = augmentation_pipeline(list_pos[rand_P_num])\n",
    "            N_augment = augmentation_pipeline(X[rand_N_num])\n",
    "                    \n",
    "            A_batch.append(A_augment)\n",
    "            P_batch.append(P_augment)\n",
    "            N_batch.append(N_augment)\n",
    "            \n",
    "        A_batch = np.array(A_batch)\n",
    "        P_batch = np.array(P_batch)\n",
    "        N_batch = np.array(N_batch)\n",
    "        \n",
    "        y_dummie = np.zeros((len(A_batch),))\n",
    "        \n",
    "        yield ([A_batch, P_batch, N_batch], y_dummie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E/Training procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the model training we will use a mix of several techniques to optimize our training accuracy and speed :\n",
    "* Adam optimizer :\n",
    "* Reduce Learning Rate on Plateau\n",
    "* Stochastic Gradient Descent with Restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TANH32-1.h5\n",
      "Epoch 1/200\n",
      "550/550 [==============================] - 187s 341ms/step - loss: 16.6291 - triplet_acc: 0.9692 - val_loss: 74.1449 - val_triplet_acc: 0.7997\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 74.14487, saving model to TANH32-1.h5\n",
      "Epoch 2/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 15.3093 - triplet_acc: 0.9730 - val_loss: 107.9700 - val_triplet_acc: 0.5620\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 3/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 14.1778 - triplet_acc: 0.9747 - val_loss: 16.9408 - val_triplet_acc: 0.9662\n",
      "\n",
      "Epoch 00003: val_loss improved from 74.14487 to 16.94080, saving model to TANH32-1.h5\n",
      "Epoch 4/200\n",
      "550/550 [==============================] - 175s 319ms/step - loss: 12.9151 - triplet_acc: 0.9772 - val_loss: 12.3717 - val_triplet_acc: 0.9797\n",
      "\n",
      "Epoch 00004: val_loss improved from 16.94080 to 12.37169, saving model to TANH32-1.h5\n",
      "Epoch 5/200\n",
      "550/550 [==============================] - 175s 318ms/step - loss: 12.4902 - triplet_acc: 0.9775 - val_loss: 13.9389 - val_triplet_acc: 0.9747\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/200\n",
      "550/550 [==============================] - 176s 319ms/step - loss: 12.1411 - triplet_acc: 0.9778 - val_loss: 13.5312 - val_triplet_acc: 0.9745\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 11.3935 - triplet_acc: 0.9786 - val_loss: 44.3978 - val_triplet_acc: 0.8620\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 11.1888 - triplet_acc: 0.9789 - val_loss: 12.4313 - val_triplet_acc: 0.9759\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 9/200\n",
      "550/550 [==============================] - 177s 321ms/step - loss: 9.3051 - triplet_acc: 0.9855 - val_loss: 9.4278 - val_triplet_acc: 0.9847\n",
      "\n",
      "Epoch 00009: val_loss improved from 12.37169 to 9.42781, saving model to TANH32-1.h5\n",
      "Epoch 10/200\n",
      "550/550 [==============================] - 175s 319ms/step - loss: 8.0864 - triplet_acc: 0.9891 - val_loss: 105.1524 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/200\n",
      "550/550 [==============================] - 175s 319ms/step - loss: 7.6339 - triplet_acc: 0.9895 - val_loss: 7.9783 - val_triplet_acc: 0.9876\n",
      "\n",
      "Epoch 00012: val_loss improved from 8.43877 to 7.97834, saving model to TANH32-1.h5\n",
      "Epoch 13/200\n",
      "550/550 [==============================] - 176s 321ms/step - loss: 7.3937 - triplet_acc: 0.9897 - val_loss: 9.5929 - val_triplet_acc: 0.9812\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/200\n",
      "550/550 [==============================] - 175s 319ms/step - loss: 7.1374 - triplet_acc: 0.9891 - val_loss: 7.8450 - val_triplet_acc: 0.9873\n",
      "\n",
      "Epoch 00014: val_loss improved from 7.97834 to 7.84501, saving model to TANH32-1.h5\n",
      "Epoch 15/200\n",
      "550/550 [==============================] - 176s 319ms/step - loss: 6.6137 - triplet_acc: 0.9907 - val_loss: 7.4646 - val_triplet_acc: 0.9884\n",
      "\n",
      "Epoch 00015: val_loss improved from 7.84501 to 7.46465, saving model to TANH32-1.h5\n",
      "Epoch 16/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 6.9332 - triplet_acc: 0.9893 - val_loss: 7.7900 - val_triplet_acc: 0.9863\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 6.6134 - triplet_acc: 0.9899 - val_loss: 7.0498 - val_triplet_acc: 0.9885\n",
      "\n",
      "Epoch 00017: val_loss improved from 7.46465 to 7.04976, saving model to TANH32-1.h5\n",
      "Epoch 18/200\n",
      "550/550 [==============================] - 175s 318ms/step - loss: 6.2048 - triplet_acc: 0.9912 - val_loss: 8.4207 - val_triplet_acc: 0.9837\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/200\n",
      "550/550 [==============================] - 175s 318ms/step - loss: 6.0982 - triplet_acc: 0.9917 - val_loss: 7.5517 - val_triplet_acc: 0.9871\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 5.9303 - triplet_acc: 0.9914 - val_loss: 8.0160 - val_triplet_acc: 0.9847\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 21/200\n",
      "550/550 [==============================] - 175s 319ms/step - loss: 6.0778 - triplet_acc: 0.9915 - val_loss: 7.4493 - val_triplet_acc: 0.9860\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 22/200\n",
      "550/550 [==============================] - 175s 318ms/step - loss: 5.5905 - triplet_acc: 0.9920 - val_loss: 6.8274 - val_triplet_acc: 0.9879\n",
      "\n",
      "Epoch 00022: val_loss improved from 7.04976 to 6.82740, saving model to TANH32-1.h5\n",
      "Epoch 23/200\n",
      "550/550 [==============================] - 175s 319ms/step - loss: 5.5305 - triplet_acc: 0.9927 - val_loss: 6.7797 - val_triplet_acc: 0.9885\n",
      "\n",
      "Epoch 00023: val_loss improved from 6.82740 to 6.77967, saving model to TANH32-1.h5\n",
      "Epoch 24/200\n",
      "550/550 [==============================] - 175s 317ms/step - loss: 5.3633 - triplet_acc: 0.9920 - val_loss: 6.6333 - val_triplet_acc: 0.9889\n",
      "\n",
      "Epoch 00024: val_loss improved from 6.77967 to 6.63327, saving model to TANH32-1.h5\n",
      "Epoch 25/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 4.9744 - triplet_acc: 0.9941 - val_loss: 6.3828 - val_triplet_acc: 0.9895\n",
      "\n",
      "Epoch 00025: val_loss improved from 6.63327 to 6.38284, saving model to TANH32-1.h5\n",
      "Epoch 26/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 4.8061 - triplet_acc: 0.9951 - val_loss: 6.4012 - val_triplet_acc: 0.9892\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 27/200\n",
      "550/550 [==============================] - 182s 331ms/step - loss: 4.9020 - triplet_acc: 0.9944 - val_loss: 6.1614 - val_triplet_acc: 0.9897\n",
      "\n",
      "Epoch 00027: val_loss improved from 6.38284 to 6.16140, saving model to TANH32-1.h5\n",
      "Epoch 28/200\n",
      "550/550 [==============================] - 190s 346ms/step - loss: 4.6971 - triplet_acc: 0.9948 - val_loss: 6.1027 - val_triplet_acc: 0.9899\n",
      "\n",
      "Epoch 00028: val_loss improved from 6.16140 to 6.10269, saving model to TANH32-1.h5\n",
      "Epoch 29/200\n",
      "550/550 [==============================] - 192s 350ms/step - loss: 4.9746 - triplet_acc: 0.9941 - val_loss: 6.0172 - val_triplet_acc: 0.9901\n",
      "\n",
      "Epoch 00029: val_loss improved from 6.10269 to 6.01720, saving model to TANH32-1.h5\n",
      "Epoch 30/200\n",
      "550/550 [==============================] - 193s 350ms/step - loss: 4.9444 - triplet_acc: 0.9936 - val_loss: 6.0027 - val_triplet_acc: 0.9902\n",
      "\n",
      "Epoch 00030: val_loss improved from 6.01720 to 6.00270, saving model to TANH32-1.h5\n",
      "Epoch 31/200\n",
      "550/550 [==============================] - 196s 357ms/step - loss: 4.7830 - triplet_acc: 0.9943 - val_loss: 5.9601 - val_triplet_acc: 0.9898\n",
      "\n",
      "Epoch 00031: val_loss improved from 6.00270 to 5.96008, saving model to TANH32-1.h5\n",
      "Epoch 32/200\n",
      "550/550 [==============================] - 185s 335ms/step - loss: 4.7797 - triplet_acc: 0.9937 - val_loss: 5.9119 - val_triplet_acc: 0.9900\n",
      "\n",
      "Epoch 00032: val_loss improved from 5.96008 to 5.91193, saving model to TANH32-1.h5\n",
      "Epoch 33/200\n",
      "550/550 [==============================] - 175s 319ms/step - loss: 4.6234 - triplet_acc: 0.9945 - val_loss: 6.1004 - val_triplet_acc: 0.9888\n",
      "\n",
      "Epoch 00033: val_loss did not improve\n",
      "Epoch 34/200\n",
      "550/550 [==============================] - 175s 318ms/step - loss: 4.4064 - triplet_acc: 0.9950 - val_loss: 5.8326 - val_triplet_acc: 0.9903\n",
      "\n",
      "Epoch 00034: val_loss improved from 5.91193 to 5.83257, saving model to TANH32-1.h5\n",
      "Epoch 35/200\n",
      "550/550 [==============================] - 176s 321ms/step - loss: 4.8702 - triplet_acc: 0.9932 - val_loss: 5.7470 - val_triplet_acc: 0.9902\n",
      "\n",
      "Epoch 00035: val_loss improved from 5.83257 to 5.74698, saving model to TANH32-1.h5\n",
      "Epoch 36/200\n",
      "550/550 [==============================] - 177s 322ms/step - loss: 4.5426 - triplet_acc: 0.9943 - val_loss: 5.6260 - val_triplet_acc: 0.9906\n",
      "\n",
      "Epoch 00036: val_loss improved from 5.74698 to 5.62603, saving model to TANH32-1.h5\n",
      "Epoch 37/200\n",
      "550/550 [==============================] - 177s 321ms/step - loss: 4.3414 - triplet_acc: 0.9943 - val_loss: 5.8007 - val_triplet_acc: 0.9900\n",
      "\n",
      "Epoch 00037: val_loss did not improve\n",
      "Epoch 38/200\n",
      "550/550 [==============================] - 176s 321ms/step - loss: 4.3819 - triplet_acc: 0.9947 - val_loss: 5.8793 - val_triplet_acc: 0.9892\n",
      "\n",
      "Epoch 00038: val_loss did not improve\n",
      "Epoch 39/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 4.3473 - triplet_acc: 0.9948 - val_loss: 5.8010 - val_triplet_acc: 0.9896\n",
      "\n",
      "Epoch 00039: val_loss did not improve\n",
      "Epoch 40/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 4.6682 - triplet_acc: 0.9936 - val_loss: 5.7729 - val_triplet_acc: 0.9896\n",
      "\n",
      "Epoch 00040: val_loss did not improve\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 41/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 4.0959 - triplet_acc: 0.9959 - val_loss: 5.6037 - val_triplet_acc: 0.9903\n",
      "\n",
      "Epoch 00041: val_loss improved from 5.62603 to 5.60370, saving model to TANH32-1.h5\n",
      "Epoch 42/200\n",
      "550/550 [==============================] - 176s 321ms/step - loss: 4.1827 - triplet_acc: 0.9953 - val_loss: 5.5209 - val_triplet_acc: 0.9904\n",
      "\n",
      "Epoch 00042: val_loss improved from 5.60370 to 5.52085, saving model to TANH32-1.h5\n",
      "Epoch 43/200\n",
      "550/550 [==============================] - 175s 319ms/step - loss: 4.2981 - triplet_acc: 0.9950 - val_loss: 5.4616 - val_triplet_acc: 0.9908\n",
      "\n",
      "Epoch 00043: val_loss improved from 5.52085 to 5.46159, saving model to TANH32-1.h5\n",
      "Epoch 44/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 4.1865 - triplet_acc: 0.9953 - val_loss: 5.5046 - val_triplet_acc: 0.9905\n",
      "\n",
      "Epoch 00044: val_loss did not improve\n",
      "Epoch 45/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 4.1834 - triplet_acc: 0.9951 - val_loss: 5.4221 - val_triplet_acc: 0.9904\n",
      "\n",
      "Epoch 00045: val_loss improved from 5.46159 to 5.42208, saving model to TANH32-1.h5\n",
      "Epoch 46/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 4.2831 - triplet_acc: 0.9952 - val_loss: 5.3944 - val_triplet_acc: 0.9911\n",
      "\n",
      "Epoch 00046: val_loss improved from 5.42208 to 5.39440, saving model to TANH32-1.h5\n",
      "Epoch 47/200\n",
      "550/550 [==============================] - 175s 319ms/step - loss: 4.2080 - triplet_acc: 0.9948 - val_loss: 5.4116 - val_triplet_acc: 0.9907\n",
      "\n",
      "Epoch 00047: val_loss did not improve\n",
      "Epoch 48/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 4.0630 - triplet_acc: 0.9952 - val_loss: 5.3936 - val_triplet_acc: 0.9910\n",
      "\n",
      "Epoch 00048: val_loss improved from 5.39440 to 5.39359, saving model to TANH32-1.h5\n",
      "Epoch 49/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 4.0115 - triplet_acc: 0.9957 - val_loss: 5.4073 - val_triplet_acc: 0.9909\n",
      "\n",
      "Epoch 00049: val_loss did not improve\n",
      "Epoch 50/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 4.0260 - triplet_acc: 0.9957 - val_loss: 5.3940 - val_triplet_acc: 0.9908\n",
      "\n",
      "Epoch 00050: val_loss did not improve\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 51/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 3.9521 - triplet_acc: 0.9958 - val_loss: 5.3779 - val_triplet_acc: 0.9906\n",
      "\n",
      "Epoch 00051: val_loss improved from 5.39359 to 5.37786, saving model to TANH32-1.h5\n",
      "Epoch 52/200\n",
      "550/550 [==============================] - 175s 319ms/step - loss: 3.9644 - triplet_acc: 0.9961 - val_loss: 5.3451 - val_triplet_acc: 0.9905\n",
      "\n",
      "Epoch 00052: val_loss improved from 5.37786 to 5.34515, saving model to TANH32-1.h5\n",
      "Epoch 53/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 4.1203 - triplet_acc: 0.9952 - val_loss: 5.3459 - val_triplet_acc: 0.9908\n",
      "\n",
      "Epoch 00053: val_loss did not improve\n",
      "Epoch 54/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 4.0151 - triplet_acc: 0.9959 - val_loss: 5.3405 - val_triplet_acc: 0.9908\n",
      "\n",
      "Epoch 00054: val_loss improved from 5.34515 to 5.34048, saving model to TANH32-1.h5\n",
      "Epoch 55/200\n",
      "550/550 [==============================] - 176s 321ms/step - loss: 4.2567 - triplet_acc: 0.9947 - val_loss: 5.3422 - val_triplet_acc: 0.9909\n",
      "\n",
      "Epoch 00055: val_loss did not improve\n",
      "Epoch 56/200\n",
      "550/550 [==============================] - 175s 319ms/step - loss: 4.2278 - triplet_acc: 0.9953 - val_loss: 5.3670 - val_triplet_acc: 0.9912\n",
      "\n",
      "Epoch 00056: val_loss did not improve\n",
      "Epoch 57/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 3.8593 - triplet_acc: 0.9965 - val_loss: 5.3358 - val_triplet_acc: 0.9910\n",
      "\n",
      "Epoch 00057: val_loss improved from 5.34048 to 5.33582, saving model to TANH32-1.h5\n",
      "Epoch 58/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 3.9701 - triplet_acc: 0.9959 - val_loss: 5.3457 - val_triplet_acc: 0.9908\n",
      "\n",
      "Epoch 00058: val_loss did not improve\n",
      "Epoch 59/200\n",
      "550/550 [==============================] - 177s 321ms/step - loss: 4.1848 - triplet_acc: 0.9951 - val_loss: 5.3432 - val_triplet_acc: 0.9910\n",
      "\n",
      "Epoch 00059: val_loss did not improve\n",
      "Epoch 60/200\n",
      "550/550 [==============================] - 177s 321ms/step - loss: 3.9020 - triplet_acc: 0.9961 - val_loss: 9.8319 - val_triplet_acc: 0.9744\n",
      "\n",
      "Epoch 00060: val_loss did not improve\n",
      "Epoch 61/200\n",
      "550/550 [==============================] - 178s 323ms/step - loss: 3.9967 - triplet_acc: 0.9958 - val_loss: 5.3432 - val_triplet_acc: 0.9912\n",
      "\n",
      "Epoch 00061: val_loss did not improve\n",
      "\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "Epoch 62/200\n",
      "550/550 [==============================] - 177s 323ms/step - loss: 4.2134 - triplet_acc: 0.9952 - val_loss: 5.4414 - val_triplet_acc: 0.9906\n",
      "\n",
      "Epoch 00062: val_loss did not improve\n",
      "Epoch 63/200\n",
      "550/550 [==============================] - 175s 319ms/step - loss: 3.8857 - triplet_acc: 0.9958 - val_loss: 5.3371 - val_triplet_acc: 0.9913\n",
      "\n",
      "Epoch 00063: val_loss did not improve\n",
      "Epoch 64/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 3.9972 - triplet_acc: 0.9959 - val_loss: 5.3439 - val_triplet_acc: 0.9911\n",
      "\n",
      "Epoch 00064: val_loss did not improve\n",
      "Epoch 65/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 3.9756 - triplet_acc: 0.9957 - val_loss: 5.3389 - val_triplet_acc: 0.9911\n",
      "\n",
      "Epoch 00065: val_loss did not improve\n",
      "Epoch 66/200\n",
      "550/550 [==============================] - 176s 319ms/step - loss: 4.0456 - triplet_acc: 0.9956 - val_loss: 5.3357 - val_triplet_acc: 0.9912\n",
      "\n",
      "Epoch 00066: val_loss improved from 5.33582 to 5.33568, saving model to TANH32-1.h5\n",
      "Epoch 67/200\n",
      "550/550 [==============================] - 176s 321ms/step - loss: 4.0335 - triplet_acc: 0.9955 - val_loss: 5.4838 - val_triplet_acc: 0.9905\n",
      "\n",
      "Epoch 00067: val_loss did not improve\n",
      "Epoch 68/200\n",
      "550/550 [==============================] - 181s 330ms/step - loss: 4.0381 - triplet_acc: 0.9955 - val_loss: 5.3323 - val_triplet_acc: 0.9910\n",
      "\n",
      "Epoch 00068: val_loss improved from 5.33568 to 5.33226, saving model to TANH32-1.h5\n",
      "Epoch 69/200\n",
      "550/550 [==============================] - 193s 351ms/step - loss: 3.9205 - triplet_acc: 0.9965 - val_loss: 5.3361 - val_triplet_acc: 0.9911\n",
      "\n",
      "Epoch 00069: val_loss did not improve\n",
      "Epoch 70/200\n",
      "550/550 [==============================] - 210s 381ms/step - loss: 4.0904 - triplet_acc: 0.9955 - val_loss: 5.3294 - val_triplet_acc: 0.9912\n",
      "\n",
      "Epoch 00070: val_loss improved from 5.33226 to 5.32936, saving model to TANH32-1.h5\n",
      "Epoch 71/200\n",
      "550/550 [==============================] - 202s 366ms/step - loss: 4.0346 - triplet_acc: 0.9959 - val_loss: 94.8569 - val_triplet_acc: 0.6908\n",
      "\n",
      "Epoch 00071: val_loss did not improve\n",
      "Epoch 72/200\n",
      "550/550 [==============================] - 206s 374ms/step - loss: 4.0937 - triplet_acc: 0.9949 - val_loss: 5.3315 - val_triplet_acc: 0.9911\n",
      "\n",
      "Epoch 00072: val_loss did not improve\n",
      "Epoch 73/200\n",
      "550/550 [==============================] - 201s 365ms/step - loss: 3.9028 - triplet_acc: 0.9964 - val_loss: 5.3246 - val_triplet_acc: 0.9911\n",
      "\n",
      "Epoch 00073: val_loss improved from 5.32936 to 5.32457, saving model to TANH32-1.h5\n",
      "Epoch 74/200\n",
      "550/550 [==============================] - 201s 365ms/step - loss: 4.1128 - triplet_acc: 0.9955 - val_loss: 5.3271 - val_triplet_acc: 0.9911\n",
      "\n",
      "Epoch 00074: val_loss did not improve\n",
      "Epoch 75/200\n",
      "550/550 [==============================] - 206s 375ms/step - loss: 3.5969 - triplet_acc: 0.9976 - val_loss: 5.3214 - val_triplet_acc: 0.9912\n",
      "\n",
      "Epoch 00075: val_loss improved from 5.32457 to 5.32135, saving model to TANH32-1.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/200\n",
      "550/550 [==============================] - 206s 375ms/step - loss: 4.0010 - triplet_acc: 0.9959 - val_loss: 5.3199 - val_triplet_acc: 0.9911\n",
      "\n",
      "Epoch 00076: val_loss improved from 5.32135 to 5.31995, saving model to TANH32-1.h5\n",
      "Epoch 77/200\n",
      "550/550 [==============================] - 195s 355ms/step - loss: 4.0996 - triplet_acc: 0.9953 - val_loss: 101.1303 - val_triplet_acc: 0.5775\n",
      "\n",
      "Epoch 00077: val_loss did not improve\n",
      "Epoch 78/200\n",
      "550/550 [==============================] - 217s 394ms/step - loss: 4.0121 - triplet_acc: 0.9961 - val_loss: 5.3153 - val_triplet_acc: 0.9911\n",
      "\n",
      "Epoch 00078: val_loss improved from 5.31995 to 5.31532, saving model to TANH32-1.h5\n",
      "Epoch 79/200\n",
      "550/550 [==============================] - 206s 374ms/step - loss: 4.0338 - triplet_acc: 0.9958 - val_loss: 6.6662 - val_triplet_acc: 0.9867\n",
      "\n",
      "Epoch 00079: val_loss did not improve\n",
      "Epoch 80/200\n",
      "550/550 [==============================] - 209s 379ms/step - loss: 4.0262 - triplet_acc: 0.9956 - val_loss: 5.3156 - val_triplet_acc: 0.9911\n",
      "\n",
      "Epoch 00080: val_loss did not improve\n",
      "Epoch 81/200\n",
      "550/550 [==============================] - 206s 374ms/step - loss: 3.9335 - triplet_acc: 0.9957 - val_loss: 5.3143 - val_triplet_acc: 0.9912\n",
      "\n",
      "Epoch 00082: val_loss did not improve\n",
      "Epoch 83/200\n",
      "550/550 [==============================] - 206s 375ms/step - loss: 4.0284 - triplet_acc: 0.9958 - val_loss: 5.3156 - val_triplet_acc: 0.9911\n",
      "\n",
      "Epoch 00083: val_loss did not improve\n",
      "Epoch 84/200\n",
      "550/550 [==============================] - 206s 375ms/step - loss: 3.8397 - triplet_acc: 0.9966 - val_loss: 5.3217 - val_triplet_acc: 0.9912\n",
      "\n",
      "Epoch 00084: val_loss did not improve\n",
      "Epoch 85/200\n",
      "550/550 [==============================] - 207s 377ms/step - loss: 3.9174 - triplet_acc: 0.9965 - val_loss: 5.3126 - val_triplet_acc: 0.9912\n",
      "\n",
      "Epoch 00085: val_loss did not improve\n",
      "Epoch 86/200\n",
      "550/550 [==============================] - 204s 371ms/step - loss: 3.8905 - triplet_acc: 0.9961 - val_loss: 5.3144 - val_triplet_acc: 0.9912\n",
      "\n",
      "Epoch 00086: val_loss did not improve\n",
      "Epoch 87/200\n",
      "550/550 [==============================] - 207s 375ms/step - loss: 4.0093 - triplet_acc: 0.9957 - val_loss: 5.3179 - val_triplet_acc: 0.9911\n",
      "\n",
      "Epoch 00087: val_loss did not improve\n",
      "Epoch 88/200\n",
      "550/550 [==============================] - 202s 367ms/step - loss: 3.7492 - triplet_acc: 0.9972 - val_loss: 5.3215 - val_triplet_acc: 0.9913\n",
      "\n",
      "Epoch 00088: val_loss did not improve\n",
      "Epoch 89/200\n",
      "550/550 [==============================] - 202s 368ms/step - loss: 4.0308 - triplet_acc: 0.9959 - val_loss: 5.3154 - val_triplet_acc: 0.9910\n",
      "\n",
      "Epoch 00089: val_loss did not improve\n",
      "Epoch 90/200\n",
      "550/550 [==============================] - 182s 331ms/step - loss: 4.1247 - triplet_acc: 0.9952 - val_loss: 5.3109 - val_triplet_acc: 0.9912\n",
      "\n",
      "Epoch 00090: val_loss improved from 5.31201 to 5.31092, saving model to TANH32-1.h5\n",
      "Epoch 91/200\n",
      "550/550 [==============================] - 183s 332ms/step - loss: 3.9681 - triplet_acc: 0.9960 - val_loss: 6.8442 - val_triplet_acc: 0.9853\n",
      "\n",
      "Epoch 00091: val_loss did not improve\n",
      "Epoch 92/200\n",
      "550/550 [==============================] - 179s 325ms/step - loss: 3.7782 - triplet_acc: 0.9970 - val_loss: 5.3098 - val_triplet_acc: 0.9909\n",
      "\n",
      "Epoch 00092: val_loss improved from 5.31092 to 5.30984, saving model to TANH32-1.h5\n",
      "Epoch 93/200\n",
      "550/550 [==============================] - 179s 325ms/step - loss: 3.9060 - triplet_acc: 0.9961 - val_loss: 5.3096 - val_triplet_acc: 0.9912\n",
      "\n",
      "Epoch 00093: val_loss improved from 5.30984 to 5.30963, saving model to TANH32-1.h5\n",
      "Epoch 94/200\n",
      "550/550 [==============================] - 179s 326ms/step - loss: 3.9642 - triplet_acc: 0.9962 - val_loss: 5.2992 - val_triplet_acc: 0.9912\n",
      "\n",
      "Epoch 00094: val_loss improved from 5.30963 to 5.29920, saving model to TANH32-1.h5\n",
      "Epoch 95/200\n",
      "550/550 [==============================] - 179s 325ms/step - loss: 3.8857 - triplet_acc: 0.9963 - val_loss: 5.3432 - val_triplet_acc: 0.9913\n",
      "\n",
      "Epoch 00095: val_loss did not improve\n",
      "Epoch 96/200\n",
      "550/550 [==============================] - 179s 325ms/step - loss: 4.0843 - triplet_acc: 0.9950 - val_loss: 5.2961 - val_triplet_acc: 0.9912\n",
      "\n",
      "Epoch 00096: val_loss improved from 5.29920 to 5.29610, saving model to TANH32-1.h5\n",
      "Epoch 97/200\n",
      "550/550 [==============================] - 180s 327ms/step - loss: 3.9315 - triplet_acc: 0.9960 - val_loss: 102.8835 - val_triplet_acc: 0.9984\n",
      "\n",
      "Epoch 00097: val_loss did not improve\n",
      "Epoch 98/200\n",
      "550/550 [==============================] - 177s 321ms/step - loss: 3.9452 - triplet_acc: 0.9956 - val_loss: 5.2941 - val_triplet_acc: 0.9913\n",
      "\n",
      "Epoch 00098: val_loss improved from 5.29610 to 5.29412, saving model to TANH32-1.h5\n",
      "Epoch 99/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 3.9213 - triplet_acc: 0.9959 - val_loss: 5.2948 - val_triplet_acc: 0.9912\n",
      "\n",
      "Epoch 00099: val_loss did not improve\n",
      "Epoch 100/200\n",
      "550/550 [==============================] - 175s 318ms/step - loss: 3.8313 - triplet_acc: 0.9968 - val_loss: 5.4156 - val_triplet_acc: 0.9913\n",
      "\n",
      "Epoch 00100: val_loss did not improve\n",
      "Epoch 101/200\n",
      "550/550 [==============================] - 176s 321ms/step - loss: 4.0587 - triplet_acc: 0.9955 - val_loss: 5.2964 - val_triplet_acc: 0.9911\n",
      "\n",
      "Epoch 00101: val_loss did not improve\n",
      "Epoch 102/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 3.8990 - triplet_acc: 0.9962 - val_loss: 5.3000 - val_triplet_acc: 0.9912\n",
      "\n",
      "Epoch 00102: val_loss did not improve\n",
      "Epoch 103/200\n",
      "550/550 [==============================] - 176s 319ms/step - loss: 3.9328 - triplet_acc: 0.9961 - val_loss: 5.2965 - val_triplet_acc: 0.9911\n",
      "\n",
      "Epoch 00103: val_loss did not improve\n",
      "Epoch 104/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 4.0307 - triplet_acc: 0.9957 - val_loss: 5.3139 - val_triplet_acc: 0.9912\n",
      "\n",
      "Epoch 00104: val_loss did not improve\n",
      "Epoch 105/200\n",
      "550/550 [==============================] - 182s 330ms/step - loss: 3.9652 - triplet_acc: 0.9957 - val_loss: 5.2945 - val_triplet_acc: 0.9912\n",
      "\n",
      "Epoch 00105: val_loss did not improve\n",
      "Epoch 106/200\n",
      "550/550 [==============================] - 182s 330ms/step - loss: 3.8457 - triplet_acc: 0.9964 - val_loss: 5.2964 - val_triplet_acc: 0.9913\n",
      "\n",
      "Epoch 00106: val_loss did not improve\n",
      "Epoch 107/200\n",
      "550/550 [==============================] - 179s 325ms/step - loss: 4.0921 - triplet_acc: 0.9959 - val_loss: 5.2949 - val_triplet_acc: 0.9913\n",
      "\n",
      "Epoch 00107: val_loss did not improve\n",
      "Epoch 108/200\n",
      "550/550 [==============================] - 176s 319ms/step - loss: 4.0706 - triplet_acc: 0.9952 - val_loss: 5.3011 - val_triplet_acc: 0.9912\n",
      "\n",
      "Epoch 00108: val_loss did not improve\n",
      "Epoch 00108: early stopping\n",
      "TANH32-2.h5\n",
      "Epoch 1/200\n",
      "550/550 [==============================] - 188s 342ms/step - loss: 8.0560 - triplet_acc: 0.9807 - val_loss: 102.6184 - val_triplet_acc: 0.6092\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 102.61838, saving model to TANH32-2.h5\n",
      "Epoch 2/200\n",
      "550/550 [==============================] - 175s 319ms/step - loss: 8.4058 - triplet_acc: 0.9811 - val_loss: 11.1319 - val_triplet_acc: 0.9722\n",
      "\n",
      "Epoch 00002: val_loss improved from 102.61838 to 11.13190, saving model to TANH32-2.h5\n",
      "Epoch 3/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 8.3915 - triplet_acc: 0.9824 - val_loss: 65.3160 - val_triplet_acc: 0.7370\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/200\n",
      "550/550 [==============================] - 175s 318ms/step - loss: 8.4790 - triplet_acc: 0.9824 - val_loss: 29.5047 - val_triplet_acc: 0.9047\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/200\n",
      "550/550 [==============================] - 175s 318ms/step - loss: 7.7448 - triplet_acc: 0.9851 - val_loss: 51.2958 - val_triplet_acc: 0.8027\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/200\n",
      "550/550 [==============================] - 174s 317ms/step - loss: 7.8150 - triplet_acc: 0.9859 - val_loss: 10.4474 - val_triplet_acc: 0.9762\n",
      "\n",
      "Epoch 00006: val_loss improved from 11.13190 to 10.44739, saving model to TANH32-2.h5\n",
      "Epoch 7/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 7.6655 - triplet_acc: 0.9855 - val_loss: 8.9786 - val_triplet_acc: 0.9827\n",
      "\n",
      "Epoch 00007: val_loss improved from 10.44739 to 8.97858, saving model to TANH32-2.h5\n",
      "Epoch 8/200\n",
      "550/550 [==============================] - 175s 319ms/step - loss: 7.9650 - triplet_acc: 0.9852 - val_loss: 103.4707 - val_triplet_acc: 0.6481\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 8.0024 - triplet_acc: 0.9839 - val_loss: 10.7483 - val_triplet_acc: 0.9772\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/200\n",
      "550/550 [==============================] - 175s 318ms/step - loss: 8.0201 - triplet_acc: 0.9847 - val_loss: 103.9488 - val_triplet_acc: 0.7941\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 8.2090 - triplet_acc: 0.9841 - val_loss: 10.2537 - val_triplet_acc: 0.9765\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 12/200\n",
      "550/550 [==============================] - 175s 319ms/step - loss: 6.6458 - triplet_acc: 0.9895 - val_loss: 59.4202 - val_triplet_acc: 0.7560\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/200\n",
      "550/550 [==============================] - 175s 318ms/step - loss: 5.8128 - triplet_acc: 0.9918 - val_loss: 7.1256 - val_triplet_acc: 0.9871\n",
      "\n",
      "Epoch 00013: val_loss improved from 8.97858 to 7.12559, saving model to TANH32-2.h5\n",
      "Epoch 14/200\n",
      "550/550 [==============================] - 176s 319ms/step - loss: 5.5328 - triplet_acc: 0.9926 - val_loss: 102.5116 - val_triplet_acc: 0.6124\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/200\n",
      "550/550 [==============================] - 176s 319ms/step - loss: 5.3278 - triplet_acc: 0.9923 - val_loss: 13.9470 - val_triplet_acc: 0.9635\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/200\n",
      "550/550 [==============================] - 176s 319ms/step - loss: 4.9923 - triplet_acc: 0.9939 - val_loss: 6.1257 - val_triplet_acc: 0.9895\n",
      "\n",
      "Epoch 00016: val_loss improved from 7.12559 to 6.12572, saving model to TANH32-2.h5\n",
      "Epoch 17/200\n",
      "550/550 [==============================] - 175s 318ms/step - loss: 4.9918 - triplet_acc: 0.9927 - val_loss: 6.2640 - val_triplet_acc: 0.9887\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/200\n",
      "550/550 [==============================] - 175s 319ms/step - loss: 4.6067 - triplet_acc: 0.9937 - val_loss: 6.1655 - val_triplet_acc: 0.9881\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/200\n",
      "550/550 [==============================] - 175s 319ms/step - loss: 4.6773 - triplet_acc: 0.9936 - val_loss: 9.3142 - val_triplet_acc: 0.9778\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/200\n",
      "550/550 [==============================] - 175s 318ms/step - loss: 4.6381 - triplet_acc: 0.9932 - val_loss: 6.2014 - val_triplet_acc: 0.9875\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 21/200\n",
      "550/550 [==============================] - 175s 319ms/step - loss: 4.3548 - triplet_acc: 0.9944 - val_loss: 5.8427 - val_triplet_acc: 0.9890\n",
      "\n",
      "Epoch 00021: val_loss improved from 6.12572 to 5.84272, saving model to TANH32-2.h5\n",
      "Epoch 22/200\n",
      "550/550 [==============================] - 177s 321ms/step - loss: 4.3298 - triplet_acc: 0.9940 - val_loss: 102.7760 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/200\n",
      "550/550 [==============================] - 175s 319ms/step - loss: 4.0960 - triplet_acc: 0.9946 - val_loss: 99.0392 - val_triplet_acc: 0.5935\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 24/200\n",
      "550/550 [==============================] - 175s 318ms/step - loss: 4.4718 - triplet_acc: 0.9924 - val_loss: 5.3733 - val_triplet_acc: 0.9903\n",
      "\n",
      "Epoch 00024: val_loss improved from 5.84272 to 5.37326, saving model to TANH32-2.h5\n",
      "Epoch 25/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 4.2790 - triplet_acc: 0.9941 - val_loss: 5.6424 - val_triplet_acc: 0.9891\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 26/200\n",
      "550/550 [==============================] - 175s 319ms/step - loss: 3.9002 - triplet_acc: 0.9958 - val_loss: 5.3723 - val_triplet_acc: 0.9901\n",
      "\n",
      "Epoch 00026: val_loss improved from 5.37326 to 5.37232, saving model to TANH32-2.h5\n",
      "Epoch 27/200\n",
      "550/550 [==============================] - 175s 317ms/step - loss: 3.7977 - triplet_acc: 0.9959 - val_loss: 82.0794 - val_triplet_acc: 0.7770\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 28/200\n",
      "550/550 [==============================] - 175s 318ms/step - loss: 4.0156 - triplet_acc: 0.9941 - val_loss: 5.2103 - val_triplet_acc: 0.9903\n",
      "\n",
      "Epoch 00028: val_loss improved from 5.37232 to 5.21032, saving model to TANH32-2.h5\n",
      "Epoch 29/200\n",
      "550/550 [==============================] - 175s 319ms/step - loss: 3.7813 - triplet_acc: 0.9954 - val_loss: 5.0666 - val_triplet_acc: 0.9908\n",
      "\n",
      "Epoch 00029: val_loss improved from 5.21032 to 5.06662, saving model to TANH32-2.h5\n",
      "Epoch 30/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 3.6902 - triplet_acc: 0.9959 - val_loss: 5.1557 - val_triplet_acc: 0.9907\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      "Epoch 31/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 3.7842 - triplet_acc: 0.9959 - val_loss: 5.0343 - val_triplet_acc: 0.9906\n",
      "\n",
      "Epoch 00031: val_loss improved from 5.06662 to 5.03428, saving model to TANH32-2.h5\n",
      "Epoch 32/200\n",
      "550/550 [==============================] - 176s 319ms/step - loss: 3.6890 - triplet_acc: 0.9953 - val_loss: 5.0590 - val_triplet_acc: 0.9905\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 33/200\n",
      "550/550 [==============================] - 176s 319ms/step - loss: 3.7335 - triplet_acc: 0.9949 - val_loss: 100.0973 - val_triplet_acc: 0.7419\n",
      "\n",
      "Epoch 00033: val_loss did not improve\n",
      "Epoch 34/200\n",
      "550/550 [==============================] - 176s 321ms/step - loss: 3.6567 - triplet_acc: 0.9957 - val_loss: 5.0604 - val_triplet_acc: 0.9905\n",
      "\n",
      "Epoch 00034: val_loss did not improve\n",
      "Epoch 35/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 3.7065 - triplet_acc: 0.9952 - val_loss: 5.0325 - val_triplet_acc: 0.9908\n",
      "\n",
      "Epoch 00035: val_loss improved from 5.03428 to 5.03246, saving model to TANH32-2.h5\n",
      "Epoch 36/200\n",
      "550/550 [==============================] - 176s 319ms/step - loss: 3.5826 - triplet_acc: 0.9956 - val_loss: 4.9365 - val_triplet_acc: 0.9913\n",
      "\n",
      "Epoch 00036: val_loss improved from 5.03246 to 4.93646, saving model to TANH32-2.h5\n",
      "Epoch 37/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 3.6594 - triplet_acc: 0.9951 - val_loss: 6.9700 - val_triplet_acc: 0.9831\n",
      "\n",
      "Epoch 00037: val_loss did not improve\n",
      "Epoch 38/200\n",
      "550/550 [==============================] - 174s 317ms/step - loss: 3.7022 - triplet_acc: 0.9952 - val_loss: 4.9691 - val_triplet_acc: 0.9904\n",
      "\n",
      "Epoch 00038: val_loss did not improve\n",
      "Epoch 39/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 3.6968 - triplet_acc: 0.9951 - val_loss: 4.9501 - val_triplet_acc: 0.9911\n",
      "\n",
      "Epoch 00039: val_loss did not improve\n",
      "Epoch 40/200\n",
      "550/550 [==============================] - 175s 318ms/step - loss: 3.7652 - triplet_acc: 0.9947 - val_loss: 5.0811 - val_triplet_acc: 0.9901\n",
      "\n",
      "Epoch 00040: val_loss did not improve\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 41/200\n",
      "550/550 [==============================] - 175s 318ms/step - loss: 3.5130 - triplet_acc: 0.9955 - val_loss: 12.5191 - val_triplet_acc: 0.9621\n",
      "\n",
      "Epoch 00041: val_loss did not improve\n",
      "Epoch 42/200\n",
      "550/550 [==============================] - 176s 319ms/step - loss: 3.5106 - triplet_acc: 0.9950 - val_loss: 101.2542 - val_triplet_acc: 0.7367\n",
      "\n",
      "Epoch 00042: val_loss did not improve\n",
      "Epoch 43/200\n",
      "550/550 [==============================] - 176s 319ms/step - loss: 3.5162 - triplet_acc: 0.9952 - val_loss: 4.9230 - val_triplet_acc: 0.9908\n",
      "\n",
      "Epoch 00043: val_loss improved from 4.93646 to 4.92298, saving model to TANH32-2.h5\n",
      "Epoch 44/200\n",
      "550/550 [==============================] - 174s 317ms/step - loss: 3.2214 - triplet_acc: 0.9969 - val_loss: 66.3000 - val_triplet_acc: 0.7445\n",
      "\n",
      "Epoch 00044: val_loss did not improve\n",
      "Epoch 45/200\n",
      "550/550 [==============================] - 175s 319ms/step - loss: 3.3662 - triplet_acc: 0.9962 - val_loss: 5.6406 - val_triplet_acc: 0.9880\n",
      "\n",
      "Epoch 00045: val_loss did not improve\n",
      "Epoch 46/200\n",
      "550/550 [==============================] - 176s 321ms/step - loss: 3.4161 - triplet_acc: 0.9957 - val_loss: 100.7873 - val_triplet_acc: 0.6576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00046: val_loss did not improve\n",
      "Epoch 47/200\n",
      "550/550 [==============================] - 175s 318ms/step - loss: 3.2832 - triplet_acc: 0.9963 - val_loss: 4.7529 - val_triplet_acc: 0.9914\n",
      "\n",
      "Epoch 00047: val_loss improved from 4.92298 to 4.75291, saving model to TANH32-2.h5\n",
      "Epoch 48/200\n",
      "550/550 [==============================] - 175s 318ms/step - loss: 3.3237 - triplet_acc: 0.9960 - val_loss: 4.7305 - val_triplet_acc: 0.9915\n",
      "\n",
      "Epoch 00048: val_loss improved from 4.75291 to 4.73049, saving model to TANH32-2.h5\n",
      "Epoch 49/200\n",
      "550/550 [==============================] - 175s 318ms/step - loss: 3.5032 - triplet_acc: 0.9953 - val_loss: 101.5633 - val_triplet_acc: 0.6030\n",
      "\n",
      "Epoch 00049: val_loss did not improve\n",
      "Epoch 50/200\n",
      "550/550 [==============================] - 175s 318ms/step - loss: 3.3573 - triplet_acc: 0.9961 - val_loss: 4.6730 - val_triplet_acc: 0.9914\n",
      "\n",
      "Epoch 00050: val_loss improved from 4.73049 to 4.67301, saving model to TANH32-2.h5\n",
      "Epoch 51/200\n",
      "550/550 [==============================] - 175s 318ms/step - loss: 3.3506 - triplet_acc: 0.9963 - val_loss: 78.3702 - val_triplet_acc: 0.6985\n",
      "\n",
      "Epoch 00051: val_loss did not improve\n",
      "Epoch 52/200\n",
      "550/550 [==============================] - 175s 319ms/step - loss: 3.0896 - triplet_acc: 0.9970 - val_loss: 97.5509 - val_triplet_acc: 0.7226\n",
      "\n",
      "Epoch 00052: val_loss did not improve\n",
      "Epoch 53/200\n",
      "550/550 [==============================] - 175s 319ms/step - loss: 3.2736 - triplet_acc: 0.9965 - val_loss: 4.6991 - val_triplet_acc: 0.9915\n",
      "\n",
      "Epoch 00053: val_loss did not improve\n",
      "Epoch 54/200\n",
      "550/550 [==============================] - 176s 321ms/step - loss: 3.5785 - triplet_acc: 0.9951 - val_loss: 4.5465 - val_triplet_acc: 0.9918\n",
      "\n",
      "Epoch 00054: val_loss improved from 4.67301 to 4.54647, saving model to TANH32-2.h5\n",
      "Epoch 55/200\n",
      "550/550 [==============================] - 175s 318ms/step - loss: 3.2902 - triplet_acc: 0.9960 - val_loss: 4.5725 - val_triplet_acc: 0.9917\n",
      "\n",
      "Epoch 00055: val_loss did not improve\n",
      "Epoch 56/200\n",
      "550/550 [==============================] - 175s 317ms/step - loss: 3.0876 - triplet_acc: 0.9968 - val_loss: 4.5636 - val_triplet_acc: 0.9919\n",
      "\n",
      "Epoch 00056: val_loss did not improve\n",
      "Epoch 57/200\n",
      "550/550 [==============================] - 174s 317ms/step - loss: 2.9900 - triplet_acc: 0.9972 - val_loss: 49.4310 - val_triplet_acc: 0.8170\n",
      "\n",
      "Epoch 00057: val_loss did not improve\n",
      "Epoch 58/200\n",
      "550/550 [==============================] - 175s 318ms/step - loss: 3.3086 - triplet_acc: 0.9957 - val_loss: 102.2583 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00058: val_loss did not improve\n",
      "\n",
      "Epoch 00058: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 59/200\n",
      "550/550 [==============================] - 175s 319ms/step - loss: 3.4287 - triplet_acc: 0.9953 - val_loss: 4.5404 - val_triplet_acc: 0.9918\n",
      "\n",
      "Epoch 00059: val_loss improved from 4.54647 to 4.54045, saving model to TANH32-2.h5\n",
      "Epoch 60/200\n",
      "550/550 [==============================] - 175s 319ms/step - loss: 3.1922 - triplet_acc: 0.9961 - val_loss: 5.1207 - val_triplet_acc: 0.9898\n",
      "\n",
      "Epoch 00060: val_loss did not improve\n",
      "Epoch 61/200\n",
      "550/550 [==============================] - 175s 317ms/step - loss: 3.2091 - triplet_acc: 0.9965 - val_loss: 4.5504 - val_triplet_acc: 0.9916\n",
      "\n",
      "Epoch 00061: val_loss did not improve\n",
      "Epoch 62/200\n",
      "550/550 [==============================] - 175s 319ms/step - loss: 3.4846 - triplet_acc: 0.9951 - val_loss: 4.5441 - val_triplet_acc: 0.9916\n",
      "\n",
      "Epoch 00062: val_loss did not improve\n",
      "Epoch 63/200\n",
      "550/550 [==============================] - 175s 319ms/step - loss: 3.2791 - triplet_acc: 0.9960 - val_loss: 4.5121 - val_triplet_acc: 0.9920\n",
      "\n",
      "Epoch 00063: val_loss improved from 4.54045 to 4.51210, saving model to TANH32-2.h5\n",
      "Epoch 64/200\n",
      "550/550 [==============================] - 176s 319ms/step - loss: 3.3072 - triplet_acc: 0.9958 - val_loss: 4.4875 - val_triplet_acc: 0.9919\n",
      "\n",
      "Epoch 00064: val_loss improved from 4.51210 to 4.48753, saving model to TANH32-2.h5\n",
      "Epoch 65/200\n",
      "550/550 [==============================] - 175s 319ms/step - loss: 3.1512 - triplet_acc: 0.9966 - val_loss: 102.2496 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00065: val_loss did not improve\n",
      "Epoch 66/200\n",
      "550/550 [==============================] - 175s 319ms/step - loss: 3.1776 - triplet_acc: 0.9968 - val_loss: 4.4984 - val_triplet_acc: 0.9916\n",
      "\n",
      "Epoch 00066: val_loss did not improve\n",
      "Epoch 67/200\n",
      "550/550 [==============================] - 176s 319ms/step - loss: 3.2053 - triplet_acc: 0.9960 - val_loss: 4.4912 - val_triplet_acc: 0.9917\n",
      "\n",
      "Epoch 00067: val_loss did not improve\n",
      "Epoch 68/200\n",
      "550/550 [==============================] - 175s 317ms/step - loss: 3.2775 - triplet_acc: 0.9961 - val_loss: 102.2299 - val_triplet_acc: 0.7577\n",
      "\n",
      "Epoch 00068: val_loss did not improve\n",
      "\n",
      "Epoch 00068: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "Epoch 69/200\n",
      "550/550 [==============================] - 176s 319ms/step - loss: 3.4069 - triplet_acc: 0.9955 - val_loss: 4.4942 - val_triplet_acc: 0.9915\n",
      "\n",
      "Epoch 00069: val_loss did not improve\n",
      "Epoch 70/200\n",
      "242/550 [============>.................] - ETA: 1:14 - loss: 3.5416 - triplet_acc: 0.9946"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550/550 [==============================] - 175s 318ms/step - loss: 3.2086 - triplet_acc: 0.9963 - val_loss: 4.4830 - val_triplet_acc: 0.9918\n",
      "\n",
      "Epoch 00078: val_loss improved from 4.48314 to 4.48297, saving model to TANH32-2.h5\n",
      "Epoch 79/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 3.2232 - triplet_acc: 0.9961 - val_loss: 102.2424 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00079: val_loss did not improve\n",
      "Epoch 80/200\n",
      "550/550 [==============================] - 175s 318ms/step - loss: 3.1925 - triplet_acc: 0.9969 - val_loss: 4.4813 - val_triplet_acc: 0.9915\n",
      "\n",
      "Epoch 00080: val_loss improved from 4.48297 to 4.48127, saving model to TANH32-2.h5\n",
      "Epoch 81/200\n",
      "550/550 [==============================] - 175s 318ms/step - loss: 3.0962 - triplet_acc: 0.9969 - val_loss: 4.4803 - val_triplet_acc: 0.9915\n",
      "\n",
      "Epoch 00081: val_loss improved from 4.48127 to 4.48031, saving model to TANH32-2.h5\n",
      "Epoch 82/200\n",
      "550/550 [==============================] - 174s 317ms/step - loss: 3.2602 - triplet_acc: 0.9960 - val_loss: 4.4730 - val_triplet_acc: 0.9918\n",
      "\n",
      "Epoch 00082: val_loss improved from 4.48031 to 4.47296, saving model to TANH32-2.h5\n",
      "Epoch 83/200\n",
      "447/550 [=======================>......] - ETA: 25s - loss: 3.1365 - triplet_acc: 0.9966"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550/550 [==============================] - 175s 318ms/step - loss: 3.3164 - triplet_acc: 0.9958 - val_loss: 102.2387 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00090: val_loss did not improve\n",
      "Epoch 91/200\n",
      "550/550 [==============================] - 175s 318ms/step - loss: 3.2078 - triplet_acc: 0.9965 - val_loss: 76.6091 - val_triplet_acc: 0.7899\n",
      "\n",
      "Epoch 00091: val_loss did not improve\n",
      "Epoch 92/200\n",
      "550/550 [==============================] - 175s 318ms/step - loss: 3.2779 - triplet_acc: 0.9964 - val_loss: 4.4649 - val_triplet_acc: 0.9921\n",
      "\n",
      "Epoch 00092: val_loss improved from 4.47296 to 4.46486, saving model to TANH32-2.h5\n",
      "Epoch 93/200\n",
      "550/550 [==============================] - 175s 318ms/step - loss: 2.9709 - triplet_acc: 0.9972 - val_loss: 102.2377 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00093: val_loss did not improve\n",
      "Epoch 94/200\n",
      "550/550 [==============================] - 175s 319ms/step - loss: 3.1579 - triplet_acc: 0.9966 - val_loss: 4.4752 - val_triplet_acc: 0.9916\n",
      "\n",
      "Epoch 00094: val_loss did not improve\n",
      "Epoch 95/200\n",
      "550/550 [==============================] - 176s 319ms/step - loss: 3.0275 - triplet_acc: 0.9966 - val_loss: 4.4698 - val_triplet_acc: 0.9917\n",
      "\n",
      "Epoch 00095: val_loss did not improve\n",
      "Epoch 96/200\n",
      " 15/550 [..............................] - ETA: 1:40 - loss: 2.8293 - triplet_acc: 0.9979"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550/550 [==============================] - 175s 318ms/step - loss: 3.1744 - triplet_acc: 0.9967 - val_loss: 87.9087 - val_triplet_acc: 0.7376\n",
      "\n",
      "Epoch 00104: val_loss did not improve\n",
      "Epoch 105/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 3.3261 - triplet_acc: 0.9958 - val_loss: 44.4433 - val_triplet_acc: 0.8591\n",
      "\n",
      "Epoch 00105: val_loss did not improve\n",
      "Epoch 106/200\n",
      "550/550 [==============================] - 175s 318ms/step - loss: 3.1468 - triplet_acc: 0.9965 - val_loss: 4.4594 - val_triplet_acc: 0.9918\n",
      "\n",
      "Epoch 00106: val_loss did not improve\n",
      "Epoch 00106: early stopping\n",
      "TANH32-3.h5\n",
      "Epoch 1/200\n",
      "550/550 [==============================] - 188s 342ms/step - loss: 5.7770 - triplet_acc: 0.9867 - val_loss: 13.4658 - val_triplet_acc: 0.9630\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 13.46578, saving model to TANH32-3.h5\n",
      "Epoch 2/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 6.7799 - triplet_acc: 0.9841 - val_loss: 74.3112 - val_triplet_acc: 0.6890\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 3/200\n",
      "532/550 [============================>.] - ETA: 4s - loss: 6.0484 - triplet_acc: 0.9884"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550/550 [==============================] - 176s 320ms/step - loss: 6.5893 - triplet_acc: 0.9866 - val_loss: 17.0338 - val_triplet_acc: 0.9497\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 6.1605 - triplet_acc: 0.9887 - val_loss: 102.4558 - val_triplet_acc: 0.5989\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/200\n",
      "550/550 [==============================] - 176s 319ms/step - loss: 6.3226 - triplet_acc: 0.9878 - val_loss: 103.2271 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/200\n",
      "550/550 [==============================] - 175s 318ms/step - loss: 6.6786 - triplet_acc: 0.9868 - val_loss: 8.8490 - val_triplet_acc: 0.9808\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 15/200\n",
      "550/550 [==============================] - 176s 319ms/step - loss: 5.5142 - triplet_acc: 0.9907 - val_loss: 9.4288 - val_triplet_acc: 0.9784\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/200\n",
      "550/550 [==============================] - 176s 319ms/step - loss: 4.8375 - triplet_acc: 0.9930 - val_loss: 102.9822 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/200\n",
      " 44/550 [=>............................] - ETA: 1:41 - loss: 4.2227 - triplet_acc: 0.9957"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550/550 [==============================] - 176s 319ms/step - loss: 3.9480 - triplet_acc: 0.9952 - val_loss: 71.9395 - val_triplet_acc: 0.7498\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 26/200\n",
      "550/550 [==============================] - 175s 318ms/step - loss: 3.7793 - triplet_acc: 0.9956 - val_loss: 5.2544 - val_triplet_acc: 0.9912\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 27/200\n",
      "550/550 [==============================] - 175s 319ms/step - loss: 4.0282 - triplet_acc: 0.9948 - val_loss: 5.2560 - val_triplet_acc: 0.9909\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 28/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 3.9646 - triplet_acc: 0.9951 - val_loss: 5.2401 - val_triplet_acc: 0.9911\n",
      "\n",
      "Epoch 00028: val_loss improved from 5.24525 to 5.24008, saving model to TANH32-3.h5\n",
      "Epoch 29/200\n",
      "550/550 [==============================] - 176s 319ms/step - loss: 3.8452 - triplet_acc: 0.9961 - val_loss: 5.1984 - val_triplet_acc: 0.9912\n",
      "\n",
      "Epoch 00029: val_loss improved from 5.24008 to 5.19844, saving model to TANH32-3.h5\n",
      "Epoch 30/200\n",
      "385/550 [====================>.........] - ETA: 40s - loss: 3.9292 - triplet_acc: 0.9957"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550/550 [==============================] - 175s 319ms/step - loss: 3.9740 - triplet_acc: 0.9949 - val_loss: 102.6024 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00038: val_loss did not improve\n",
      "Epoch 39/200\n",
      "550/550 [==============================] - 176s 319ms/step - loss: 3.8281 - triplet_acc: 0.9954 - val_loss: 13.6023 - val_triplet_acc: 0.9590\n",
      "\n",
      "Epoch 00039: val_loss did not improve\n",
      "Epoch 40/200\n",
      "550/550 [==============================] - 175s 319ms/step - loss: 3.7004 - triplet_acc: 0.9960 - val_loss: 5.1064 - val_triplet_acc: 0.9909\n",
      "\n",
      "Epoch 00040: val_loss improved from 5.16138 to 5.10644, saving model to TANH32-3.h5\n",
      "Epoch 41/200\n",
      "550/550 [==============================] - 175s 318ms/step - loss: 3.8021 - triplet_acc: 0.9952 - val_loss: 6.4139 - val_triplet_acc: 0.9868\n",
      "\n",
      "Epoch 00041: val_loss did not improve\n",
      "Epoch 42/200\n",
      "550/550 [==============================] - 175s 318ms/step - loss: 3.9306 - triplet_acc: 0.9951 - val_loss: 102.5995 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00042: val_loss did not improve\n",
      "Epoch 43/200\n",
      "518/550 [===========================>..] - ETA: 7s - loss: 3.8770 - triplet_acc: 0.9954"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550/550 [==============================] - 176s 320ms/step - loss: 3.6363 - triplet_acc: 0.9959 - val_loss: 5.0936 - val_triplet_acc: 0.9913\n",
      "\n",
      "Epoch 00051: val_loss did not improve\n",
      "Epoch 52/200\n",
      "550/550 [==============================] - 174s 317ms/step - loss: 3.9543 - triplet_acc: 0.9948 - val_loss: 102.5953 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00052: val_loss did not improve\n",
      "Epoch 53/200\n",
      "550/550 [==============================] - 175s 318ms/step - loss: 3.8331 - triplet_acc: 0.9957 - val_loss: 5.0898 - val_triplet_acc: 0.9912\n",
      "\n",
      "Epoch 00053: val_loss did not improve\n",
      "Epoch 54/200\n",
      "550/550 [==============================] - 175s 317ms/step - loss: 3.8352 - triplet_acc: 0.9951 - val_loss: 5.1171 - val_triplet_acc: 0.9913\n",
      "\n",
      "Epoch 00054: val_loss did not improve\n",
      "Epoch 55/200\n",
      "550/550 [==============================] - 175s 319ms/step - loss: 3.6934 - triplet_acc: 0.9961 - val_loss: 5.0892 - val_triplet_acc: 0.9912\n",
      "\n",
      "Epoch 00055: val_loss did not improve\n",
      "Epoch 56/200\n",
      "487/550 [=========================>....] - ETA: 15s - loss: 3.7231 - triplet_acc: 0.9957"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550/550 [==============================] - 175s 318ms/step - loss: 5.7067 - triplet_acc: 0.9889 - val_loss: 7.5374 - val_triplet_acc: 0.9826\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/200\n",
      "550/550 [==============================] - 175s 319ms/step - loss: 5.7981 - triplet_acc: 0.9886 - val_loss: 8.1719 - val_triplet_acc: 0.9802\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/200\n",
      "550/550 [==============================] - 175s 318ms/step - loss: 5.4855 - triplet_acc: 0.9895 - val_loss: 6.3886 - val_triplet_acc: 0.9874\n",
      "\n",
      "Epoch 00007: val_loss improved from 6.82811 to 6.38860, saving model to TANH32-4.h5\n",
      "Epoch 8/200\n",
      "550/550 [==============================] - 176s 319ms/step - loss: 5.5718 - triplet_acc: 0.9894 - val_loss: 87.8187 - val_triplet_acc: 0.7950\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/200\n",
      "550/550 [==============================] - 175s 319ms/step - loss: 5.4985 - triplet_acc: 0.9898 - val_loss: 7.5564 - val_triplet_acc: 0.9837\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/200\n",
      "550/550 [==============================] - 176s 319ms/step - loss: 5.8447 - triplet_acc: 0.9885 - val_loss: 8.6076 - val_triplet_acc: 0.9780\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/200\n",
      "101/550 [====>.........................] - ETA: 1:41 - loss: 6.8863 - triplet_acc: 0.9851"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550/550 [==============================] - 176s 320ms/step - loss: 3.8362 - triplet_acc: 0.9937 - val_loss: 33.2070 - val_triplet_acc: 0.8897\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/200\n",
      "550/550 [==============================] - 175s 319ms/step - loss: 3.8010 - triplet_acc: 0.9944 - val_loss: 59.0015 - val_triplet_acc: 0.7705\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/200\n",
      "550/550 [==============================] - 176s 321ms/step - loss: 3.8356 - triplet_acc: 0.9939 - val_loss: 4.8812 - val_triplet_acc: 0.9904\n",
      "\n",
      "Epoch 00020: val_loss improved from 5.08247 to 4.88120, saving model to TANH32-4.h5\n",
      "Epoch 21/200\n",
      "550/550 [==============================] - 176s 319ms/step - loss: 3.4155 - triplet_acc: 0.9951 - val_loss: 102.1819 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 3.4896 - triplet_acc: 0.9950 - val_loss: 4.8977 - val_triplet_acc: 0.9902\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/200\n",
      "363/550 [==================>...........] - ETA: 45s - loss: 3.2777 - triplet_acc: 0.9955"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550/550 [==============================] - 176s 320ms/step - loss: 3.1237 - triplet_acc: 0.9958 - val_loss: 101.9065 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00031: val_loss did not improve\n",
      "Epoch 32/200\n",
      "550/550 [==============================] - 174s 317ms/step - loss: 2.8903 - triplet_acc: 0.9961 - val_loss: 101.8931 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 33/200\n",
      "550/550 [==============================] - 175s 319ms/step - loss: 3.0285 - triplet_acc: 0.9957 - val_loss: 4.1810 - val_triplet_acc: 0.9917\n",
      "\n",
      "Epoch 00033: val_loss improved from 4.23617 to 4.18101, saving model to TANH32-4.h5\n",
      "Epoch 34/200\n",
      "550/550 [==============================] - 177s 321ms/step - loss: 2.9069 - triplet_acc: 0.9957 - val_loss: 101.8685 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00034: val_loss did not improve\n",
      "Epoch 35/200\n",
      "550/550 [==============================] - 178s 323ms/step - loss: 2.9096 - triplet_acc: 0.9959 - val_loss: 4.1651 - val_triplet_acc: 0.9917\n",
      "\n",
      "Epoch 00035: val_loss improved from 4.18101 to 4.16513, saving model to TANH32-4.h5\n",
      "Epoch 36/200\n",
      "550/550 [==============================] - 181s 328ms/step - loss: 2.7568 - triplet_acc: 0.9969 - val_loss: 101.8456 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00036: val_loss did not improve\n",
      "Epoch 37/200\n",
      "151/550 [=======>......................] - ETA: 1:35 - loss: 2.9578 - triplet_acc: 0.9961"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550/550 [==============================] - 176s 319ms/step - loss: 2.8208 - triplet_acc: 0.9960 - val_loss: 3.9646 - val_triplet_acc: 0.9921\n",
      "\n",
      "Epoch 00041: val_loss improved from 4.12509 to 3.96464, saving model to TANH32-4.h5\n",
      "Epoch 42/200\n",
      "550/550 [==============================] - 175s 318ms/step - loss: 2.6642 - triplet_acc: 0.9968 - val_loss: 5.1948 - val_triplet_acc: 0.9873\n",
      "\n",
      "Epoch 00042: val_loss did not improve\n",
      "Epoch 43/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 2.7585 - triplet_acc: 0.9964 - val_loss: 67.3021 - val_triplet_acc: 0.7442\n",
      "\n",
      "Epoch 00043: val_loss did not improve\n",
      "Epoch 44/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 2.5807 - triplet_acc: 0.9972 - val_loss: 101.7586 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00044: val_loss did not improve\n",
      "Epoch 45/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 2.6425 - triplet_acc: 0.9965 - val_loss: 101.7454 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00045: val_loss did not improve\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 46/200\n",
      "550/550 [==============================] - 177s 321ms/step - loss: 2.5817 - triplet_acc: 0.9968 - val_loss: 55.1064 - val_triplet_acc: 0.7978\n",
      "\n",
      "Epoch 00046: val_loss did not improve\n",
      "Epoch 47/200\n",
      "550/550 [==============================] - 176s 319ms/step - loss: 2.5798 - triplet_acc: 0.9969 - val_loss: 3.9483 - val_triplet_acc: 0.9923\n",
      "\n",
      "Epoch 00047: val_loss improved from 3.96464 to 3.94826, saving model to TANH32-4.h5\n",
      "Epoch 48/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 2.5352 - triplet_acc: 0.9969 - val_loss: 101.7394 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00048: val_loss did not improve\n",
      "Epoch 49/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 2.6008 - triplet_acc: 0.9965 - val_loss: 101.7365 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00049: val_loss did not improve\n",
      "Epoch 50/200\n",
      "550/550 [==============================] - 177s 321ms/step - loss: 2.6855 - triplet_acc: 0.9963 - val_loss: 3.9365 - val_triplet_acc: 0.9919\n",
      "\n",
      "Epoch 00050: val_loss improved from 3.94826 to 3.93648, saving model to TANH32-4.h5\n",
      "Epoch 51/200\n",
      "550/550 [==============================] - 175s 318ms/step - loss: 2.4501 - triplet_acc: 0.9976 - val_loss: 3.9156 - val_triplet_acc: 0.9922\n",
      "\n",
      "Epoch 00051: val_loss improved from 3.93648 to 3.91557, saving model to TANH32-4.h5\n",
      "Epoch 52/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 2.5508 - triplet_acc: 0.9970 - val_loss: 3.8850 - val_triplet_acc: 0.9922\n",
      "\n",
      "Epoch 00052: val_loss improved from 3.91557 to 3.88498, saving model to TANH32-4.h5\n",
      "Epoch 53/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 2.5473 - triplet_acc: 0.9968 - val_loss: 101.7251 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00053: val_loss did not improve\n",
      "Epoch 54/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 2.4641 - triplet_acc: 0.9972 - val_loss: 3.9032 - val_triplet_acc: 0.9920\n",
      "\n",
      "Epoch 00054: val_loss did not improve\n",
      "Epoch 55/200\n",
      "550/550 [==============================] - 175s 319ms/step - loss: 2.6766 - triplet_acc: 0.9961 - val_loss: 3.8566 - val_triplet_acc: 0.9922\n",
      "\n",
      "Epoch 00055: val_loss improved from 3.88498 to 3.85656, saving model to TANH32-4.h5\n",
      "Epoch 56/200\n",
      "550/550 [==============================] - 177s 322ms/step - loss: 2.5836 - triplet_acc: 0.9963 - val_loss: 3.8802 - val_triplet_acc: 0.9920\n",
      "\n",
      "Epoch 00056: val_loss did not improve\n",
      "Epoch 57/200\n",
      "550/550 [==============================] - 177s 321ms/step - loss: 2.5187 - triplet_acc: 0.9973 - val_loss: 3.8606 - val_triplet_acc: 0.9920\n",
      "\n",
      "Epoch 00057: val_loss did not improve\n",
      "Epoch 58/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 2.7419 - triplet_acc: 0.9956 - val_loss: 3.8314 - val_triplet_acc: 0.9921\n",
      "\n",
      "Epoch 00058: val_loss improved from 3.85656 to 3.83140, saving model to TANH32-4.h5\n",
      "Epoch 59/200\n",
      "550/550 [==============================] - 176s 321ms/step - loss: 2.4912 - triplet_acc: 0.9972 - val_loss: 3.8026 - val_triplet_acc: 0.9922\n",
      "\n",
      "Epoch 00059: val_loss improved from 3.83140 to 3.80261, saving model to TANH32-4.h5\n",
      "Epoch 60/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 2.6311 - triplet_acc: 0.9964 - val_loss: 3.7966 - val_triplet_acc: 0.9921\n",
      "\n",
      "Epoch 00060: val_loss improved from 3.80261 to 3.79659, saving model to TANH32-4.h5\n",
      "Epoch 61/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 2.3758 - triplet_acc: 0.9974 - val_loss: 4.1915 - val_triplet_acc: 0.9910\n",
      "\n",
      "Epoch 00061: val_loss did not improve\n",
      "Epoch 62/200\n",
      "550/550 [==============================] - 176s 321ms/step - loss: 2.4728 - triplet_acc: 0.9967 - val_loss: 3.7878 - val_triplet_acc: 0.9924\n",
      "\n",
      "Epoch 00062: val_loss improved from 3.79659 to 3.78776, saving model to TANH32-4.h5\n",
      "Epoch 63/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 2.4585 - triplet_acc: 0.9973 - val_loss: 3.8501 - val_triplet_acc: 0.9925\n",
      "\n",
      "Epoch 00063: val_loss did not improve\n",
      "Epoch 64/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 2.3336 - triplet_acc: 0.9975 - val_loss: 101.6945 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00064: val_loss did not improve\n",
      "Epoch 65/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 2.4216 - triplet_acc: 0.9971 - val_loss: 3.7788 - val_triplet_acc: 0.9923\n",
      "\n",
      "Epoch 00065: val_loss improved from 3.78776 to 3.77876, saving model to TANH32-4.h5\n",
      "Epoch 66/200\n",
      "550/550 [==============================] - 175s 319ms/step - loss: 2.4218 - triplet_acc: 0.9973 - val_loss: 3.8025 - val_triplet_acc: 0.9921\n",
      "\n",
      "Epoch 00066: val_loss did not improve\n",
      "Epoch 67/200\n",
      "550/550 [==============================] - 175s 319ms/step - loss: 2.5834 - triplet_acc: 0.9966 - val_loss: 3.7672 - val_triplet_acc: 0.9919\n",
      "\n",
      "Epoch 00067: val_loss improved from 3.77876 to 3.76718, saving model to TANH32-4.h5\n",
      "Epoch 68/200\n",
      "550/550 [==============================] - 175s 318ms/step - loss: 2.6267 - triplet_acc: 0.9965 - val_loss: 101.6837 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00068: val_loss did not improve\n",
      "Epoch 69/200\n",
      "550/550 [==============================] - 175s 319ms/step - loss: 2.6770 - triplet_acc: 0.9961 - val_loss: 3.7673 - val_triplet_acc: 0.9921\n",
      "\n",
      "Epoch 00069: val_loss did not improve\n",
      "Epoch 70/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 2.4502 - triplet_acc: 0.9970 - val_loss: 3.7533 - val_triplet_acc: 0.9923\n",
      "\n",
      "Epoch 00070: val_loss improved from 3.76718 to 3.75335, saving model to TANH32-4.h5\n",
      "Epoch 71/200\n",
      "550/550 [==============================] - 175s 319ms/step - loss: 2.4221 - triplet_acc: 0.9974 - val_loss: 101.6760 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00071: val_loss did not improve\n",
      "Epoch 72/200\n",
      "550/550 [==============================] - 176s 321ms/step - loss: 2.4733 - triplet_acc: 0.9968 - val_loss: 3.7593 - val_triplet_acc: 0.9924\n",
      "\n",
      "Epoch 00072: val_loss did not improve\n",
      "Epoch 73/200\n",
      "550/550 [==============================] - 177s 321ms/step - loss: 2.4257 - triplet_acc: 0.9968 - val_loss: 3.7457 - val_triplet_acc: 0.9927\n",
      "\n",
      "Epoch 00073: val_loss improved from 3.75335 to 3.74569, saving model to TANH32-4.h5\n",
      "Epoch 74/200\n",
      "550/550 [==============================] - 175s 318ms/step - loss: 2.4981 - triplet_acc: 0.9969 - val_loss: 3.7566 - val_triplet_acc: 0.9926\n",
      "\n",
      "Epoch 00074: val_loss did not improve\n",
      "Epoch 75/200\n",
      "550/550 [==============================] - 176s 319ms/step - loss: 2.5087 - triplet_acc: 0.9964 - val_loss: 71.4093 - val_triplet_acc: 0.7420\n",
      "\n",
      "Epoch 00075: val_loss did not improve\n",
      "Epoch 76/200\n",
      "550/550 [==============================] - 175s 319ms/step - loss: 2.4270 - triplet_acc: 0.9971 - val_loss: 43.8635 - val_triplet_acc: 0.8265\n",
      "\n",
      "Epoch 00076: val_loss did not improve\n",
      "Epoch 77/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 2.4709 - triplet_acc: 0.9965 - val_loss: 101.6445 - val_triplet_acc: 0.5245\n",
      "\n",
      "Epoch 00077: val_loss did not improve\n",
      "\n",
      "Epoch 00077: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 78/200\n",
      "550/550 [==============================] - 175s 319ms/step - loss: 2.4541 - triplet_acc: 0.9966 - val_loss: 101.6594 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00078: val_loss did not improve\n",
      "Epoch 79/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 2.5970 - triplet_acc: 0.9964 - val_loss: 3.7348 - val_triplet_acc: 0.9921\n",
      "\n",
      "Epoch 00079: val_loss improved from 3.74569 to 3.73484, saving model to TANH32-4.h5\n",
      "Epoch 80/200\n",
      "550/550 [==============================] - 175s 319ms/step - loss: 2.5469 - triplet_acc: 0.9965 - val_loss: 3.7503 - val_triplet_acc: 0.9921\n",
      "\n",
      "Epoch 00080: val_loss did not improve\n",
      "Epoch 81/200\n",
      "550/550 [==============================] - 176s 319ms/step - loss: 2.5517 - triplet_acc: 0.9965 - val_loss: 101.6576 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00081: val_loss did not improve\n",
      "Epoch 82/200\n",
      "550/550 [==============================] - 175s 318ms/step - loss: 2.4290 - triplet_acc: 0.9969 - val_loss: 91.4846 - val_triplet_acc: 0.7454\n",
      "\n",
      "Epoch 00082: val_loss did not improve\n",
      "Epoch 83/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 2.5641 - triplet_acc: 0.9963 - val_loss: 101.1899 - val_triplet_acc: 0.7438\n",
      "\n",
      "Epoch 00083: val_loss did not improve\n",
      "\n",
      "Epoch 00083: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "Epoch 84/200\n",
      "550/550 [==============================] - 177s 323ms/step - loss: 2.3182 - triplet_acc: 0.9976 - val_loss: 3.7462 - val_triplet_acc: 0.9921\n",
      "\n",
      "Epoch 00084: val_loss did not improve\n",
      "Epoch 85/200\n",
      "550/550 [==============================] - 176s 319ms/step - loss: 2.5189 - triplet_acc: 0.9970 - val_loss: 101.6560 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00085: val_loss did not improve\n",
      "Epoch 86/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 2.4221 - triplet_acc: 0.9971 - val_loss: 4.7139 - val_triplet_acc: 0.9894\n",
      "\n",
      "Epoch 00086: val_loss did not improve\n",
      "Epoch 87/200\n",
      "550/550 [==============================] - 176s 319ms/step - loss: 2.4003 - triplet_acc: 0.9971 - val_loss: 17.8704 - val_triplet_acc: 0.9384\n",
      "\n",
      "Epoch 00087: val_loss did not improve\n",
      "Epoch 88/200\n",
      "550/550 [==============================] - 175s 317ms/step - loss: 2.5855 - triplet_acc: 0.9964 - val_loss: 5.8003 - val_triplet_acc: 0.9853\n",
      "\n",
      "Epoch 00088: val_loss did not improve\n",
      "Epoch 89/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 2.2806 - triplet_acc: 0.9976 - val_loss: 3.7487 - val_triplet_acc: 0.9921\n",
      "\n",
      "Epoch 00089: val_loss did not improve\n",
      "Epoch 00089: early stopping\n",
      "TANH32-5.h5\n",
      "Epoch 1/200\n",
      "550/550 [==============================] - 190s 346ms/step - loss: 4.5460 - triplet_acc: 0.9893 - val_loss: 76.9952 - val_triplet_acc: 0.7433\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 76.99519, saving model to TANH32-5.h5\n",
      "Epoch 2/200\n",
      "550/550 [==============================] - 175s 319ms/step - loss: 5.3998 - triplet_acc: 0.9873 - val_loss: 5.9513 - val_triplet_acc: 0.9864\n",
      "\n",
      "Epoch 00002: val_loss improved from 76.99519 to 5.95134, saving model to TANH32-5.h5\n",
      "Epoch 3/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 4.5341 - triplet_acc: 0.9903 - val_loss: 102.1058 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 4.7061 - triplet_acc: 0.9900 - val_loss: 102.1616 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/200\n",
      "550/550 [==============================] - 175s 319ms/step - loss: 5.2971 - triplet_acc: 0.9878 - val_loss: 34.4065 - val_triplet_acc: 0.8726\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 4.6866 - triplet_acc: 0.9909 - val_loss: 6.5650 - val_triplet_acc: 0.9846\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 7/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 4.0149 - triplet_acc: 0.9934 - val_loss: 64.2982 - val_triplet_acc: 0.7891\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/200\n",
      "550/550 [==============================] - 175s 318ms/step - loss: 3.4685 - triplet_acc: 0.9947 - val_loss: 102.1326 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/200\n",
      "550/550 [==============================] - 176s 321ms/step - loss: 3.7122 - triplet_acc: 0.9937 - val_loss: 101.1021 - val_triplet_acc: 0.6177\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 10/200\n",
      "550/550 [==============================] - 177s 322ms/step - loss: 3.3707 - triplet_acc: 0.9950 - val_loss: 4.9068 - val_triplet_acc: 0.9898\n",
      "\n",
      "Epoch 00010: val_loss improved from 5.95134 to 4.90683, saving model to TANH32-5.h5\n",
      "Epoch 11/200\n",
      "550/550 [==============================] - 177s 322ms/step - loss: 3.4179 - triplet_acc: 0.9953 - val_loss: 4.7446 - val_triplet_acc: 0.9905\n",
      "\n",
      "Epoch 00011: val_loss improved from 4.90683 to 4.74463, saving model to TANH32-5.h5\n",
      "Epoch 12/200\n",
      "550/550 [==============================] - 177s 322ms/step - loss: 3.3856 - triplet_acc: 0.9950 - val_loss: 4.7685 - val_triplet_acc: 0.9901\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/200\n",
      "550/550 [==============================] - 185s 335ms/step - loss: 3.5008 - triplet_acc: 0.9941 - val_loss: 4.7927 - val_triplet_acc: 0.9899\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/200\n",
      "550/550 [==============================] - 182s 331ms/step - loss: 3.0522 - triplet_acc: 0.9964 - val_loss: 101.9479 - val_triplet_acc: 0.6743\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/200\n",
      "550/550 [==============================] - 185s 336ms/step - loss: 3.0904 - triplet_acc: 0.9957 - val_loss: 4.6336 - val_triplet_acc: 0.9902\n",
      "\n",
      "Epoch 00015: val_loss improved from 4.74463 to 4.63355, saving model to TANH32-5.h5\n",
      "Epoch 16/200\n",
      "550/550 [==============================] - 182s 330ms/step - loss: 2.9729 - triplet_acc: 0.9966 - val_loss: 4.6075 - val_triplet_acc: 0.9906\n",
      "\n",
      "Epoch 00016: val_loss improved from 4.63355 to 4.60751, saving model to TANH32-5.h5\n",
      "Epoch 17/200\n",
      "550/550 [==============================] - 183s 334ms/step - loss: 2.9784 - triplet_acc: 0.9957 - val_loss: 101.8946 - val_triplet_acc: 0.5366\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/200\n",
      "550/550 [==============================] - 180s 328ms/step - loss: 2.9137 - triplet_acc: 0.9961 - val_loss: 4.5161 - val_triplet_acc: 0.9907\n",
      "\n",
      "Epoch 00018: val_loss improved from 4.60751 to 4.51612, saving model to TANH32-5.h5\n",
      "Epoch 19/200\n",
      "550/550 [==============================] - 184s 334ms/step - loss: 2.8976 - triplet_acc: 0.9961 - val_loss: 61.1937 - val_triplet_acc: 0.7713\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/200\n",
      "550/550 [==============================] - 180s 328ms/step - loss: 2.9672 - triplet_acc: 0.9959 - val_loss: 4.3395 - val_triplet_acc: 0.9912\n",
      "\n",
      "Epoch 00020: val_loss improved from 4.51612 to 4.33954, saving model to TANH32-5.h5\n",
      "Epoch 21/200\n",
      "550/550 [==============================] - 182s 331ms/step - loss: 2.9083 - triplet_acc: 0.9960 - val_loss: 18.2469 - val_triplet_acc: 0.9376\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/200\n",
      "550/550 [==============================] - 181s 329ms/step - loss: 2.6769 - triplet_acc: 0.9969 - val_loss: 101.8662 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/200\n",
      "550/550 [==============================] - 183s 334ms/step - loss: 2.7832 - triplet_acc: 0.9961 - val_loss: 101.8523 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 24/200\n",
      "550/550 [==============================] - 180s 327ms/step - loss: 2.8495 - triplet_acc: 0.9958 - val_loss: 83.6028 - val_triplet_acc: 0.7144\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 25/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 2.7203 - triplet_acc: 0.9965 - val_loss: 4.4410 - val_triplet_acc: 0.9905\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 26/200\n",
      "550/550 [==============================] - 176s 321ms/step - loss: 2.8202 - triplet_acc: 0.9965 - val_loss: 101.8311 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 27/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 2.8783 - triplet_acc: 0.9962 - val_loss: 4.3722 - val_triplet_acc: 0.9903\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 28/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550/550 [==============================] - 176s 320ms/step - loss: 2.8298 - triplet_acc: 0.9960 - val_loss: 4.3648 - val_triplet_acc: 0.9903\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      "Epoch 29/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 2.7637 - triplet_acc: 0.9965 - val_loss: 4.3546 - val_triplet_acc: 0.9905\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      "Epoch 30/200\n",
      "550/550 [==============================] - 184s 334ms/step - loss: 2.5683 - triplet_acc: 0.9970 - val_loss: 94.0531 - val_triplet_acc: 0.7104\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "Epoch 00030: early stopping\n",
      "TANH32-6.h5\n",
      "Epoch 1/200\n",
      "550/550 [==============================] - 199s 362ms/step - loss: 4.5300 - triplet_acc: 0.9897 - val_loss: 15.8859 - val_triplet_acc: 0.9507\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 15.88593, saving model to TANH32-6.h5\n",
      "Epoch 2/200\n",
      "550/550 [==============================] - 181s 330ms/step - loss: 4.7050 - triplet_acc: 0.9891 - val_loss: 7.3682 - val_triplet_acc: 0.9818\n",
      "\n",
      "Epoch 00002: val_loss improved from 15.88593 to 7.36819, saving model to TANH32-6.h5\n",
      "Epoch 3/200\n",
      "550/550 [==============================] - 184s 335ms/step - loss: 5.0160 - triplet_acc: 0.9885 - val_loss: 13.8268 - val_triplet_acc: 0.9587\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/200\n",
      "550/550 [==============================] - 183s 333ms/step - loss: 4.8196 - triplet_acc: 0.9897 - val_loss: 23.3904 - val_triplet_acc: 0.9207\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/200\n",
      "550/550 [==============================] - 184s 334ms/step - loss: 5.1168 - triplet_acc: 0.9893 - val_loss: 96.9996 - val_triplet_acc: 0.6688\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/200\n",
      "550/550 [==============================] - 182s 330ms/step - loss: 4.4662 - triplet_acc: 0.9914 - val_loss: 5.7584 - val_triplet_acc: 0.9879\n",
      "\n",
      "Epoch 00006: val_loss improved from 7.36819 to 5.75844, saving model to TANH32-6.h5\n",
      "Epoch 7/200\n",
      "550/550 [==============================] - 185s 337ms/step - loss: 4.8305 - triplet_acc: 0.9898 - val_loss: 102.2419 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/200\n",
      "550/550 [==============================] - 182s 331ms/step - loss: 4.6151 - triplet_acc: 0.9903 - val_loss: 102.2349 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/200\n",
      "550/550 [==============================] - 184s 334ms/step - loss: 5.0414 - triplet_acc: 0.9888 - val_loss: 102.2622 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/200\n",
      "550/550 [==============================] - 181s 329ms/step - loss: 4.9576 - triplet_acc: 0.9899 - val_loss: 102.2941 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 11/200\n",
      "550/550 [==============================] - 178s 323ms/step - loss: 4.2151 - triplet_acc: 0.9927 - val_loss: 53.6946 - val_triplet_acc: 0.8039\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/200\n",
      "550/550 [==============================] - 179s 326ms/step - loss: 3.6364 - triplet_acc: 0.9945 - val_loss: 5.0628 - val_triplet_acc: 0.9898\n",
      "\n",
      "Epoch 00012: val_loss improved from 5.75844 to 5.06280, saving model to TANH32-6.h5\n",
      "Epoch 13/200\n",
      "550/550 [==============================] - 183s 332ms/step - loss: 3.3644 - triplet_acc: 0.9948 - val_loss: 102.1291 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/200\n",
      "550/550 [==============================] - 183s 333ms/step - loss: 3.3197 - triplet_acc: 0.9951 - val_loss: 102.0773 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/200\n",
      "550/550 [==============================] - 181s 330ms/step - loss: 3.4431 - triplet_acc: 0.9943 - val_loss: 5.6571 - val_triplet_acc: 0.9868\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/200\n",
      "550/550 [==============================] - 185s 336ms/step - loss: 3.2356 - triplet_acc: 0.9952 - val_loss: 101.9865 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 17/200\n",
      "550/550 [==============================] - 181s 329ms/step - loss: 3.2520 - triplet_acc: 0.9952 - val_loss: 4.5044 - val_triplet_acc: 0.9906\n",
      "\n",
      "Epoch 00017: val_loss improved from 5.06280 to 4.50441, saving model to TANH32-6.h5\n",
      "Epoch 18/200\n",
      "550/550 [==============================] - 183s 332ms/step - loss: 2.8736 - triplet_acc: 0.9963 - val_loss: 4.5597 - val_triplet_acc: 0.9906\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/200\n",
      "550/550 [==============================] - 181s 330ms/step - loss: 2.9906 - triplet_acc: 0.9959 - val_loss: 11.7374 - val_triplet_acc: 0.9634\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/200\n",
      "550/550 [==============================] - 185s 336ms/step - loss: 3.0389 - triplet_acc: 0.9955 - val_loss: 4.3765 - val_triplet_acc: 0.9915\n",
      "\n",
      "Epoch 00020: val_loss improved from 4.50441 to 4.37653, saving model to TANH32-6.h5\n",
      "Epoch 21/200\n",
      "550/550 [==============================] - 182s 330ms/step - loss: 3.0339 - triplet_acc: 0.9959 - val_loss: 101.7020 - val_triplet_acc: 0.6702\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/200\n",
      "550/550 [==============================] - 184s 335ms/step - loss: 3.0312 - triplet_acc: 0.9956 - val_loss: 101.9001 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/200\n",
      "550/550 [==============================] - 182s 330ms/step - loss: 2.7924 - triplet_acc: 0.9964 - val_loss: 4.3587 - val_triplet_acc: 0.9910\n",
      "\n",
      "Epoch 00023: val_loss improved from 4.37653 to 4.35866, saving model to TANH32-6.h5\n",
      "Epoch 24/200\n",
      "550/550 [==============================] - 184s 334ms/step - loss: 2.7417 - triplet_acc: 0.9964 - val_loss: 4.2291 - val_triplet_acc: 0.9912\n",
      "\n",
      "Epoch 00024: val_loss improved from 4.35866 to 4.22907, saving model to TANH32-6.h5\n",
      "Epoch 25/200\n",
      "550/550 [==============================] - 182s 331ms/step - loss: 2.7801 - triplet_acc: 0.9965 - val_loss: 101.8616 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 26/200\n",
      "550/550 [==============================] - 184s 334ms/step - loss: 2.7447 - triplet_acc: 0.9965 - val_loss: 54.0330 - val_triplet_acc: 0.8083\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 27/200\n",
      "550/550 [==============================] - 182s 331ms/step - loss: 2.7935 - triplet_acc: 0.9964 - val_loss: 101.8367 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 28/200\n",
      "550/550 [==============================] - 182s 331ms/step - loss: 2.9200 - triplet_acc: 0.9959 - val_loss: 4.0643 - val_triplet_acc: 0.9918\n",
      "\n",
      "Epoch 00028: val_loss improved from 4.22907 to 4.06431, saving model to TANH32-6.h5\n",
      "Epoch 29/200\n",
      "550/550 [==============================] - 182s 332ms/step - loss: 2.7184 - triplet_acc: 0.9965 - val_loss: 4.3105 - val_triplet_acc: 0.9903\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      "Epoch 30/200\n",
      "550/550 [==============================] - 183s 332ms/step - loss: 2.8210 - triplet_acc: 0.9959 - val_loss: 101.8010 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      "Epoch 31/200\n",
      "550/550 [==============================] - 184s 334ms/step - loss: 2.7944 - triplet_acc: 0.9964 - val_loss: 101.7898 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00031: val_loss did not improve\n",
      "Epoch 32/200\n",
      "550/550 [==============================] - 181s 329ms/step - loss: 2.8010 - triplet_acc: 0.9962 - val_loss: 12.7649 - val_triplet_acc: 0.9582\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 33/200\n",
      "550/550 [==============================] - 183s 333ms/step - loss: 2.6452 - triplet_acc: 0.9966 - val_loss: 101.7768 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00033: val_loss did not improve\n",
      "Epoch 34/200\n",
      "550/550 [==============================] - 181s 330ms/step - loss: 2.6915 - triplet_acc: 0.9966 - val_loss: 3.9579 - val_triplet_acc: 0.9919\n",
      "\n",
      "Epoch 00034: val_loss improved from 4.06431 to 3.95789, saving model to TANH32-6.h5\n",
      "Epoch 35/200\n",
      "550/550 [==============================] - 185s 336ms/step - loss: 2.6244 - triplet_acc: 0.9970 - val_loss: 101.7657 - val_triplet_acc: 0.9620\n",
      "\n",
      "Epoch 00035: val_loss did not improve\n",
      "Epoch 36/200\n",
      "550/550 [==============================] - 183s 332ms/step - loss: 2.7268 - triplet_acc: 0.9966 - val_loss: 85.1170 - val_triplet_acc: 0.6436\n",
      "\n",
      "Epoch 00036: val_loss did not improve\n",
      "Epoch 37/200\n",
      "550/550 [==============================] - 185s 336ms/step - loss: 2.9117 - triplet_acc: 0.9956 - val_loss: 49.9769 - val_triplet_acc: 0.8095\n",
      "\n",
      "Epoch 00037: val_loss did not improve\n",
      "Epoch 38/200\n",
      "550/550 [==============================] - 181s 330ms/step - loss: 2.8156 - triplet_acc: 0.9958 - val_loss: 3.9435 - val_triplet_acc: 0.9921\n",
      "\n",
      "Epoch 00038: val_loss improved from 3.95789 to 3.94346, saving model to TANH32-6.h5\n",
      "Epoch 39/200\n",
      "550/550 [==============================] - 184s 335ms/step - loss: 2.4328 - triplet_acc: 0.9976 - val_loss: 67.1573 - val_triplet_acc: 0.7593\n",
      "\n",
      "Epoch 00039: val_loss did not improve\n",
      "Epoch 40/200\n",
      "550/550 [==============================] - 182s 331ms/step - loss: 2.5305 - triplet_acc: 0.9971 - val_loss: 5.8885 - val_triplet_acc: 0.9852\n",
      "\n",
      "Epoch 00040: val_loss did not improve\n",
      "Epoch 41/200\n",
      "550/550 [==============================] - 183s 333ms/step - loss: 2.5565 - triplet_acc: 0.9969 - val_loss: 58.0110 - val_triplet_acc: 0.7852\n",
      "\n",
      "Epoch 00041: val_loss did not improve\n",
      "Epoch 42/200\n",
      "550/550 [==============================] - 180s 328ms/step - loss: 2.5964 - triplet_acc: 0.9969 - val_loss: 9.9442 - val_triplet_acc: 0.9695\n",
      "\n",
      "Epoch 00042: val_loss did not improve\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 43/200\n",
      "550/550 [==============================] - 184s 334ms/step - loss: 2.7905 - triplet_acc: 0.9959 - val_loss: 5.6255 - val_triplet_acc: 0.9856\n",
      "\n",
      "Epoch 00043: val_loss did not improve\n",
      "Epoch 44/200\n",
      "550/550 [==============================] - 182s 330ms/step - loss: 2.6119 - triplet_acc: 0.9968 - val_loss: 3.8556 - val_triplet_acc: 0.9923\n",
      "\n",
      "Epoch 00044: val_loss improved from 3.94346 to 3.85557, saving model to TANH32-6.h5\n",
      "Epoch 45/200\n",
      "229/550 [===========>..................] - ETA: 1:22 - loss: 2.3119 - triplet_acc: 0.9982"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550/550 [==============================] - 182s 331ms/step - loss: 2.4483 - triplet_acc: 0.9973 - val_loss: 74.5225 - val_triplet_acc: 0.8042\n",
      "\n",
      "Epoch 00052: val_loss did not improve\n",
      "Epoch 53/200\n",
      "550/550 [==============================] - 183s 332ms/step - loss: 2.5493 - triplet_acc: 0.9972 - val_loss: 101.7455 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00053: val_loss did not improve\n",
      "Epoch 54/200\n",
      "550/550 [==============================] - 183s 332ms/step - loss: 2.6081 - triplet_acc: 0.9966 - val_loss: 3.8522 - val_triplet_acc: 0.9923\n",
      "\n",
      "Epoch 00054: val_loss improved from 3.85557 to 3.85221, saving model to TANH32-6.h5\n",
      "Epoch 55/200\n",
      "550/550 [==============================] - 181s 328ms/step - loss: 2.7565 - triplet_acc: 0.9963 - val_loss: 3.8564 - val_triplet_acc: 0.9921\n",
      "\n",
      "Epoch 00055: val_loss did not improve\n",
      "Epoch 56/200\n",
      "550/550 [==============================] - 184s 334ms/step - loss: 2.4597 - triplet_acc: 0.9970 - val_loss: 72.5512 - val_triplet_acc: 0.7884\n",
      "\n",
      "Epoch 00056: val_loss did not improve\n",
      "Epoch 57/200\n",
      "387/550 [====================>.........] - ETA: 41s - loss: 2.5055 - triplet_acc: 0.9973"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550/550 [==============================] - 177s 322ms/step - loss: 2.6999 - triplet_acc: 0.9963 - val_loss: 7.4497 - val_triplet_acc: 0.9778\n",
      "\n",
      "Epoch 00059: val_loss did not improve\n",
      "Epoch 60/200\n",
      "550/550 [==============================] - 184s 334ms/step - loss: 2.4925 - triplet_acc: 0.9975 - val_loss: 101.7441 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00060: val_loss did not improve\n",
      "Epoch 61/200\n",
      "550/550 [==============================] - 181s 328ms/step - loss: 2.7400 - triplet_acc: 0.9963 - val_loss: 3.8515 - val_triplet_acc: 0.9922\n",
      "\n",
      "Epoch 00061: val_loss improved from 3.85221 to 3.85147, saving model to TANH32-6.h5\n",
      "Epoch 62/200\n",
      "550/550 [==============================] - 184s 334ms/step - loss: 2.7100 - triplet_acc: 0.9964 - val_loss: 101.7438 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00062: val_loss did not improve\n",
      "Epoch 63/200\n",
      "550/550 [==============================] - 181s 328ms/step - loss: 2.6838 - triplet_acc: 0.9963 - val_loss: 42.6572 - val_triplet_acc: 0.8319\n",
      "\n",
      "Epoch 00063: val_loss did not improve\n",
      "Epoch 64/200\n",
      "550/550 [==============================] - 185s 337ms/step - loss: 2.5655 - triplet_acc: 0.9970 - val_loss: 101.7434 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00064: val_loss did not improve\n",
      "Epoch 65/200\n",
      "550/550 [==============================] - 181s 330ms/step - loss: 2.6511 - triplet_acc: 0.9964 - val_loss: 53.0386 - val_triplet_acc: 0.7984\n",
      "\n",
      "Epoch 00065: val_loss did not improve\n",
      "Epoch 66/200\n",
      "550/550 [==============================] - 184s 335ms/step - loss: 2.5344 - triplet_acc: 0.9970 - val_loss: 3.8401 - val_triplet_acc: 0.9924\n",
      "\n",
      "Epoch 00066: val_loss improved from 3.85147 to 3.84008, saving model to TANH32-6.h5\n",
      "Epoch 67/200\n",
      "550/550 [==============================] - 181s 329ms/step - loss: 2.5446 - triplet_acc: 0.9968 - val_loss: 4.0313 - val_triplet_acc: 0.9914\n",
      "\n",
      "Epoch 00067: val_loss did not improve\n",
      "Epoch 68/200\n",
      "550/550 [==============================] - 184s 334ms/step - loss: 2.5698 - triplet_acc: 0.9966 - val_loss: 4.1251 - val_triplet_acc: 0.9916\n",
      "\n",
      "Epoch 00068: val_loss did not improve\n",
      "Epoch 69/200\n",
      "550/550 [==============================] - 181s 329ms/step - loss: 2.7330 - triplet_acc: 0.9959 - val_loss: 101.7425 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00069: val_loss did not improve\n",
      "Epoch 70/200\n",
      "550/550 [==============================] - 184s 335ms/step - loss: 2.4783 - triplet_acc: 0.9972 - val_loss: 3.8562 - val_triplet_acc: 0.9922\n",
      "\n",
      "Epoch 00070: val_loss did not improve\n",
      "Epoch 71/200\n",
      "550/550 [==============================] - 181s 328ms/step - loss: 2.4900 - triplet_acc: 0.9971 - val_loss: 101.7421 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00071: val_loss did not improve\n",
      "Epoch 72/200\n",
      "550/550 [==============================] - 183s 332ms/step - loss: 2.6712 - triplet_acc: 0.9962 - val_loss: 101.7419 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00072: val_loss did not improve\n",
      "Epoch 73/200\n",
      "550/550 [==============================] - 182s 330ms/step - loss: 2.5693 - triplet_acc: 0.9968 - val_loss: 95.5900 - val_triplet_acc: 0.7129\n",
      "\n",
      "Epoch 00073: val_loss did not improve\n",
      "Epoch 74/200\n",
      "550/550 [==============================] - 183s 333ms/step - loss: 2.5983 - triplet_acc: 0.9963 - val_loss: 3.8532 - val_triplet_acc: 0.9922\n",
      "\n",
      "Epoch 00074: val_loss did not improve\n",
      "Epoch 75/200\n",
      "550/550 [==============================] - 183s 332ms/step - loss: 2.4881 - triplet_acc: 0.9969 - val_loss: 3.8527 - val_triplet_acc: 0.9922\n",
      "\n",
      "Epoch 00075: val_loss did not improve\n",
      "Epoch 76/200\n",
      "550/550 [==============================] - 181s 330ms/step - loss: 2.4053 - triplet_acc: 0.9977 - val_loss: 3.8854 - val_triplet_acc: 0.9922\n",
      "\n",
      "Epoch 00076: val_loss did not improve\n",
      "Epoch 00076: early stopping\n",
      "TANH32-7.h5\n",
      "Epoch 1/200\n",
      "550/550 [==============================] - 201s 366ms/step - loss: 4.0336 - triplet_acc: 0.9899 - val_loss: 46.7642 - val_triplet_acc: 0.8157\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 46.76419, saving model to TANH32-7.h5\n",
      "Epoch 2/200\n",
      "550/550 [==============================] - 181s 330ms/step - loss: 3.9632 - triplet_acc: 0.9910 - val_loss: 5.7680 - val_triplet_acc: 0.9853\n",
      "\n",
      "Epoch 00002: val_loss improved from 46.76419 to 5.76801, saving model to TANH32-7.h5\n",
      "Epoch 3/200\n",
      "550/550 [==============================] - 185s 336ms/step - loss: 4.4598 - triplet_acc: 0.9894 - val_loss: 51.2518 - val_triplet_acc: 0.7909\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/200\n",
      "550/550 [==============================] - 180s 327ms/step - loss: 4.3101 - triplet_acc: 0.9898 - val_loss: 97.3343 - val_triplet_acc: 0.5971\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/200\n",
      "550/550 [==============================] - 186s 339ms/step - loss: 4.3070 - triplet_acc: 0.9907 - val_loss: 5.8740 - val_triplet_acc: 0.9856\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/200\n",
      "550/550 [==============================] - 186s 338ms/step - loss: 4.4054 - triplet_acc: 0.9901 - val_loss: 6.6148 - val_triplet_acc: 0.9828\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 7/200\n",
      "550/550 [==============================] - 190s 346ms/step - loss: 3.5956 - triplet_acc: 0.9934 - val_loss: 27.0024 - val_triplet_acc: 0.9065\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/200\n",
      "550/550 [==============================] - 185s 336ms/step - loss: 2.9483 - triplet_acc: 0.9952 - val_loss: 101.7891 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/200\n",
      "550/550 [==============================] - 189s 344ms/step - loss: 3.0550 - triplet_acc: 0.9947 - val_loss: 101.6038 - val_triplet_acc: 0.6463\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 10/200\n",
      "550/550 [==============================] - 180s 327ms/step - loss: 2.9941 - triplet_acc: 0.9952 - val_loss: 101.7388 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/200\n",
      "550/550 [==============================] - 180s 328ms/step - loss: 2.7476 - triplet_acc: 0.9963 - val_loss: 98.6093 - val_triplet_acc: 0.7419\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/200\n",
      "550/550 [==============================] - 181s 330ms/step - loss: 2.8600 - triplet_acc: 0.9958 - val_loss: 100.4871 - val_triplet_acc: 0.6279\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 00012: early stopping\n",
      "TANH32-8.h5\n",
      "Epoch 1/200\n",
      "550/550 [==============================] - 199s 362ms/step - loss: 4.2799 - triplet_acc: 0.9903 - val_loss: 99.8067 - val_triplet_acc: 0.6615\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 99.80674, saving model to TANH32-8.h5\n",
      "Epoch 2/200\n",
      "550/550 [==============================] - 183s 332ms/step - loss: 4.8409 - triplet_acc: 0.9881 - val_loss: 101.2947 - val_triplet_acc: 0.6076\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 3/200\n",
      "550/550 [==============================] - 180s 328ms/step - loss: 3.9569 - triplet_acc: 0.9923 - val_loss: 98.5934 - val_triplet_acc: 0.7797\n",
      "\n",
      "Epoch 00003: val_loss improved from 99.80674 to 98.59336, saving model to TANH32-8.h5\n",
      "Epoch 4/200\n",
      "550/550 [==============================] - 178s 323ms/step - loss: 4.5568 - triplet_acc: 0.9897 - val_loss: 98.5059 - val_triplet_acc: 0.7544\n",
      "\n",
      "Epoch 00004: val_loss improved from 98.59336 to 98.50589, saving model to TANH32-8.h5\n",
      "Epoch 5/200\n",
      "550/550 [==============================] - 177s 321ms/step - loss: 4.2991 - triplet_acc: 0.9913 - val_loss: 101.9007 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/200\n",
      "550/550 [==============================] - 178s 324ms/step - loss: 4.2702 - triplet_acc: 0.9913 - val_loss: 97.6926 - val_triplet_acc: 0.7563\n",
      "\n",
      "Epoch 00006: val_loss improved from 98.50589 to 97.69257, saving model to TANH32-8.h5\n",
      "Epoch 7/200\n",
      "550/550 [==============================] - 177s 322ms/step - loss: 3.9716 - triplet_acc: 0.9922 - val_loss: 98.1254 - val_triplet_acc: 0.7413\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/200\n",
      "550/550 [==============================] - 177s 321ms/step - loss: 4.8773 - triplet_acc: 0.9890 - val_loss: 101.9575 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/200\n",
      "550/550 [==============================] - 179s 325ms/step - loss: 4.0532 - triplet_acc: 0.9920 - val_loss: 99.5797 - val_triplet_acc: 0.6931\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550/550 [==============================] - 176s 320ms/step - loss: 4.1653 - triplet_acc: 0.9919 - val_loss: 101.9719 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 11/200\n",
      "550/550 [==============================] - 179s 326ms/step - loss: 3.5556 - triplet_acc: 0.9941 - val_loss: 97.8362 - val_triplet_acc: 0.7667\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/200\n",
      "550/550 [==============================] - 177s 322ms/step - loss: 3.0912 - triplet_acc: 0.9951 - val_loss: 101.5858 - val_triplet_acc: 0.5739\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/200\n",
      "550/550 [==============================] - 178s 324ms/step - loss: 3.0587 - triplet_acc: 0.9955 - val_loss: 100.3080 - val_triplet_acc: 0.7073\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 14/200\n",
      "550/550 [==============================] - 178s 323ms/step - loss: 2.8143 - triplet_acc: 0.9963 - val_loss: 101.8321 - val_triplet_acc: 1.0000\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/200\n",
      "550/550 [==============================] - 176s 320ms/step - loss: 2.9632 - triplet_acc: 0.9956 - val_loss: 97.5174 - val_triplet_acc: 0.7675\n",
      "\n",
      "Epoch 00015: val_loss improved from 97.69257 to 97.51743, saving model to TANH32-8.h5\n",
      "Epoch 16/200\n",
      "550/550 [==============================] - 177s 321ms/step - loss: 2.9199 - triplet_acc: 0.9957 - val_loss: 101.8021 - val_triplet_acc: 0.6266\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/200\n",
      "550/550 [==============================] - 177s 322ms/step - loss: 2.7458 - triplet_acc: 0.9963 - val_loss: 97.7157 - val_triplet_acc: 0.7674\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/200\n",
      "550/550 [==============================] - 177s 322ms/step - loss: 2.7786 - triplet_acc: 0.9961 - val_loss: 101.7716 - val_triplet_acc: 0.5979\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/200\n",
      "170/550 [========>.....................] - ETA: 1:31 - loss: 2.8934 - triplet_acc: 0.9949"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-fd021743fa81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m                                    \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                                    \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mA_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mP_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_val\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeros_vect_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                                    \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                                   )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2222\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2223\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2224\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2226\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1883\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Full network 1/32 - Embeddings 64 - Maxout\n",
    "A_val, P_val, N_val = triplets_list_val\n",
    "zeros_vect_val = np.zeros(A_val[:,1,1].shape) \n",
    "\n",
    "\n",
    "batch_sz = 32\n",
    "\n",
    "\n",
    "for num in range(1, 10):\n",
    "    model_name = 'TANH32-%01d.h5' % num\n",
    "    print(model_name)\n",
    "    \n",
    "    #We create a checkpoint to save the best model and add an early stopping\n",
    "    checkpoint = ModelCheckpoint(model_name, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min')\n",
    "    lr_annealing = ReduceLROnPlateau(monitor='val_loss', patience = 3, epsilon=0.01, factor = 0.25, min_lr = 1e-6, verbose = 1, mode='min' )\n",
    "    callbacks_list = [checkpoint, early_stop, lr_annealing]\n",
    "    \n",
    "\n",
    "    #We compile our model with the custom made triplet_loss\n",
    "    classification_model.compile(optimizer = 'adam', loss = triplet_loss, metrics =[triplet_acc])\n",
    "    \n",
    "    classification_model.fit_generator(batch_generator(X_train, Y_train, batch_sz, hardmode=False), \n",
    "                                   steps_per_epoch = 550,\n",
    "                                   epochs = 200,\n",
    "                                   verbose = 1,\n",
    "                                   validation_data = ([A_val, P_val, N_val], zeros_vect_val),\n",
    "                                   callbacks = callbacks_list,\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We selected the model with the best validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19770/19770 [==============================] - 43s 2ms/step\n",
      "TANH32-1.h5 5.294122102396863 0.9912999493821317\n",
      "19770/19770 [==============================] - 43s 2ms/step\n",
      "TANH32-2.h5 4.429839496101221 0.9920586747235581\n",
      "19770/19770 [==============================] - 43s 2ms/step\n",
      "TANH32-3.h5 5.077177170834037 0.9918563480020233\n",
      "19770/19770 [==============================] - 43s 2ms/step\n",
      "TANH32-4.h5 3.7348441636254828 0.9921092564129865\n",
      "19770/19770 [==============================] - 44s 2ms/step\n",
      "TANH32-5.h5 4.339544625250933 0.9912493676927033\n",
      "19770/19770 [==============================] - 44s 2ms/step\n",
      "TANH32-6.h5 3.84008408043803 0.9923621648963076\n",
      "19770/19770 [==============================] - 45s 2ms/step\n",
      "TANH32-7.h5 5.768011224975354 0.9852807283401489\n",
      "19770/19770 [==============================] - 44s 2ms/step\n",
      "TANH32-8.h5 97.51743054233178 0.7675265553990095\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('TANH32-4.h5', 3.7348441636254828)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_loss=1e9\n",
    "A_val, P_val, N_val = triplets_list_val\n",
    "zeros_vect_val = np.zeros(A_val[:,1,1].shape) \n",
    "\n",
    "for weights in ['TANH32-1.h5','TANH32-2.h5','TANH32-3.h5','TANH32-4.h5','TANH32-5.h5','TANH32-6.h5','TANH32-7.h5','TANH32-8.h5']:\n",
    "    classification_model.load_weights(weights)\n",
    "    loss, acc = classification_model.evaluate([A_val, P_val, N_val], zeros_vect_val, batch_size=32, verbose=0)\n",
    "    print(weights, loss, acc)\n",
    "    if (loss<global_loss):\n",
    "        model_name = weights\n",
    "        global_loss = loss\n",
    "        \n",
    "classification_model.load_weights(model_name)\n",
    "model_name, global_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model.load_weights('TANH32-4.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Model Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_emb(emb_model, item):\n",
    "    classification_model.load_weights('TANH32-2.h5')\n",
    "    emb1 = emb_model.predict_on_batch(item)\n",
    "    \n",
    "    classification_model.load_weights('TANH32-4.h5')\n",
    "    emb2 = emb_model.predict_on_batch(item)\n",
    "    \n",
    "    classification_model.load_weights('TANH32-6.h5')\n",
    "    emb3 = emb_model.predict_on_batch(item)\n",
    "    \n",
    "    emb = np.mean([emb1,emb2,emb3],axis = 0)\n",
    "    \n",
    "    return emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison with Modified Hausdorff Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from demo_classification import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNNDistance(itemA, itemB):\n",
    "    \"\"\"\n",
    "    Compute the euclidean distance between the embeddings of the two images. \n",
    "    The embeddings have been calculated by using the emb_model.\n",
    "    \n",
    "    Arguments:\n",
    "    itemA -- array of images\n",
    "    itemB -- array of images\n",
    "    \n",
    "    Returns:\n",
    "    dist - euclidean distance\n",
    "    \"\"\"\n",
    "    \n",
    "    itemA = itemA.reshape(1, itemA.shape[0], itemA.shape[1], 1)\n",
    "    itemB = itemB.reshape(1, itemB.shape[0], itemB.shape[1], 1)\n",
    "    itemA_emb = emb_model.predict_on_batch(itemA)\n",
    "    itemB_emb = emb_model.predict_on_batch(itemB)\n",
    "    dist = np.linalg.norm(itemA_emb - itemB_emb) #2-norm by default\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadImgAsArray(fn):\n",
    "    \"\"\"\n",
    "    Load an image file, resize it and return an array\n",
    "    \n",
    "    Arguments:\n",
    "    fn -- an image\n",
    "    \n",
    "    Returns:\n",
    "    image -- an array of image \n",
    "    \"\"\"\n",
    "    picture = mpimg.imread(fn)\n",
    "    image = resize(picture, (SZ,SZ), mode='nearest')\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then calculate the error rate by comparing ModHausdhorff Distance with our Embeddings triplet loss Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-shot classification with Modified Hausdorff Distance versus Siamese triplet loss Distance\n",
      " run 1 ModHausdorffDistance(error 45.0%)  -  Siamese_triplet_loss_Distance (error 15.0%)\n",
      " run 2 ModHausdorffDistance(error 35.0%)  -  Siamese_triplet_loss_Distance (error 5.0%)\n",
      " run 3 ModHausdorffDistance(error 40.0%)  -  Siamese_triplet_loss_Distance (error 0.0%)\n",
      " run 4 ModHausdorffDistance(error 25.0%)  -  Siamese_triplet_loss_Distance (error 5.0%)\n",
      " run 5 ModHausdorffDistance(error 30.0%)  -  Siamese_triplet_loss_Distance (error 20.0%)\n",
      " run 6 ModHausdorffDistance(error 15.0%)  -  Siamese_triplet_loss_Distance (error 0.0%)\n",
      " run 7 ModHausdorffDistance(error 60.0%)  -  Siamese_triplet_loss_Distance (error 0.0%)\n",
      " run 8 ModHausdorffDistance(error 35.0%)  -  Siamese_triplet_loss_Distance (error 10.0%)\n",
      " run 9 ModHausdorffDistance(error 40.0%)  -  Siamese_triplet_loss_Distance (error 0.0%)\n",
      " run 10 ModHausdorffDistance(error 55.0%)  -  Siamese_triplet_loss_Distance (error 5.0%)\n",
      " run 11 ModHausdorffDistance(error 15.0%)  -  Siamese_triplet_loss_Distance (error 0.0%)\n",
      " run 12 ModHausdorffDistance(error 70.0%)  -  Siamese_triplet_loss_Distance (error 10.0%)\n",
      " run 13 ModHausdorffDistance(error 65.0%)  -  Siamese_triplet_loss_Distance (error 20.0%)\n",
      " run 14 ModHausdorffDistance(error 35.0%)  -  Siamese_triplet_loss_Distance (error 10.0%)\n",
      " run 15 ModHausdorffDistance(error 15.0%)  -  Siamese_triplet_loss_Distance (error 0.0%)\n",
      " run 16 ModHausdorffDistance(error 25.0%)  -  Siamese_triplet_loss_Distance (error 0.0%)\n",
      " run 17 ModHausdorffDistance(error 30.0%)  -  Siamese_triplet_loss_Distance (error 10.0%)\n",
      " run 18 ModHausdorffDistance(error 40.0%)  -  Siamese_triplet_loss_Distance (error 5.0%)\n",
      " run 19 ModHausdorffDistance(error 70.0%)  -  Siamese_triplet_loss_Distance (error 5.0%)\n",
      " run 20 ModHausdorffDistance(error 30.0%)  -  Siamese_triplet_loss_Distance (error 10.0%)\n",
      " average error ModHausdorffDistance 38.75%  average error Siamese_triplet_loss_Distance 6.5%\n"
     ]
    }
   ],
   "source": [
    "#tanh32-4\n",
    "print ('One-shot classification with Modified Hausdorff Distance versus embeddings triplet loss Distance')\n",
    "perror = np.zeros(nrun)\n",
    "perror_cnn =np.zeros(nrun)\n",
    "for r in range(1,nrun+1):\n",
    "\trs = str(r)\n",
    "\tif len(rs)==1:\n",
    "\t\trs = '0' + rs\t\t\n",
    "\tperror[r-1] = classification_run('one-shot-classification','/run'+rs, LoadImgAsPoints, ModHausdorffDistance, 'cost')\n",
    "\tperror_cnn[r-1] = classification_run('one-shot-classification','run'+rs, LoadImgAsArray, CNNDistance, 'cost')\n",
    "\tprint (\" run \" + str(r) + \" ModHausdorffDistance\" + \"(error \" + str(\tperror[r-1] ) + \"%)\"+ \"  -  Embeddings_triplet_loss_Distance\" + \" (error \" + str(\tperror_cnn[r-1] ) + \"%)\")\t\t\n",
    "total = np.mean(perror)\n",
    "total_cnn = np.mean(perror_cnn)\n",
    "print (\" average error ModHausdorffDistance \" + str(total) + \"%\" + \"  average error Embeddings_triplet_loss_Distance \" + str(total_cnn) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "20 way classification\n",
    "\n",
    "\n",
    "def nway(num_way, num_run):\n",
    "    \"\"\"\n",
    "    Create a N way classification\n",
    "    \"\"\"\n",
    "    \n",
    "    choice_classes np.random.rand()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to improve the model :\n",
    "Bayesian optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    def make_oneshot_task(N,s=\"val\"):\n",
    "        \"\"\"Create pairs of test image, support set for testing N way one-shot learning.\n",
    "        \n",
    "        Arguments :\n",
    "        N -- Number of different characters\n",
    "        \"\"\"\n",
    "        X=self.data[s]\n",
    "        n_classes, n_examples, w, h = X.shape\n",
    "        indices = rng.randint(0,n_examples,size=(N,))\n",
    "        if language is not None:\n",
    "            low, high = self.categories[s][language]\n",
    "            if N > high - low:\n",
    "                raise ValueError(\"This language ({}) has less than {} letters\".format(language, N))\n",
    "            categories = rng.choice(range(low,high),size=(N,),replace=False)\n",
    "            \n",
    "        else:#if no language specified just pick a bunch of random letters\n",
    "            categories = rng.choice(range(n_classes),size=(N,),replace=False)            \n",
    "        true_category = categories[0]\n",
    "        ex1, ex2 = rng.choice(n_examples,replace=False,size=(2,))\n",
    "        test_image = np.asarray([X[true_category,ex1,:,:]]*N).reshape(N, w, h,1)\n",
    "        support_set = X[categories,indices,:,:]\n",
    "        support_set[0,:,:] = X[true_category,ex2]\n",
    "        support_set = support_set.reshape(N, w, h,1)\n",
    "        targets = np.zeros((N,))\n",
    "        targets[0] = 1\n",
    "        targets, test_image, support_set = shuffle(targets, test_image, support_set)\n",
    "        pairs = [test_image,support_set]\n",
    "\n",
    "        return pairs, targets\n",
    "    \n",
    "    def test_oneshot(self,model,N,k,s=\"val\",verbose=0):\n",
    "        \"\"\"Test average N way oneshot learning accuracy of a siamese neural net over k one-shot tasks\"\"\"\n",
    "        n_correct = 0\n",
    "        if verbose:\n",
    "            print(\"Evaluating model on {} random {} way one-shot learning tasks ...\".format(k,N))\n",
    "        for i in range(k):\n",
    "            inputs, targets = self.make_oneshot_task(N,s)\n",
    "            probs = model.predict(inputs)\n",
    "            if np.argmax(probs) == np.argmax(targets):\n",
    "                n_correct+=1\n",
    "        percent_correct = (100.0*n_correct / k)\n",
    "        if verbose:\n",
    "            print(\"Got an average of {}% {} way one-shot learning accuracy\".format(percent_correct,N))\n",
    "        return percent_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_nn_accuracy(N_ways,n_trials,loader):\n",
    "    \"\"\"Returns accuracy of one shot \"\"\"\n",
    "    print(\"Evaluating nearest neighbour on {} unique {} way one-shot learning tasks ...\".format(n_trials,N_ways))\n",
    "\n",
    "    n_right = 0\n",
    "    \n",
    "    for i in range(n_trials):\n",
    "        pairs,targets = loader.make_oneshot_task(N_ways,\"val\")\n",
    "        correct = nearest_neighbour_correct(pairs,targets)\n",
    "        n_right += correct\n",
    "    return 100.0 * n_right / n_trials\n",
    "\n",
    "\n",
    "ways = np.arange(1, 60, 2)\n",
    "resume =  False\n",
    "val_accs, train_accs,nn_accs = [], [], []\n",
    "trials = 450\n",
    "for N in ways:\n",
    "    val_accs.append(loader.test_oneshot(siamese_net, N,trials, \"val\", verbose=True))\n",
    "    train_accs.append(loader.test_oneshot(siamese_net, N,trials, \"train\", verbose=True))\n",
    "    nn_accs.append(test_nn_accuracy(N,trials, loader))\n",
    "    \n",
    "#plot the accuracy vs num categories for each\n",
    "plt.plot(ways, val_accs, \"m\")\n",
    "plt.plot(ways, train_accs, \"y\")\n",
    "plt.plot(ways, nn_accs, \"c\")\n",
    "\n",
    "plt.plot(ways,100.0/ways,\"r\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
