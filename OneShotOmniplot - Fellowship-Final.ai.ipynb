{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few-shot learning - Omniglot - Fellowship.ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fellowship.ai - Few-shot learning\n",
    "This is my personal project to the few-shot learning challenge from [Fellowship.ai](https://fellowship.ai/challenge/) with the following goal:\n",
    "> Omniglot, the “transpose” of MNIST, with 1623 character classes, each with 20 examples.  Build a few-shot classifier with a target of <35% error.\n",
    "\n",
    "#### Omniglot - Dataset\n",
    "*Dataset reference:* [Link](https://github.com/brendenlake/omniglot)\n",
    "> Lake, B. M., Salakhutdinov, R., and Tenenbaum, J. B. (2015). Human-level concept learning through probabilistic program induction. Science, 350(6266), 1332-1338.\n",
    "\n",
    "The Omniglot dataset is often considered as the transpose of the MNIST dataset. While the latter contains only 10 classes with a training set of 60000 examples, Omniglot contains an important number of classes (1623 different handwritten characters from 50 different alphabets) with only a low number of examples (20) for each, making it an ideal dataset for few-shot learning problems.\n",
    "\n",
    "#### Few-shot learning\n",
    "Whereas, lots of deep learning projects are based on a huge number of training examples to be trained, few-shot learning is  based only on a few one. This approach is much closer to the one experienced by humans. We are able to memorize and recognize objects we have never seen before from a few number of examples. Then for each new encounter with these types of object we can classify them in an accurate and easy way.\n",
    "\n",
    "\n",
    "#### Stategy\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, concatenate, MaxPooling2D, Dropout, Lambda\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from PIL import Image                                                                                                                               \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import glob\n",
    "from skimage.transform import resize\n",
    "import h5py\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We indicate the PATH of the training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training images path\n",
    "PATH=\"images_background/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a look at the list of all alphabets contained in the training set, and their total number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of the different alphabets:\n",
      "\n",
      " ['Alphabet_of_the_Magi' 'Anglo-Saxon_Futhorc' 'Arcadian' 'Armenian'\n",
      " 'Asomtavruli_(Georgian)' 'Balinese' 'Bengali'\n",
      " 'Blackfoot_(Canadian_Aboriginal_Syllabics)' 'Braille' 'Burmese_(Myanmar)'\n",
      " 'Cyrillic' 'Early_Aramaic' 'Futurama' 'Grantha' 'Greek' 'Gujarati'\n",
      " 'Hebrew' 'Inuktitut_(Canadian_Aboriginal_Syllabics)'\n",
      " 'Japanese_(hiragana)' 'Japanese_(katakana)' 'Korean' 'Latin'\n",
      " 'Malay_(Jawi_-_Arabic)' 'Mkhedruli_(Georgian)' 'N_Ko'\n",
      " 'Ojibwe_(Canadian_Aboriginal_Syllabics)' 'Sanskrit' 'Syriac_(Estrangelo)'\n",
      " 'Tagalog' 'Tifinagh']\n",
      "\n",
      "Number of different alphabets: 30\n"
     ]
    }
   ],
   "source": [
    "alph_type = np.array(os.listdir(PATH)) #Give the different types of alphabet in our data\n",
    "print(\"List of the different alphabets:\\n\\n {}\".format(alph_type))\n",
    "print(\"\\nNumber of different alphabets: {}\".format(len(alph_type)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then check the number of character for each alphabets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of chracters corresponding to each alphabet is : \n",
      "{'Alphabet_of_the_Magi': 20, 'Anglo-Saxon_Futhorc': 29, 'Arcadian': 26, 'Armenian': 41, 'Asomtavruli_(Georgian)': 40, 'Balinese': 24, 'Bengali': 46, 'Blackfoot_(Canadian_Aboriginal_Syllabics)': 14, 'Braille': 26, 'Burmese_(Myanmar)': 34, 'Cyrillic': 33, 'Early_Aramaic': 22, 'Futurama': 26, 'Grantha': 43, 'Greek': 24, 'Gujarati': 48, 'Hebrew': 22, 'Inuktitut_(Canadian_Aboriginal_Syllabics)': 16, 'Japanese_(hiragana)': 52, 'Japanese_(katakana)': 47, 'Korean': 40, 'Latin': 26, 'Malay_(Jawi_-_Arabic)': 40, 'Mkhedruli_(Georgian)': 41, 'N_Ko': 33, 'Ojibwe_(Canadian_Aboriginal_Syllabics)': 14, 'Sanskrit': 42, 'Syriac_(Estrangelo)': 23, 'Tagalog': 17, 'Tifinagh': 55}\n",
      "\n",
      "The maximum number of different character for one alphabet is 55\n",
      "The minimum number of different character for one alphabet is 14\n",
      "The total number of different character is 964\n"
     ]
    }
   ],
   "source": [
    "alph_num_char ={}\n",
    "for alphabet in alph_type:\n",
    "    alph_num_char[alphabet]= len(os.listdir(f'{PATH}{alphabet}'))\n",
    "print(\"The number of chracters corresponding to each alphabet is : \\n{}\".format(alph_num_char))\n",
    "\n",
    "num_of_char = alph_num_char.values()\n",
    "print('\\nThe maximum number of different character for one alphabet is {}'.format(max(num_of_char)))\n",
    "print('The minimum number of different character for one alphabet is {}'.format(min(num_of_char)))\n",
    "total_char = sum(num_of_char)\n",
    "print('The total number of different character is {}'.format(total_char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum number of different examples for one character is 20\n",
      "The minimum number of different examples for one character is 20\n",
      "The total number of different pictures is 19280\n"
     ]
    }
   ],
   "source": [
    "alph_num_char_ex={}\n",
    "class_num=0\n",
    "for alphabet in alph_type:\n",
    "    char_list=os.listdir(f'{PATH}{alphabet}')\n",
    "    for char in char_list:\n",
    "        alph_num_char_ex[(alphabet,char)]= len(os.listdir(f'{PATH}{alphabet}/{char}'))\n",
    "\n",
    "num_of_example = alph_num_char_ex.values()\n",
    "print('The maximum number of different examples for one character is {}'.format(max(num_of_example)))\n",
    "print('The minimum number of different examples for one character is {}'.format(min(num_of_example)))\n",
    "total_example = sum(num_of_example) \n",
    "print('The total number of different pictures is {}'.format(total_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that each character have 20 examples(pictures)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add label for each character\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each picture in our dataset we give a corresponding label(an integer) which allow us to determine the corresponding character. Here an integer is sufficient as we are not really interested in knowing from which alphabet an image is coming from and as we don't have need to know the character name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19280,)\n"
     ]
    }
   ],
   "source": [
    "class_char=np.array([])\n",
    "for i in range(total_char):\n",
    "    #As each character have 20 examples\n",
    "    class_char= np.concatenate((class_char, np.ones(20)*(i+1))) \n",
    "class_char = class_char.astype(int)\n",
    "print(class_char.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reshape, our data to have the number of channel including (here is 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19280, 1)\n"
     ]
    }
   ],
   "source": [
    "class_char=class_char.reshape(class_char.shape[0],1)\n",
    "print(class_char.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert images to datafile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first retrieve the path for each picture in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['images_background\\\\Alphabet_of_the_Magi\\\\character01\\\\0709_01.png',\n",
       " 'images_background\\\\Alphabet_of_the_Magi\\\\character01\\\\0709_02.png',\n",
       " 'images_background\\\\Alphabet_of_the_Magi\\\\character01\\\\0709_03.png',\n",
       " 'images_background\\\\Alphabet_of_the_Magi\\\\character01\\\\0709_04.png',\n",
       " 'images_background\\\\Alphabet_of_the_Magi\\\\character01\\\\0709_05.png']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagePath = glob.glob(\"{}*/*/*.png\".format(PATH))\n",
    "imagePath[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print some random images of the dataset, convert them to arrays and resize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We decide to resize our images to the following "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imagePath = glob.glob(\"{}*/*/*.png\".format(PATH))\n",
    "\n",
    "SZ=60 #Dimension of the output image expected\n",
    "\n",
    "#Dimensions of the grill of sample pictures\n",
    "columns = 4\n",
    "rows = 5\n",
    "\n",
    "fig=plt.figure(figsize=(5, 5))\n",
    "list_example = np.random.randint(total_example, size = columns*rows)\n",
    "pos=0\n",
    "for i in list_example:\n",
    "    pos+=1\n",
    "    img = mpimg.imread(imagePath[i])\n",
    "    img = resize(img, (SZ,SZ), mode='reflect')\n",
    "    fig.add_subplot(rows, columns, pos)\n",
    "    plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All images seems clear and well centered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert all images into arrays and resize them to the 60x60 format. We concatenate all arrays into the variable im_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19280, 60, 60)\n"
     ]
    }
   ],
   "source": [
    "#Transform in array and resize all 19280 images \n",
    "im_array = np.array([resize(mpimg.imread(i), (SZ,SZ), mode='reflect') for i in imagePath] )\n",
    "print (im_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reshape the image to take into account the number of channel to pass them in our CNN, which is 1 here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19280, 60, 60, 1)\n"
     ]
    }
   ],
   "source": [
    "channel_sz = 1 #number of channel\n",
    "im_array= im_array.reshape(im_array.shape[0],im_array.shape[1],im_array.shape[2],channel_sz)\n",
    "print(im_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train/ validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the traning set into Train and Validation sets of pictures (Train : 70%, Validation : 3O%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, Y_train, Y_valid = train_test_split(im_array, class_char, test_size=0.3, stratify= class_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13496, 60, 60, 1), (5784, 60, 60, 1), (13496, 1), (5784, 1))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_valid.shape, Y_train.shape, Y_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triplet loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our loss function is determined by the following triplet loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(y_true, y_pred, alpha = 0.2):\n",
    "    \"\"\"\n",
    "    Implementation of the triplet loss\n",
    "    \n",
    "    Arguments:\n",
    "    y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.\n",
    "    y_pred -- python list containing three objects:\n",
    "            anchor -- the encodings for the anchor images, of shape (None, 64)\n",
    "            positive -- the encodings for the positive images, of shape (None, 64)\n",
    "            negative -- the encodings for the negative images, of shape (None, 64)\n",
    "    \n",
    "    Returns:\n",
    "    loss -- real number, value of the loss\n",
    "    \"\"\"\n",
    "    \n",
    "    anchor, positive, negative = y_pred[:,0:64], y_pred[:,64:128], y_pred[:,128:256]\n",
    "    \n",
    "    #Compute the (encoding) distance between the anchor and the positive, you will need to sum over axis=-1\n",
    "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), axis = -1)\n",
    "    #Compute the (encoding) distance between the anchor and the negative, you will need to sum over axis=-1\n",
    "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), axis = -1)\n",
    "    #Subtract the two previous distances and add alpha.\n",
    "    basic_loss = tf.add(tf.subtract(pos_dist, neg_dist), alpha)\n",
    "    #Take the maximum of basic_loss and 0.0. Sum over the training examples.\n",
    "    loss = tf.reduce_sum(tf.maximum(basic_loss, 0))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Triplet Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_triplets(X, Y, num=1):\n",
    "    \"\"\"\n",
    "    Create a list of valid triplets\n",
    "    \n",
    "    Arguments:\n",
    "    X -- array of images\n",
    "    Y -- array of classes corresponding to each image\n",
    "    num -- number of negative images for each valid anchor and positive images - must be positive\n",
    "           if num = 0, all possible valid couples are created\n",
    "            For example : for one valid (A,P) couple we can select 'num' random N images. \n",
    "                          Thus 'num' triplets are created for this (A,P) couple\n",
    "\n",
    "    \n",
    "    Returns:\n",
    "    (A,P,N) -- python tuple containing 3 arrays : \n",
    "            A -- the array for the anchor images, of shape (None, 64)\n",
    "            P -- the array for the positive images, of shape (None, 64)\n",
    "            N -- the array for the negative images, of shape (None, 64)\n",
    "    \"\"\"\n",
    "\n",
    "    Y = Y.reshape(Y.shape[0],)\n",
    "    A = []\n",
    "    P = []\n",
    "    N = []\n",
    "    \n",
    "    #We loop over all possible valid (A,P)\n",
    "    for i in range(X.shape[0]):  \n",
    "        list_pos = X[Y==Y[i]]\n",
    "        for j in list_pos:\n",
    "            #We provide a number 'num' of triplets for each valid (A,P)\n",
    "            if num >=1:\n",
    "                for k in range(num):\n",
    "                    rand_num = np.random.randint(X.shape[0])\n",
    "                    if np.array_equal(X[i],j) == False:\n",
    "                        A.append(X[i])\n",
    "                        P.append(j)\n",
    "                        while np.array_equal(Y[rand_num], Y[i]):\n",
    "                            rand_num = np.random.randint(X.shape[0])\n",
    "                        N.append(X[rand_num])\n",
    "            if num == 0:\n",
    "                for k in range(X.shape[0]):\n",
    "                    if np.array_equal(X[i],j) == False:\n",
    "                        if np.array_equal(Y[i],Y[k]) == False:\n",
    "                            A.append(X[i])\n",
    "                            P.append(j)\n",
    "                            N.append(X[k])\n",
    "    \n",
    "    A = np.array(A)\n",
    "    P = np.array(P)\n",
    "    N = np.array(N)\n",
    "    \n",
    "    return (A, P, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28920, 60, 60, 1)\n",
      "(28920, 60, 60, 1)\n",
      "(28920, 60, 60, 1)\n"
     ]
    }
   ],
   "source": [
    "#We create one triplet for each of the possible (A,P) couple in our validation set \n",
    "triplets_list_valid = create_triplets(X_valid,Y_valid)\n",
    "for i in range(len(triplets_list_valid)):\n",
    "    print(triplets_list_valid[i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a number of 28920 examples for the validation. Thus 1 negative image per (A,P) couple is enough for our evaluation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EmbModel(input_shape):\n",
    "    \"\"\"\n",
    "    Define our shared embedding model\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of array of input images\n",
    "\n",
    "    \n",
    "    Returns:\n",
    "    model - Our model which transform an array of images into an array of embeddings\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Define the input placeholder as a tensor with shape input_shape.\n",
    "    X_input = Input(input_shape)\n",
    "    \n",
    "    #Padding\n",
    "    X = ZeroPadding2D((1,1))(X_input)\n",
    "    \n",
    "    #CONV\n",
    "    X = Conv2D(8,(3,3),strides =(1,1), name ='conv0', kernel_initializer='glorot_uniform') (X)\n",
    "    X = BatchNormalization(axis = 1, name = \"bn0\") (X)\n",
    "    X = Activation('relu', name='a0')(X)\n",
    "    \n",
    "    #MAXPOOL\n",
    "    X = MaxPooling2D((2,2), name='max_pool0')(X)\n",
    "    \n",
    "    X = Dropout(0.15)(X)\n",
    "    \n",
    "    #Padding\n",
    "    #X = ZeroPadding2D((1,1))(X)\n",
    "    \n",
    "     #CONV\n",
    "    X = Conv2D(16,(3,3),strides =(1,1), name ='conv1', kernel_initializer='glorot_uniform') (X)\n",
    "    X = BatchNormalization(axis = 1, name = \"bn1\") (X)\n",
    "    X = Activation('relu', name='a1')(X)\n",
    "    \n",
    "    #MAXPOOL\n",
    "    X = MaxPooling2D((2,2), name='max_pool1')(X)\n",
    "    \n",
    "    X = Dropout(0.15)(X)\n",
    "    \n",
    "    #Padding\n",
    "    #X = ZeroPadding2D((1,1))(X)\n",
    "    \n",
    "     #CONV\n",
    "    X = Conv2D(32,(3,3),strides =(1,1), name ='conv2', kernel_initializer='glorot_uniform') (X)\n",
    "    X = BatchNormalization(axis = 1, name = \"bn2\") (X)\n",
    "    X = Activation('relu', name='a2')(X)\n",
    "    \n",
    "     #MAXPOOL\n",
    "    X = MaxPooling2D((2,2), name='max_pool2')(X)\n",
    "    \n",
    "    X = Dropout(0.15)(X)\n",
    "    \n",
    "    #Padding\n",
    "    #X = ZeroPadding2D((1,1))(X)\n",
    "    \n",
    "     #CONV\n",
    "    X = Conv2D(64,(3,3),strides =(1,1), name ='conv3', kernel_initializer='glorot_uniform') (X)\n",
    "    X = BatchNormalization(axis = 1, name = \"bn3\") (X)\n",
    "    X = Activation('relu', name='a3')(X)\n",
    "    \n",
    "     #MAXPOOL\n",
    "    X = MaxPooling2D((2,2), name='max_pool3')(X)\n",
    "    \n",
    "    X = Dropout(0.2)(X)\n",
    "    \n",
    "    #FLATTEN X + FC\n",
    "    X = Flatten(name='f3')(X)\n",
    "    X = Dense (256, activation ='relu', name='fc4', kernel_initializer='glorot_uniform') (X)\n",
    "    X = Dropout(0.3)(X)\n",
    "    X = Dense (64, activation ='relu', name='fc5', kernel_initializer='glorot_uniform') (X)\n",
    "    X = Lambda(lambda  x: tf.nn.l2_normalize(x,axis=1))(X)\n",
    "    \n",
    "    ##Create model\n",
    "    model = Model(inputs = X_input, outputs = X, name='EmbModel')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We define our global model\n",
    "input_size = (SZ, SZ, channel_sz)                     \n",
    "\n",
    "A = Input(input_size)\n",
    "P = Input(input_size)\n",
    "N = Input(input_size)\n",
    "\n",
    "emb_model= EmbModel(input_size)\n",
    "\n",
    "out_A = emb_model(A)\n",
    "out_P = emb_model(P)\n",
    "out_N = emb_model(N)\n",
    "\n",
    "y_pred = concatenate([out_A, out_P, out_N], axis =-1)\n",
    "\n",
    "classification_model = Model(inputs = [A, P, N], outputs = y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 60, 60, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 60, 60, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 60, 60, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "EmbModel (Model)                (None, 64)           107040      input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 192)          0           EmbModel[1][0]                   \n",
      "                                                                 EmbModel[2][0]                   \n",
      "                                                                 EmbModel[3][0]                   \n",
      "==================================================================================================\n",
      "Total params: 107,040\n",
      "Trainable params: 106,832\n",
      "Non-trainable params: 208\n",
      "__________________________________________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 60, 60, 1)         0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 62, 62, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv0 (Conv2D)               (None, 60, 60, 8)         80        \n",
      "_________________________________________________________________\n",
      "bn0 (BatchNormalization)     (None, 60, 60, 8)         240       \n",
      "_________________________________________________________________\n",
      "a0 (Activation)              (None, 60, 60, 8)         0         \n",
      "_________________________________________________________________\n",
      "max_pool0 (MaxPooling2D)     (None, 30, 30, 8)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 30, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 28, 28, 16)        1168      \n",
      "_________________________________________________________________\n",
      "bn1 (BatchNormalization)     (None, 28, 28, 16)        112       \n",
      "_________________________________________________________________\n",
      "a1 (Activation)              (None, 28, 28, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pool1 (MaxPooling2D)     (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 12, 12, 32)        4640      \n",
      "_________________________________________________________________\n",
      "bn2 (BatchNormalization)     (None, 12, 12, 32)        48        \n",
      "_________________________________________________________________\n",
      "a2 (Activation)              (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pool2 (MaxPooling2D)     (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv3 (Conv2D)               (None, 4, 4, 64)          18496     \n",
      "_________________________________________________________________\n",
      "bn3 (BatchNormalization)     (None, 4, 4, 64)          16        \n",
      "_________________________________________________________________\n",
      "a3 (Activation)              (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pool3 (MaxPooling2D)     (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "f3 (Flatten)                 (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "fc4 (Dense)                  (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "fc5 (Dense)                  (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 64)                0         \n",
      "=================================================================\n",
      "Total params: 107,040\n",
      "Trainable params: 106,832\n",
      "Non-trainable params: 208\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classification_model.summary()\n",
    "emb_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X,Y, bs=32):\n",
    "    \"\"\"\n",
    "    Create a mini-batch generator\n",
    "    \n",
    "    Arguments:\n",
    "    X -- array of images\n",
    "    Y -- array of classes corresponding to each image\n",
    "    bs -- size of the minibatch\n",
    "\n",
    "    \n",
    "    Returns:\n",
    "    [A_batch, P_batch, N_batch], y_dummie) -- a mini-batch of size bs\n",
    "    \"\"\"\n",
    "    \n",
    "    Y = Y.reshape(Y.shape[0],)\n",
    "    while True:\n",
    "        #0. Initialize Anchor,Postive, Negative\n",
    "        A_batch = []\n",
    "        P_batch = []\n",
    "        N_batch = []\n",
    "        for i in range(bs):      \n",
    "            #1.Choose a random Anchor Image\n",
    "            rand_A_num = np.random.randint(X.shape[0])\n",
    "            A_batch.append(X[rand_A_num])\n",
    "            \n",
    "            #2.Choose a random Positive Image\n",
    "            list_pos = X[Y==Y[rand_A_num]]                            #List of positive images\n",
    "            rand_P_num = np.random.randint(len(list_pos))\n",
    "            P_batch.append(list_pos[rand_P_num])\n",
    "            \n",
    "            #3.Choose a random Negative Image\n",
    "            rand_N_num = np.random.randint(X.shape[0])\n",
    "            while np.array_equal(Y[rand_N_num], Y[rand_A_num]):\n",
    "                rand_A_num = np.random.randint(X.shape[0])\n",
    "            N_batch.append(X[rand_N_num])\n",
    "            \n",
    "        A_batch = np.array(A_batch)\n",
    "        P_batch = np.array(P_batch)\n",
    "        N_batch = np.array(N_batch)\n",
    "        \n",
    "        y_dummie = np.zeros((len(A_batch),))\n",
    "        \n",
    "        yield ([A_batch, P_batch, N_batch], y_dummie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real shit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We compile our model with the custom made triplet_loss\n",
    "classification_model.compile(optimizer = 'adam', loss = triplet_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint and early stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create a checkpoint to save the best model and add an early stopping\n",
    "filepath = \"Weights\\weights.{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "callbacks_list = [checkpoint, early_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[1024,16,28,28] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: EmbModel_1/conv1/convolution = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](EmbModel_1/dropout_1/cond/Merge, conv1/kernel/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: loss/mul/_917 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_5511_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'EmbModel_1/conv1/convolution', defined at:\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-26-3e05ee646c44>\", line 11, in <module>\n    out_P = emb_model(P)\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\keras\\engine\\topology.py\", line 619, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\keras\\engine\\topology.py\", line 2085, in call\n    output_tensors, _, _ = self.run_internal_graph(inputs, masks)\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\keras\\engine\\topology.py\", line 2236, in run_internal_graph\n    output_tensors = _to_list(layer.call(computed_tensor, **kwargs))\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\keras\\layers\\convolutional.py\", line 168, in call\n    dilation_rate=self.dilation_rate)\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 3335, in conv2d\n    data_format=tf_data_format)\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 782, in convolution\n    return op(input, filter)\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 870, in __call__\n    return self.conv_op(inp, filter)\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 522, in __call__\n    return self.call(inp, filter)\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 206, in __call__\n    name=self.name)\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 1039, in conv2d\n    data_format=data_format, dilations=dilations, name=name)\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3290, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1654, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[1024,16,28,28] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: EmbModel_1/conv1/convolution = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](EmbModel_1/dropout_1/cond/Merge, conv1/kernel/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: loss/mul/_917 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_5511_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1312\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1420\u001b[1;33m             status, run_metadata)\n\u001b[0m\u001b[0;32m   1421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    515\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 516\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    517\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[1024,16,28,28] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: EmbModel_1/conv1/convolution = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](EmbModel_1/dropout_1/cond/Merge, conv1/kernel/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: loss/mul/_917 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_5511_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-e95b3a153611>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m                                    \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                                    \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                                    \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mA_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mP_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_valid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzeros_vect_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m                                   )\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2222\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   2223\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2224\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   2225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2226\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1883\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1884\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1885\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2478\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2479\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1140\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1141\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1321\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1338\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1339\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1340\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1342\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[1024,16,28,28] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: EmbModel_1/conv1/convolution = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](EmbModel_1/dropout_1/cond/Merge, conv1/kernel/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: loss/mul/_917 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_5511_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'EmbModel_1/conv1/convolution', defined at:\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-26-3e05ee646c44>\", line 11, in <module>\n    out_P = emb_model(P)\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\keras\\engine\\topology.py\", line 619, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\keras\\engine\\topology.py\", line 2085, in call\n    output_tensors, _, _ = self.run_internal_graph(inputs, masks)\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\keras\\engine\\topology.py\", line 2236, in run_internal_graph\n    output_tensors = _to_list(layer.call(computed_tensor, **kwargs))\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\keras\\layers\\convolutional.py\", line 168, in call\n    dilation_rate=self.dilation_rate)\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 3335, in conv2d\n    data_format=tf_data_format)\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 782, in convolution\n    return op(input, filter)\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 870, in __call__\n    return self.conv_op(inp, filter)\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 522, in __call__\n    return self.call(inp, filter)\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 206, in __call__\n    name=self.name)\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 1039, in conv2d\n    data_format=data_format, dilations=dilations, name=name)\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3290, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\Xyrion\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1654, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[1024,16,28,28] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: EmbModel_1/conv1/convolution = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](EmbModel_1/dropout_1/cond/Merge, conv1/kernel/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: loss/mul/_917 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_5511_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n"
     ]
    }
   ],
   "source": [
    "A_valid, P_valid, N_valid = triplets_list_valid\n",
    "zeros_vect_valid = np.zeros(A_valid[:,1,1].shape) \n",
    "\n",
    "batch_sz = 1024\n",
    "\n",
    "classification_model.fit_generator(batch_generator(X_train, Y_train, batch_sz), \n",
    "                                   steps_per_epoch = 172,\n",
    "                                   epochs = 1,\n",
    "                                   verbose = 1,\n",
    "                                   validation_data = ([A_valid, P_valid, N_valid], zeros_vect_valid),\n",
    "                                  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1/1 [==============================] - 32s 32s/step - loss: 0.3644 - val_loss: 3.8890\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.88902, saving model to Weights\\weights.01-3.89.hdf5\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-dee2ee9e8005>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m                                    \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                                    \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mA_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mP_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_valid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzeros_vect_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m                                    \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcallbacks_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m                                   )\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2250\u001b[0m                                 \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2251\u001b[0m                                 \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_sample_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2252\u001b[1;33m                                 verbose=0)\n\u001b[0m\u001b[0;32m   2253\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2254\u001b[0m                             \u001b[0mval_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[0;32m   1777\u001b[0m                                \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1778\u001b[0m                                \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1779\u001b[1;33m                                steps=steps)\n\u001b[0m\u001b[0;32m   1780\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1781\u001b[0m     def predict(self, x,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_test_loop\u001b[1;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1424\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1425\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1426\u001b[1;33m                 \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1427\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1428\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2478\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2479\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1140\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1141\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1321\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1312\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[0;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1420\u001b[1;33m             status, run_metadata)\n\u001b[0m\u001b[0;32m   1421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1422\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_sz = 32\n",
    "\n",
    "classification_model.fit_generator(batch_generator(X_train, Y_train, batch_sz), \n",
    "                                   steps_per_epoch = 1,\n",
    "                                   epochs = 20,\n",
    "                                   verbose = 1,\n",
    "                                   validation_data = ([A_valid, P_valid, N_valid], zeros_vect_valid),\n",
    "                                   callbacks = callbacks_list\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model.load_weights(\"weights.03-0.10.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison with Modified Hausdorff Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from demo_classification import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNNDistance(itemA, itemB):\n",
    "    itemA = itemA.reshape(1, itemA.shape[0], itemA.shape[1], 1)\n",
    "    itemB = itemB.reshape(1, itemB.shape[0], itemB.shape[1], 1)\n",
    "    itemA_emb = emb_model.predict_on_batch(itemA)\n",
    "    itemB_emb = emb_model.predict_on_batch(itemB)\n",
    "    dist = np.linalg.norm(itemA_emb - itemB_emb)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadImgAsArray(fn):\n",
    "\t# Load image file, return as array and resize\n",
    "    picture = mpimg.imread(fn)\n",
    "    image = resize(picture, (SZ,SZ), mode='constant')\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-shot classification with Modified Hausdorff Distance versus Siamese triplet loss Distance\n",
      " run 1 ModHausdorffDistance(error 45.0%)  -  Siamese_triplet_loss_Distance (error 30.0%)\n",
      " run 2 ModHausdorffDistance(error 35.0%)  -  Siamese_triplet_loss_Distance (error 30.0%)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-4bada32b015c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m                 \u001b[0mrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'0'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mperror\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassification_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'one-shot-classification'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'/run'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLoadImgAsPoints\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModHausdorffDistance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'cost'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mperror_cnn\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassification_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'one-shot-classification'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'run'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLoadImgAsArray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCNNDistance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'cost'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\" run \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" ModHausdorffDistance\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"(error \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m    \u001b[0mperror\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"%)\"\u001b[0m\u001b[1;33m+\u001b[0m \u001b[1;34m\"  -  Siamese_triplet_loss_Distance\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" (error \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m  \u001b[0mperror_cnn\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"%)\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mtotal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mperror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\fastai\\Omniplot\\omniglot-master\\python\\demo_classification.py\u001b[0m in \u001b[0;36mclassification_run\u001b[1;34m(main_folder, folder, f_load, f_cost, ftype)\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mntest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mntrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m                         \u001b[0mcostM\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf_cost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_items\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_items\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mftype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'cost'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m                 \u001b[0mYHAT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcostM\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-39-ef2d24553828>\u001b[0m in \u001b[0;36mCNNDistance\u001b[1;34m(itemA, itemB)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mitemA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitemA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitemA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitemA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mitemB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitemB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitemB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitemB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mitemA_emb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0memb_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitemA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mitemB_emb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0memb_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitemB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mdist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitemA_emb\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mitemB_emb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict_on_batch\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m   1943\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1944\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_predict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1945\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1946\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1947\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2478\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2479\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1107\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1108\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1109\u001b[1;33m             \u001b[0mnp_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1111\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m     \"\"\"\n\u001b[1;32m--> 492\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print ('One-shot classification with Modified Hausdorff Distance versus Siamese triplet loss Distance')\n",
    "perror = np.zeros(nrun)\n",
    "perror_cnn =np.zeros(nrun)\n",
    "for r in range(1,nrun+1):\n",
    "\trs = str(r)\n",
    "\tif len(rs)==1:\n",
    "\t\trs = '0' + rs\t\t\n",
    "\tperror[r-1] = classification_run('one-shot-classification','/run'+rs, LoadImgAsPoints, ModHausdorffDistance, 'cost')\n",
    "\tperror_cnn[r-1] = classification_run('one-shot-classification','run'+rs, LoadImgAsArray, CNNDistance, 'cost')\n",
    "\tprint (\" run \" + str(r) + \" ModHausdorffDistance\" + \"(error \" + str(\tperror[r-1] ) + \"%)\"+ \"  -  Siamese_triplet_loss_Distance\" + \" (error \" + str(\tperror_cnn[r-1] ) + \"%)\")\t\t\n",
    "total = np.mean(perror)\n",
    "total_cnn = np.mean(perror_cnn)\n",
    "print (\" average error ModHausdorffDistance \" + str(total) + \"%\" + \"  average error Siamese_triplet_loss_Distance \" + str(total_cnn) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
