{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few-shot learning - Omniglot - Fellowship.ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fellowship.ai - Few-shot learning\n",
    "This is my personal project to the few-shot learning challenge from [Fellowship.ai](https://fellowship.ai/challenge/) with the following goal:\n",
    "> Omniglot, the “transpose” of MNIST, with 1623 character classes, each with 20 examples.  Build a few-shot classifier with a target of <35% error.\n",
    "\n",
    "#### Omniglot - Dataset\n",
    "*Dataset reference:* [Link](https://github.com/brendenlake/omniglot)\n",
    "> Lake, B. M., Salakhutdinov, R., and Tenenbaum, J. B. (2015). Human-level concept learning through probabilistic program induction. Science, 350(6266), 1332-1338.\n",
    "\n",
    "The Omniglot dataset is often considered as the transpose of the MNIST dataset. While the latter contains only 10 classes with a training set of 60000 examples, Omniglot contains an important number of classes (1623 different handwritten characters from 50 different alphabets) with only a low number of examples (20) for each, making it an ideal dataset for few-shot learning problems.\n",
    "\n",
    "#### Few-shot learning\n",
    "Whereas, lots of deep learning projects are based on a huge number of training examples to be trained, few-shot learning is  based only on a few one. This approach is much closer to the one experienced by humans. We are able to memorize and recognize objects we have never seen before from a few number of examples. Then for each new encounter with these types of object we can classify them in an accurate and easy way.\n",
    "\n",
    "\n",
    "#### Stategy\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xyrion\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, concatenate, MaxPooling2D, Dropout, Lambda\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras import regularizers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from PIL import Image                                                                                                                               \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import glob\n",
    "from skimage.transform import resize\n",
    "import h5py\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We indicate the PATH of the training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training images path\n",
    "PATH=\"images_background/\"\n",
    "\n",
    "#Validation and test images path\n",
    "PATH_TEST = \"images_evaluation/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a look at the list of all alphabets contained in the training set, and their total number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of the different alphabets:\n",
      "\n",
      " ['Alphabet_of_the_Magi' 'Anglo-Saxon_Futhorc' 'Arcadian' 'Armenian'\n",
      " 'Asomtavruli_(Georgian)' 'Balinese' 'Bengali'\n",
      " 'Blackfoot_(Canadian_Aboriginal_Syllabics)' 'Braille' 'Burmese_(Myanmar)'\n",
      " 'Cyrillic' 'Early_Aramaic' 'Futurama' 'Grantha' 'Greek' 'Gujarati'\n",
      " 'Hebrew' 'Inuktitut_(Canadian_Aboriginal_Syllabics)'\n",
      " 'Japanese_(hiragana)' 'Japanese_(katakana)' 'Korean' 'Latin'\n",
      " 'Malay_(Jawi_-_Arabic)' 'Mkhedruli_(Georgian)' 'N_Ko'\n",
      " 'Ojibwe_(Canadian_Aboriginal_Syllabics)' 'Sanskrit' 'Syriac_(Estrangelo)'\n",
      " 'Tagalog' 'Tifinagh']\n",
      "\n",
      "Number of different alphabets: 30\n"
     ]
    }
   ],
   "source": [
    "alph_type = np.array(os.listdir(PATH)) #Give the different types of alphabet in our data\n",
    "print(\"List of the different alphabets:\\n\\n {}\".format(alph_type))\n",
    "print(\"\\nNumber of different alphabets: {}\".format(len(alph_type)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then check the number of character for each alphabets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of chracters corresponding to each alphabet is : \n",
      "{'Alphabet_of_the_Magi': 20, 'Anglo-Saxon_Futhorc': 29, 'Arcadian': 26, 'Armenian': 41, 'Asomtavruli_(Georgian)': 40, 'Balinese': 24, 'Bengali': 46, 'Blackfoot_(Canadian_Aboriginal_Syllabics)': 14, 'Braille': 26, 'Burmese_(Myanmar)': 34, 'Cyrillic': 33, 'Early_Aramaic': 22, 'Futurama': 26, 'Grantha': 43, 'Greek': 24, 'Gujarati': 48, 'Hebrew': 22, 'Inuktitut_(Canadian_Aboriginal_Syllabics)': 16, 'Japanese_(hiragana)': 52, 'Japanese_(katakana)': 47, 'Korean': 40, 'Latin': 26, 'Malay_(Jawi_-_Arabic)': 40, 'Mkhedruli_(Georgian)': 41, 'N_Ko': 33, 'Ojibwe_(Canadian_Aboriginal_Syllabics)': 14, 'Sanskrit': 42, 'Syriac_(Estrangelo)': 23, 'Tagalog': 17, 'Tifinagh': 55}\n",
      "\n",
      "The maximum number of different character for one alphabet is 55\n",
      "The minimum number of different character for one alphabet is 14\n",
      "The total number of different character is 964\n"
     ]
    }
   ],
   "source": [
    "alph_num_char ={}\n",
    "for alphabet in alph_type:\n",
    "    alph_num_char[alphabet]= len(os.listdir(f'{PATH}{alphabet}'))\n",
    "print(\"The number of chracters corresponding to each alphabet is : \\n{}\".format(alph_num_char))\n",
    "\n",
    "num_of_char = alph_num_char.values()\n",
    "print('\\nThe maximum number of different character for one alphabet is {}'.format(max(num_of_char)))\n",
    "print('The minimum number of different character for one alphabet is {}'.format(min(num_of_char)))\n",
    "total_char = sum(num_of_char)\n",
    "print('The total number of different character is {}'.format(total_char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum number of different examples for one character is 20\n",
      "The minimum number of different examples for one character is 20\n",
      "The total number of different pictures is 19280\n"
     ]
    }
   ],
   "source": [
    "alph_num_char_ex={}\n",
    "class_num=0\n",
    "for alphabet in alph_type:\n",
    "    char_list=os.listdir(f'{PATH}{alphabet}')\n",
    "    for char in char_list:\n",
    "        alph_num_char_ex[(alphabet,char)]= len(os.listdir(f'{PATH}{alphabet}/{char}'))\n",
    "\n",
    "num_of_example = alph_num_char_ex.values()\n",
    "print('The maximum number of different examples for one character is {}'.format(max(num_of_example)))\n",
    "print('The minimum number of different examples for one character is {}'.format(min(num_of_example)))\n",
    "total_example = sum(num_of_example) \n",
    "print('The total number of different pictures is {}'.format(total_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that each character have 20 examples(pictures)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add label for each character\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each picture in our dataset we give a corresponding label(an integer) which allow us to determine the corresponding character. Here an integer is sufficient as we are not really interested in knowing from which alphabet an image is coming from and as we don't have need to know the character name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19280,)\n"
     ]
    }
   ],
   "source": [
    "Y_train=np.array([])\n",
    "for i in range(total_char):\n",
    "    #As each character have 20 examples\n",
    "    Y_train= np.concatenate((Y_train, np.ones(20)*(i+1))) \n",
    "Y_train = Y_train.astype(int)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reshape, our data to have the number of channel including (here is 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19280, 1)\n"
     ]
    }
   ],
   "source": [
    "Y_train=Y_train.reshape(Y_train.shape[0],1)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert images to datafile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first retrieve the path for each picture in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['images_background\\\\Alphabet_of_the_Magi\\\\character01\\\\0709_01.png',\n",
       " 'images_background\\\\Alphabet_of_the_Magi\\\\character01\\\\0709_02.png',\n",
       " 'images_background\\\\Alphabet_of_the_Magi\\\\character01\\\\0709_03.png',\n",
       " 'images_background\\\\Alphabet_of_the_Magi\\\\character01\\\\0709_04.png',\n",
       " 'images_background\\\\Alphabet_of_the_Magi\\\\character01\\\\0709_05.png']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagePath = glob.glob(\"{}*/*/*.png\".format(PATH))\n",
    "imagePath[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print some random images of the dataset, convert them to arrays and resize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We decide to resize our images to the following "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imagePath = glob.glob(\"{}*/*/*.png\".format(PATH))\n",
    "\n",
    "\n",
    "SZ=60 #Dimension of the output image expected\n",
    "\n",
    "#Dimensions of the grill of sample pictures\n",
    "columns = 4\n",
    "rows = 5\n",
    "\n",
    "fig=plt.figure(figsize=(5, 5))\n",
    "list_example = np.random.randint(total_example, size = columns*rows)\n",
    "pos=0\n",
    "for i in list_example:\n",
    "    pos+=1\n",
    "    img = mpimg.imread(imagePath[i])\n",
    "    img = resize(img, (SZ,SZ), mode='reflect')\n",
    "    fig.add_subplot(rows, columns, pos)\n",
    "    plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['images_evaluation\\\\Angelic\\\\character01\\\\0965_01.png',\n",
       " 'images_evaluation\\\\Angelic\\\\character01\\\\0965_02.png',\n",
       " 'images_evaluation\\\\Angelic\\\\character01\\\\0965_03.png',\n",
       " 'images_evaluation\\\\Angelic\\\\character01\\\\0965_04.png',\n",
       " 'images_evaluation\\\\Angelic\\\\character01\\\\0965_05.png']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testPath = glob.glob(\"{}*/*/*.png\".format(PATH_TEST))\n",
    "testPath[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All images seems clear and well centered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert all images into arrays and resize them to the 60x60 format. We concatenate all arrays into the variable im_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19280, 60, 60)\n"
     ]
    }
   ],
   "source": [
    "#Transform in array and resize all 19280 images \n",
    "X_train = np.array([resize(mpimg.imread(i), (SZ,SZ), mode='reflect') for i in imagePath] )\n",
    "print (X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reshape the image to take into account the number of channel to pass them in our CNN, which is 1 here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19280, 60, 60, 1)\n"
     ]
    }
   ],
   "source": [
    "channel_sz = 1 #number of channel\n",
    "X_train= X_train.reshape(X_train.shape[0],X_train.shape[1],X_train.shape[2],channel_sz)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13180, 60, 60)\n",
      "(13180, 60, 60, 1)\n"
     ]
    }
   ],
   "source": [
    "test_array = np.array([resize(mpimg.imread(i), (SZ,SZ), mode='reflect') for i in testPath] )\n",
    "print (test_array.shape)\n",
    "\n",
    "test_array= test_array.reshape(test_array.shape[0],test_array.shape[1],test_array.shape[2],channel_sz)\n",
    "print(test_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13180,)\n",
      "(13180, 1)\n"
     ]
    }
   ],
   "source": [
    "class_char_test=np.array([])\n",
    "for i in range(13180//20):\n",
    "    #As each character have 20 examples\n",
    "    class_char_test= np.concatenate((class_char_test, np.ones(20)*(i+1))) \n",
    "class_char_test = class_char_test.astype(int)\n",
    "print(class_char_test.shape)\n",
    "\n",
    "class_char_test=class_char_test.reshape(class_char_test.shape[0],1)\n",
    "print(class_char_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train/ validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the traning set into Train and Validation sets of pictures (Train : 70%, Validation : 30%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_valid, Y_train, Y_valid = train_test_split(im_array, class_char, test_size=0.3, stratify= class_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#X_train.shape, X_valid.shape, Y_train.shape, Y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3954, 60, 60, 1), (9226, 60, 60, 1), (3954, 1), (9226, 1))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test, X_eval, Y_test, Y_eval = train_test_split(test_array, class_char_test, test_size=0.7, stratify= class_char_test)\n",
    "X_test.shape, X_eval.shape, Y_test.shape, Y_eval.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triplet loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our loss function is determined by the following triplet loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(y_true, y_pred, alpha = 0.2):\n",
    "    \"\"\"\n",
    "    Implementation of the triplet loss\n",
    "    \n",
    "    Arguments:\n",
    "    y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.\n",
    "    y_pred -- python list containing three objects:\n",
    "            anchor -- the encodings for the anchor images, of shape (None, 64)\n",
    "            positive -- the encodings for the positive images, of shape (None, 64)\n",
    "            negative -- the encodings for the negative images, of shape (None, 64)\n",
    "    \n",
    "    Returns:\n",
    "    loss -- real number, value of the loss\n",
    "    \"\"\"\n",
    "    \n",
    "    anchor, positive, negative = y_pred[:,0:64], y_pred[:,64:128], y_pred[:,128:256]\n",
    "    \n",
    "    #Compute the (encoding) distance between the anchor and the positive, you will need to sum over axis=-1\n",
    "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), axis = -1)\n",
    "    #Compute the (encoding) distance between the anchor and the negative, you will need to sum over axis=-1\n",
    "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), axis = -1)\n",
    "    #Subtract the two previous distances and add alpha.\n",
    "    basic_loss = tf.add(tf.subtract(pos_dist, neg_dist), alpha)\n",
    "    # Count number of positive triplets (where triplet_loss > 0)\n",
    "    valid_triplets = tf.to_float(tf.greater(basic_loss,0))\n",
    "    num_positive_triplets = tf.reduce_sum(valid_triplets)\n",
    "    #Take the maximum of basic_loss and 0.0. Sum over the training examples.\n",
    "    loss = tf.reduce_sum(tf.maximum(basic_loss, 0))/ alpha * 100 / (num_positive_triplets + 1e-16)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_acc(y_true, y_pred, alpha = 0.2):\n",
    "    \"\"\"\n",
    "    Implementation of the triplet loss\n",
    "    \n",
    "    Arguments:\n",
    "    y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.\n",
    "    y_pred -- python list containing three objects:\n",
    "            anchor -- the encodings for the anchor images, of shape (None, 64)\n",
    "            positive -- the encodings for the positive images, of shape (None, 64)\n",
    "            negative -- the encodings for the negative images, of shape (None, 64)\n",
    "    \n",
    "    Returns:\n",
    "    loss -- real number, value of the loss\n",
    "    \"\"\"\n",
    "    \n",
    "    anchor, positive, negative = y_pred[:,0:64], y_pred[:,64:128], y_pred[:,128:256]\n",
    "    \n",
    "    #Compute the (encoding) distance between the anchor and the positive, you will need to sum over axis=-1\n",
    "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), axis = -1)\n",
    "    #Compute the (encoding) distance between the anchor and the negative, you will need to sum over axis=-1\n",
    "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), axis = -1)\n",
    "    #Subtract the two previous distances and add alpha.\n",
    "    basic_loss = tf.add(tf.subtract(pos_dist, neg_dist), alpha)\n",
    "    # Count number of positive triplets (where triplet_loss > 0)\n",
    "    hard_triplets = tf.to_float(tf.greater(basic_loss,alpha))\n",
    "    num_hard_triplets = tf.reduce_sum(hard_triplets)\n",
    "    all_triplets = tf.reduce_sum(tf.to_float(tf.greater(basic_loss,-10**10)))\n",
    "    #Take the maximum of basic_loss and 0.0. Sum over the training examples.\n",
    "    acc = 1 - num_hard_triplets/all_triplets\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Triplet Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_triplets(X, Y, num=1):\n",
    "    \"\"\"\n",
    "    Create a list of valid triplets\n",
    "    \n",
    "    Arguments:\n",
    "    X -- array of images\n",
    "    Y -- array of classes corresponding to each image\n",
    "    num -- number of negative images for each valid anchor and positive images - must be positive\n",
    "           if num = 0, all possible valid couples are created\n",
    "            For example : for one valid (A,P) couple we can select 'num' random N images. \n",
    "                          Thus 'num' triplets are created for this (A,P) couple\n",
    "\n",
    "    \n",
    "    Returns:\n",
    "    (A,P,N) -- python tuple containing 3 arrays : \n",
    "            A -- the array for the anchor images, of shape (None, 64)\n",
    "            P -- the array for the positive images, of shape (None, 64)\n",
    "            N -- the array for the negative images, of shape (None, 64)\n",
    "    \"\"\"\n",
    "\n",
    "    Y = Y.reshape(Y.shape[0],)\n",
    "    A = []\n",
    "    P = []\n",
    "    N = []\n",
    "    \n",
    "    #We loop over all possible valid (A,P)\n",
    "    for i in range(X.shape[0]):  \n",
    "        list_pos = X[Y==Y[i]]\n",
    "        for j in list_pos:\n",
    "            #We provide a number 'num' of triplets for each valid (A,P)\n",
    "            if num >=1:\n",
    "                for k in range(num):\n",
    "                    rand_num = np.random.randint(X.shape[0])\n",
    "                    if np.array_equal(X[i],j) == False:\n",
    "                        A.append(X[i])\n",
    "                        P.append(j)\n",
    "                        while np.array_equal(Y[rand_num], Y[i]):\n",
    "                            rand_num = np.random.randint(X.shape[0])\n",
    "                        N.append(X[rand_num])\n",
    "            if num == 0:\n",
    "                for k in range(X.shape[0]):\n",
    "                    if np.array_equal(X[i],j) == False:\n",
    "                        if np.array_equal(Y[i],Y[k]) == False:\n",
    "                            A.append(X[i])\n",
    "                            P.append(j)\n",
    "                            N.append(X[k])\n",
    "    \n",
    "    A = np.array(A)\n",
    "    P = np.array(P)\n",
    "    N = np.array(N)\n",
    "    \n",
    "    return (A, P, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'triplets_list_valid = create_triplets(X_valid,Y_valid)\\nfor i in range(len(triplets_list_valid)):\\n    print(triplets_list_valid[i].shape)'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We create one triplet for each of the possible (A,P) couple in our validation set \n",
    "\"\"\"triplets_list_valid = create_triplets(X_valid,Y_valid)\n",
    "for i in range(len(triplets_list_valid)):\n",
    "    print(triplets_list_valid[i].shape)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19770, 60, 60, 1)\n",
      "(19770, 60, 60, 1)\n",
      "(19770, 60, 60, 1)\n"
     ]
    }
   ],
   "source": [
    "triplets_list_test = create_triplets(X_test, Y_test)\n",
    "for i in range(len(triplets_list_test)):\n",
    "    print(triplets_list_test[i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a number of 28920 examples for the validation. Thus 1 negative image per (A,P) couple is enough for our evaluation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EmbModel(input_shape, drop1=0, drop2=1/32):\n",
    "    \"\"\"\n",
    "    Define our shared embedding model\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of array of input images\n",
    "\n",
    "    \n",
    "    Returns:\n",
    "    model - Our model which transform an array of images into an array of embeddings\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Define the input placeholder as a tensor with shape input_shape.\n",
    "    X_input = Input(input_shape)\n",
    "    \n",
    "    #Padding\n",
    "    X = ZeroPadding2D((1,1))(X_input)\n",
    "    \n",
    "    #CONV\n",
    "    X = Conv2D(16,(3,3),strides =(1,1), name ='conv0', kernel_initializer='glorot_uniform') (X)\n",
    "    X = BatchNormalization(axis = 1, name = \"bn0\") (X)\n",
    "    X = Activation('relu', name='a0')(X)\n",
    "    \n",
    "    #MAXPOOL\n",
    "    X = MaxPooling2D((2,2), name='max_pool0')(X)\n",
    "    \n",
    "    X = Dropout(drop1)(X)\n",
    "    \n",
    "    #Padding\n",
    "    #X = ZeroPadding2D((1,1))(X)\n",
    "    \n",
    "     #CONV\n",
    "    X = Conv2D(32,(3,3),strides =(1,1), name ='conv1', kernel_initializer='glorot_uniform') (X)\n",
    "    X = BatchNormalization(axis = 1, name = \"bn1\") (X)\n",
    "    X = Activation('relu', name='a1')(X)\n",
    "    \n",
    "    #MAXPOOL\n",
    "    X = MaxPooling2D((2,2), name='max_pool1')(X)\n",
    "    \n",
    "    X = Dropout(drop1)(X)\n",
    "    \n",
    "    #Padding\n",
    "    #X = ZeroPadding2D((1,1))(X)\n",
    "    \n",
    "     #CONV\n",
    "    X = Conv2D(64,(3,3),strides =(1,1), name ='conv2', kernel_initializer='glorot_uniform') (X)\n",
    "    X = BatchNormalization(axis = 1, name = \"bn2\") (X)\n",
    "    X = Activation('relu', name='a2')(X)\n",
    "    \n",
    "     #MAXPOOL\n",
    "    X = MaxPooling2D((2,2), name='max_pool2')(X)\n",
    "    \n",
    "    X = Dropout(drop1)(X)\n",
    "    \n",
    "    #Padding\n",
    "    #X = ZeroPadding2D((1,1))(X)\n",
    "    \n",
    "     #CONV\n",
    "    X = Conv2D(128,(3,3),strides =(1,1), name ='conv3', kernel_initializer='glorot_uniform') (X)\n",
    "    X = BatchNormalization(axis = 1, name = \"bn3\") (X)\n",
    "    X = Activation('relu', name='a3')(X)\n",
    "    \n",
    "     #MAXPOOL\n",
    "    X = MaxPooling2D((2,2), name='max_pool3')(X)\n",
    "    \n",
    "    #X = Dropout(0.2)(X)\n",
    "    \n",
    "    #FLATTEN X + FC\n",
    "    X = Flatten(name='f3')(X)\n",
    "    #X = Dense (256, activation ='relu', name='fc4', kernel_initializer='glorot_uniform') (X)\n",
    "    X = Dropout(drop2)(X)\n",
    "    X = Dense (64, activation ='relu', name='fc5', kernel_initializer='glorot_uniform') (X)\n",
    "    X = Lambda(lambda  x: tf.nn.l2_normalize(x,axis=1))(X)\n",
    "    \n",
    "    ##Create model\n",
    "    model = Model(inputs = X_input, outputs = X, name='EmbModel')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We define our global model\n",
    "def global_model(size, channel_size=1, drop1=0, drop2=1/32):\n",
    "    input_size = (size, size, channel_sz)                     \n",
    "\n",
    "    A = Input(input_size)\n",
    "    P = Input(input_size)\n",
    "    N = Input(input_size)\n",
    "\n",
    "    emb_model= EmbModel(input_size)\n",
    "\n",
    "    out_A = emb_model(A)\n",
    "    out_P = emb_model(P)\n",
    "    out_N = emb_model(N)\n",
    "\n",
    "    y_pred = concatenate([out_A, out_P, out_N], axis =-1)\n",
    "\n",
    "    full_model = Model(inputs = [A, P, N], outputs = y_pred)\n",
    "    \n",
    "    return full_model, emb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit(model, b_size = 32, ep = 50):\n",
    "\n",
    "    A,P,N = triplets_list_train\n",
    "    A_test, P_test, N_test = triplets_list_test\n",
    "    zeros_vect = np.zeros(A[:,1,1].shape)\n",
    "    zeros_vect_test = np.zeros(A_test[:,1,1].shape) \n",
    "    class_model.fit(x = [A, P, N] , \n",
    "                             y = zeros_vect , \n",
    "                             batch_size = b_size, \n",
    "                             epochs = ep,\n",
    "                             validation_data = ([A_test, P_test, N_test], zeros_vect_test), \n",
    "                             shuffle = True)\n",
    "    return class_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model, emb_model = global_model(SZ,channel_sz,drop1 = 0.3, drop2 = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 60, 60, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 60, 60, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 60, 60, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "EmbModel (Model)                (None, 64)           130400      input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 192)          0           EmbModel[1][0]                   \n",
      "                                                                 EmbModel[2][0]                   \n",
      "                                                                 EmbModel[3][0]                   \n",
      "==================================================================================================\n",
      "Total params: 130,400\n",
      "Trainable params: 130,192\n",
      "Non-trainable params: 208\n",
      "__________________________________________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 60, 60, 1)         0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 62, 62, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv0 (Conv2D)               (None, 60, 60, 16)        160       \n",
      "_________________________________________________________________\n",
      "bn0 (BatchNormalization)     (None, 60, 60, 16)        240       \n",
      "_________________________________________________________________\n",
      "a0 (Activation)              (None, 60, 60, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pool0 (MaxPooling2D)     (None, 30, 30, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 30, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 28, 28, 32)        4640      \n",
      "_________________________________________________________________\n",
      "bn1 (BatchNormalization)     (None, 28, 28, 32)        112       \n",
      "_________________________________________________________________\n",
      "a1 (Activation)              (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pool1 (MaxPooling2D)     (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 12, 12, 64)        18496     \n",
      "_________________________________________________________________\n",
      "bn2 (BatchNormalization)     (None, 12, 12, 64)        48        \n",
      "_________________________________________________________________\n",
      "a2 (Activation)              (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pool2 (MaxPooling2D)     (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv3 (Conv2D)               (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "bn3 (BatchNormalization)     (None, 4, 4, 128)         16        \n",
      "_________________________________________________________________\n",
      "a3 (Activation)              (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pool3 (MaxPooling2D)     (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "f3 (Flatten)                 (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "fc5 (Dense)                  (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 64)                0         \n",
      "=================================================================\n",
      "Total params: 130,400\n",
      "Trainable params: 130,192\n",
      "Non-trainable params: 208\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classification_model.summary()\n",
    "emb_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X,Y, bs=32):\n",
    "    \"\"\"\n",
    "    Create a mini-batch generator\n",
    "    \n",
    "    Arguments:\n",
    "    X -- array of images\n",
    "    Y -- array of classes corresponding to each image\n",
    "    bs -- size of the minibatch\n",
    "\n",
    "    \n",
    "    Returns:\n",
    "    [A_batch, P_batch, N_batch], y_dummie) -- a mini-batch of size bs\n",
    "    \"\"\"\n",
    "    \n",
    "    Y = Y.reshape(Y.shape[0],)\n",
    "    while True:\n",
    "        #0. Initialize Anchor,Postive, Negative\n",
    "        A_batch = []\n",
    "        P_batch = []\n",
    "        N_batch = []\n",
    "        for i in range(bs):      \n",
    "            #1.Choose a random Anchor Image\n",
    "            rand_A_num = np.random.randint(X.shape[0])\n",
    "            A_batch.append(X[rand_A_num])\n",
    "            \n",
    "            #2.Choose a random Positive Image\n",
    "            list_pos = X[Y==Y[rand_A_num]]                            #List of positive images\n",
    "            rand_P_num = np.random.randint(len(list_pos))\n",
    "            P_batch.append(list_pos[rand_P_num])\n",
    "            \n",
    "            #3.Choose a random Negative Image\n",
    "            rand_N_num = np.random.randint(X.shape[0])\n",
    "            while np.array_equal(Y[rand_N_num], Y[rand_A_num]):\n",
    "                rand_A_num = np.random.randint(X.shape[0])\n",
    "            N_batch.append(X[rand_N_num])\n",
    "            \n",
    "        A_batch = np.array(A_batch)\n",
    "        P_batch = np.array(P_batch)\n",
    "        N_batch = np.array(N_batch)\n",
    "        \n",
    "        y_dummie = np.zeros((len(A_batch),))\n",
    "        \n",
    "        yield ([A_batch, P_batch, N_batch], y_dummie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real shit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We compile our model with the custom made triplet_loss\n",
    "classification_model.compile(optimizer = 'adam', loss = triplet_loss, metrics =[triplet_acc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint and early stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create a checkpoint to save the best model and add an early stopping\n",
    "filepath = \"Weights/weights.{epoch:03d}-{val_triplet_acc:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_triplet_acc', verbose=1, save_best_only=True, mode='max')\n",
    "early_stop = EarlyStopping(monitor='val_triplet_acc', patience=20, verbose=1, mode='max')\n",
    "callbacks_list = [checkpoint, early_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "   96/11448 [..............................] - ETA: 52:47 - loss: 81.5795 - triplet_acc: 0.8636"
     ]
    }
   ],
   "source": [
    "#Full network\n",
    "A_test, P_test, N_test = triplets_list_test\n",
    "zeros_vect_test = np.zeros(A_test[:,1,1].shape) \n",
    "\n",
    "\n",
    "batch_sz = 32\n",
    "\n",
    "classification_model.fit_generator(batch_generator(X_train, Y_train, batch_sz), \n",
    "                                   steps_per_epoch = 11448,\n",
    "                                   epochs = 200,\n",
    "                                   verbose = 1,\n",
    "                                   validation_data = ([A_test, P_test, N_test], zeros_vect_test)\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "143/143 [==============================] - 172s 1s/step - loss: 60.7583 - triplet_acc: 0.9963 - val_loss: 78.4933 - val_triplet_acc: 0.9860\n",
      "\n",
      "Epoch 00001: val_triplet_acc improved from -inf to 0.98599, saving model to Weights/weights.001-0.9860.hdf5\n",
      "Epoch 2/200\n",
      "143/143 [==============================] - 172s 1s/step - loss: 59.4852 - triplet_acc: 0.9961 - val_loss: 77.2877 - val_triplet_acc: 0.9848\n",
      "\n",
      "Epoch 00002: val_triplet_acc did not improve\n",
      "Epoch 3/200\n",
      "143/143 [==============================] - 172s 1s/step - loss: 61.6055 - triplet_acc: 0.9967 - val_loss: 75.4602 - val_triplet_acc: 0.9870\n",
      "\n",
      "Epoch 00003: val_triplet_acc improved from 0.98599 to 0.98700, saving model to Weights/weights.003-0.9870.hdf5\n",
      "Epoch 4/200\n",
      "143/143 [==============================] - 172s 1s/step - loss: 59.4632 - triplet_acc: 0.9963 - val_loss: 79.4516 - val_triplet_acc: 0.9859\n",
      "\n",
      "Epoch 00004: val_triplet_acc did not improve\n",
      "Epoch 5/200\n",
      "143/143 [==============================] - 172s 1s/step - loss: 57.7383 - triplet_acc: 0.9966 - val_loss: 79.5234 - val_triplet_acc: 0.9850\n",
      "\n",
      "Epoch 00005: val_triplet_acc did not improve\n",
      "Epoch 6/200\n",
      "143/143 [==============================] - 172s 1s/step - loss: 61.8464 - triplet_acc: 0.9961 - val_loss: 73.2807 - val_triplet_acc: 0.9851\n",
      "\n",
      "Epoch 00006: val_triplet_acc did not improve\n",
      "Epoch 7/200\n",
      " 22/143 [===>..........................] - ETA: 1:54 - loss: 46.0886 - triplet_acc: 0.9982"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-88-bb4d1c934f55>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m                                    \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m                                    \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mA_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mP_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_test\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzeros_vect_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m                                    \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcallbacks_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m                                   )\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2222\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   2223\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2224\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   2225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2226\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1883\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1884\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1885\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2478\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2479\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1140\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1141\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1321\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1312\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[0;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1420\u001b[1;33m             status, run_metadata)\n\u001b[0m\u001b[0;32m   1421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1422\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#0.3 ALL\n",
    "A_test, P_test, N_test = triplets_list_test\n",
    "zeros_vect_test = np.zeros(A_test[:,1,1].shape) \n",
    "\n",
    "\n",
    "batch_sz = 256\n",
    "\n",
    "classification_model.fit_generator(batch_generator(X_train, Y_train, batch_sz), \n",
    "                                   steps_per_epoch = 1432,\n",
    "                                   epochs = 200,\n",
    "                                   verbose = 1,\n",
    "                                   validation_data = ([A_test, P_test, N_test], zeros_vect_test),\n",
    "                                   callbacks = callbacks_list\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "344/344 [==============================] - 370s 1s/step - loss: 73.6986 - triplet_acc: 0.9553 - val_loss: 70.9202 - val_triplet_acc: 0.9652\n",
      "\n",
      "Epoch 00001: val_triplet_acc did not improve\n",
      "Epoch 2/200\n",
      "344/344 [==============================] - 363s 1s/step - loss: 66.7036 - triplet_acc: 0.9848 - val_loss: 65.5016 - val_triplet_acc: 0.9738\n",
      "\n",
      "Epoch 00002: val_triplet_acc did not improve\n",
      "Epoch 3/200\n",
      "344/344 [==============================] - 363s 1s/step - loss: 65.5337 - triplet_acc: 0.9894 - val_loss: 72.0153 - val_triplet_acc: 0.9762\n",
      "\n",
      "Epoch 00003: val_triplet_acc did not improve\n",
      "Epoch 4/200\n",
      "344/344 [==============================] - 362s 1s/step - loss: 65.1847 - triplet_acc: 0.9911 - val_loss: 74.1553 - val_triplet_acc: 0.9818\n",
      "\n",
      "Epoch 00004: val_triplet_acc did not improve\n",
      "Epoch 5/200\n",
      "344/344 [==============================] - 363s 1s/step - loss: 62.3307 - triplet_acc: 0.9930 - val_loss: 67.7295 - val_triplet_acc: 0.9795\n",
      "\n",
      "Epoch 00005: val_triplet_acc did not improve\n",
      "Epoch 6/200\n",
      "344/344 [==============================] - 364s 1s/step - loss: 65.7923 - triplet_acc: 0.9932 - val_loss: 75.4103 - val_triplet_acc: 0.9816\n",
      "\n",
      "Epoch 00006: val_triplet_acc did not improve\n",
      "Epoch 7/200\n",
      "344/344 [==============================] - 363s 1s/step - loss: 63.7418 - triplet_acc: 0.9943 - val_loss: 69.8082 - val_triplet_acc: 0.9812\n",
      "\n",
      "Epoch 00007: val_triplet_acc did not improve\n",
      "Epoch 8/200\n",
      "344/344 [==============================] - 362s 1s/step - loss: 66.8652 - triplet_acc: 0.9938 - val_loss: 70.0085 - val_triplet_acc: 0.9827\n",
      "\n",
      "Epoch 00008: val_triplet_acc did not improve\n",
      "Epoch 9/200\n",
      "344/344 [==============================] - 362s 1s/step - loss: 63.6215 - triplet_acc: 0.9948 - val_loss: 69.1615 - val_triplet_acc: 0.9803\n",
      "\n",
      "Epoch 00009: val_triplet_acc did not improve\n",
      "Epoch 10/200\n",
      "344/344 [==============================] - 362s 1s/step - loss: 62.8020 - triplet_acc: 0.9952 - val_loss: 76.8279 - val_triplet_acc: 0.9848\n",
      "\n",
      "Epoch 00010: val_triplet_acc did not improve\n",
      "Epoch 11/200\n",
      "344/344 [==============================] - 362s 1s/step - loss: 63.1153 - triplet_acc: 0.9953 - val_loss: 68.9866 - val_triplet_acc: 0.9853\n",
      "\n",
      "Epoch 00011: val_triplet_acc did not improve\n",
      "Epoch 12/200\n",
      "344/344 [==============================] - 362s 1s/step - loss: 65.1463 - triplet_acc: 0.9955 - val_loss: 75.2142 - val_triplet_acc: 0.9851\n",
      "\n",
      "Epoch 00012: val_triplet_acc did not improve\n",
      "Epoch 13/200\n",
      "161/344 [=============>................] - ETA: 2:52 - loss: 65.5985 - triplet_acc: 0.9952"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-95-99e62da7bbe2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m                                    \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m                                    \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mA_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mP_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_test\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzeros_vect_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m                                    \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcallbacks_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m                                   )\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2222\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   2223\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2224\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   2225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2226\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1883\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1884\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1885\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2478\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2479\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1140\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1141\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1321\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1312\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[0;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1420\u001b[1;33m             status, run_metadata)\n\u001b[0m\u001b[0;32m   1421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1422\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#1/32 DROPOUT ALL\n",
    "\n",
    "classification_model, emb_model = global_model(SZ,channel_sz,drop1 = 0, drop2 = 1/32)\n",
    "classification_model.compile(optimizer = 'adam', loss = triplet_loss, metrics =[triplet_acc])\n",
    "\n",
    "batch_sz = 256\n",
    "\n",
    "classification_model.fit_generator(batch_generator(X_train, Y_train, batch_sz), \n",
    "                                   steps_per_epoch = 344,\n",
    "                                   epochs = 200,\n",
    "                                   verbose = 1,\n",
    "                                   validation_data = ([A_test, P_test, N_test], zeros_vect_test),\n",
    "                                   callbacks = callbacks_list\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "344/344 [==============================] - 368s 1s/step - loss: 73.8831 - triplet_acc: 0.9555 - val_loss: 76.0018 - val_triplet_acc: 0.9640\n",
      "\n",
      "Epoch 00001: val_triplet_acc did not improve\n",
      "Epoch 2/200\n",
      "344/344 [==============================] - 362s 1s/step - loss: 68.4087 - triplet_acc: 0.9843 - val_loss: 81.3286 - val_triplet_acc: 0.9734\n",
      "\n",
      "Epoch 00002: val_triplet_acc did not improve\n",
      "Epoch 3/200\n",
      "344/344 [==============================] - 362s 1s/step - loss: 64.8603 - triplet_acc: 0.9892 - val_loss: 71.3560 - val_triplet_acc: 0.9803\n",
      "\n",
      "Epoch 00003: val_triplet_acc did not improve\n",
      "Epoch 4/200\n",
      "344/344 [==============================] - 363s 1s/step - loss: 66.5147 - triplet_acc: 0.9909 - val_loss: 77.9457 - val_triplet_acc: 0.9780\n",
      "\n",
      "Epoch 00004: val_triplet_acc did not improve\n",
      "Epoch 5/200\n",
      "344/344 [==============================] - 362s 1s/step - loss: 66.7208 - triplet_acc: 0.9921 - val_loss: 68.8181 - val_triplet_acc: 0.9773\n",
      "\n",
      "Epoch 00005: val_triplet_acc did not improve\n",
      "Epoch 6/200\n",
      "344/344 [==============================] - 362s 1s/step - loss: 64.5280 - triplet_acc: 0.9927 - val_loss: 74.3926 - val_triplet_acc: 0.9802\n",
      "\n",
      "Epoch 00006: val_triplet_acc did not improve\n",
      "Epoch 7/200\n",
      "344/344 [==============================] - 362s 1s/step - loss: 63.5312 - triplet_acc: 0.9937 - val_loss: 71.2929 - val_triplet_acc: 0.9811\n",
      "\n",
      "Epoch 00007: val_triplet_acc did not improve\n",
      "Epoch 8/200\n",
      "344/344 [==============================] - 362s 1s/step - loss: 63.2641 - triplet_acc: 0.9942 - val_loss: 72.8676 - val_triplet_acc: 0.9831\n",
      "\n",
      "Epoch 00008: val_triplet_acc did not improve\n",
      "Epoch 9/200\n",
      "344/344 [==============================] - 362s 1s/step - loss: 64.4131 - triplet_acc: 0.9948 - val_loss: 63.7198 - val_triplet_acc: 0.9838\n",
      "\n",
      "Epoch 00009: val_triplet_acc did not improve\n",
      "Epoch 10/200\n",
      "344/344 [==============================] - 361s 1s/step - loss: 63.5435 - triplet_acc: 0.9949 - val_loss: 79.4518 - val_triplet_acc: 0.9834\n",
      "\n",
      "Epoch 00010: val_triplet_acc did not improve\n",
      "Epoch 11/200\n",
      "344/344 [==============================] - 362s 1s/step - loss: 62.6620 - triplet_acc: 0.9951 - val_loss: 74.4811 - val_triplet_acc: 0.9840\n",
      "\n",
      "Epoch 00011: val_triplet_acc did not improve\n",
      "Epoch 12/200\n",
      "344/344 [==============================] - 361s 1s/step - loss: 61.0666 - triplet_acc: 0.9957 - val_loss: 77.9961 - val_triplet_acc: 0.9859\n",
      "\n",
      "Epoch 00012: val_triplet_acc did not improve\n",
      "Epoch 13/200\n",
      "344/344 [==============================] - 361s 1s/step - loss: 63.2697 - triplet_acc: 0.9957 - val_loss: 72.2046 - val_triplet_acc: 0.9860\n",
      "\n",
      "Epoch 00013: val_triplet_acc did not improve\n",
      "Epoch 14/200\n",
      "344/344 [==============================] - 361s 1s/step - loss: 64.6129 - triplet_acc: 0.9956 - val_loss: 65.7908 - val_triplet_acc: 0.9861\n",
      "\n",
      "Epoch 00014: val_triplet_acc did not improve\n",
      "Epoch 15/200\n",
      "344/344 [==============================] - 361s 1s/step - loss: 63.7177 - triplet_acc: 0.9960 - val_loss: 78.4936 - val_triplet_acc: 0.9847\n",
      "\n",
      "Epoch 00015: val_triplet_acc did not improve\n",
      "Epoch 16/200\n",
      "344/344 [==============================] - 361s 1s/step - loss: 64.2688 - triplet_acc: 0.9962 - val_loss: 74.4860 - val_triplet_acc: 0.9868\n",
      "\n",
      "Epoch 00016: val_triplet_acc did not improve\n",
      "Epoch 17/200\n",
      "344/344 [==============================] - 361s 1s/step - loss: 64.1054 - triplet_acc: 0.9962 - val_loss: 73.3197 - val_triplet_acc: 0.9871\n",
      "\n",
      "Epoch 00017: val_triplet_acc improved from 0.98700 to 0.98705, saving model to Weights/weights.017-0.9871.hdf5\n",
      "Epoch 18/200\n",
      "344/344 [==============================] - 361s 1s/step - loss: 63.4975 - triplet_acc: 0.9966 - val_loss: 71.1992 - val_triplet_acc: 0.9875\n",
      "\n",
      "Epoch 00018: val_triplet_acc improved from 0.98705 to 0.98751, saving model to Weights/weights.018-0.9875.hdf5\n",
      "Epoch 19/200\n",
      "344/344 [==============================] - 361s 1s/step - loss: 63.4709 - triplet_acc: 0.9964 - val_loss: 77.2420 - val_triplet_acc: 0.9840\n",
      "\n",
      "Epoch 00019: val_triplet_acc did not improve\n",
      "Epoch 20/200\n",
      "344/344 [==============================] - 361s 1s/step - loss: 60.9064 - triplet_acc: 0.9970 - val_loss: 83.3458 - val_triplet_acc: 0.9836\n",
      "\n",
      "Epoch 00020: val_triplet_acc did not improve\n",
      "Epoch 21/200\n",
      "344/344 [==============================] - 361s 1s/step - loss: 66.5631 - triplet_acc: 0.9969 - val_loss: 77.1074 - val_triplet_acc: 0.9850\n",
      "\n",
      "Epoch 00021: val_triplet_acc did not improve\n",
      "Epoch 22/200\n",
      "344/344 [==============================] - 361s 1s/step - loss: 57.9214 - triplet_acc: 0.9973 - val_loss: 78.8919 - val_triplet_acc: 0.9867\n",
      "\n",
      "Epoch 00022: val_triplet_acc did not improve\n",
      "Epoch 23/200\n",
      "344/344 [==============================] - 361s 1s/step - loss: 61.5471 - triplet_acc: 0.9969 - val_loss: 73.2359 - val_triplet_acc: 0.9880\n",
      "\n",
      "Epoch 00023: val_triplet_acc improved from 0.98751 to 0.98801, saving model to Weights/weights.023-0.9880.hdf5\n",
      "Epoch 24/200\n",
      "344/344 [==============================] - 361s 1s/step - loss: 63.8391 - triplet_acc: 0.9971 - val_loss: 79.2579 - val_triplet_acc: 0.9874\n",
      "\n",
      "Epoch 00024: val_triplet_acc did not improve\n",
      "Epoch 25/200\n",
      "344/344 [==============================] - 361s 1s/step - loss: 58.1980 - triplet_acc: 0.9973 - val_loss: 73.0286 - val_triplet_acc: 0.9870\n",
      "\n",
      "Epoch 00025: val_triplet_acc did not improve\n",
      "Epoch 26/200\n",
      "344/344 [==============================] - 361s 1s/step - loss: 61.7366 - triplet_acc: 0.9971 - val_loss: 69.8035 - val_triplet_acc: 0.9867\n",
      "\n",
      "Epoch 00026: val_triplet_acc did not improve\n",
      "Epoch 27/200\n",
      "344/344 [==============================] - 361s 1s/step - loss: 60.1182 - triplet_acc: 0.9973 - val_loss: 79.5578 - val_triplet_acc: 0.9869\n",
      "\n",
      "Epoch 00027: val_triplet_acc did not improve\n",
      "Epoch 28/200\n",
      "344/344 [==============================] - 361s 1s/step - loss: 59.7910 - triplet_acc: 0.9973 - val_loss: 73.7559 - val_triplet_acc: 0.9885\n",
      "\n",
      "Epoch 00028: val_triplet_acc improved from 0.98801 to 0.98847, saving model to Weights/weights.028-0.9885.hdf5\n",
      "Epoch 29/200\n",
      "344/344 [==============================] - 361s 1s/step - loss: 62.3970 - triplet_acc: 0.9972 - val_loss: 74.0941 - val_triplet_acc: 0.9867\n",
      "\n",
      "Epoch 00029: val_triplet_acc did not improve\n",
      "Epoch 30/200\n",
      "344/344 [==============================] - 362s 1s/step - loss: 58.4569 - triplet_acc: 0.9973 - val_loss: 70.3570 - val_triplet_acc: 0.9875\n",
      "\n",
      "Epoch 00030: val_triplet_acc did not improve\n",
      "Epoch 31/200\n",
      "344/344 [==============================] - 361s 1s/step - loss: 57.7921 - triplet_acc: 0.9975 - val_loss: 71.7821 - val_triplet_acc: 0.9850\n",
      "\n",
      "Epoch 00031: val_triplet_acc did not improve\n",
      "Epoch 32/200\n",
      "344/344 [==============================] - 361s 1s/step - loss: 60.2796 - triplet_acc: 0.9975 - val_loss: 76.3722 - val_triplet_acc: 0.9885\n",
      "\n",
      "Epoch 00032: val_triplet_acc improved from 0.98847 to 0.98852, saving model to Weights/weights.032-0.9885.hdf5\n",
      "Epoch 33/200\n",
      "344/344 [==============================] - 361s 1s/step - loss: 58.8664 - triplet_acc: 0.9976 - val_loss: 75.5768 - val_triplet_acc: 0.9880\n",
      "\n",
      "Epoch 00033: val_triplet_acc did not improve\n",
      "Epoch 34/200\n",
      "344/344 [==============================] - 361s 1s/step - loss: 55.9668 - triplet_acc: 0.9976 - val_loss: 76.2091 - val_triplet_acc: 0.9887\n",
      "\n",
      "Epoch 00034: val_triplet_acc improved from 0.98852 to 0.98872, saving model to Weights/weights.034-0.9887.hdf5\n",
      "Epoch 35/200\n",
      "344/344 [==============================] - 361s 1s/step - loss: 60.7392 - triplet_acc: 0.9977 - val_loss: 83.9033 - val_triplet_acc: 0.9875\n",
      "\n",
      "Epoch 00035: val_triplet_acc did not improve\n",
      "Epoch 36/200\n",
      "344/344 [==============================] - 361s 1s/step - loss: 56.3279 - triplet_acc: 0.9980 - val_loss: 81.3317 - val_triplet_acc: 0.9859\n",
      "\n",
      "Epoch 00036: val_triplet_acc did not improve\n",
      "Epoch 37/200\n",
      "344/344 [==============================] - 361s 1s/step - loss: 58.9480 - triplet_acc: 0.9979 - val_loss: 76.2195 - val_triplet_acc: 0.9883\n",
      "\n",
      "Epoch 00037: val_triplet_acc did not improve\n",
      "Epoch 38/200\n",
      "344/344 [==============================] - 361s 1s/step - loss: 60.4318 - triplet_acc: 0.9976 - val_loss: 76.9462 - val_triplet_acc: 0.9867\n",
      "\n",
      "Epoch 00038: val_triplet_acc did not improve\n",
      "Epoch 39/200\n",
      "344/344 [==============================] - 361s 1s/step - loss: 56.4877 - triplet_acc: 0.9979 - val_loss: 70.3826 - val_triplet_acc: 0.9849\n",
      "\n",
      "Epoch 00039: val_triplet_acc did not improve\n",
      "Epoch 40/200\n",
      "344/344 [==============================] - 361s 1s/step - loss: 59.8648 - triplet_acc: 0.9979 - val_loss: 79.1267 - val_triplet_acc: 0.9884\n",
      "\n",
      "Epoch 00040: val_triplet_acc did not improve\n",
      "Epoch 41/200\n",
      "344/344 [==============================] - 361s 1s/step - loss: 58.1886 - triplet_acc: 0.9980 - val_loss: 79.6831 - val_triplet_acc: 0.9888\n",
      "\n",
      "Epoch 00041: val_triplet_acc improved from 0.98872 to 0.98882, saving model to Weights/weights.041-0.9888.hdf5\n",
      "Epoch 42/200\n",
      "344/344 [==============================] - 361s 1s/step - loss: 57.8728 - triplet_acc: 0.9978 - val_loss: 78.8714 - val_triplet_acc: 0.9883\n",
      "\n",
      "Epoch 00042: val_triplet_acc did not improve\n",
      "Epoch 43/200\n",
      "344/344 [==============================] - 361s 1s/step - loss: 56.1380 - triplet_acc: 0.9979 - val_loss: 79.4006 - val_triplet_acc: 0.9874\n",
      "\n",
      "Epoch 00043: val_triplet_acc did not improve\n",
      "Epoch 44/200\n",
      "344/344 [==============================] - 361s 1s/step - loss: 57.2109 - triplet_acc: 0.9980 - val_loss: 76.3606 - val_triplet_acc: 0.9883\n",
      "\n",
      "Epoch 00044: val_triplet_acc did not improve\n",
      "Epoch 45/200\n",
      "344/344 [==============================] - 361s 1s/step - loss: 54.5003 - triplet_acc: 0.9980 - val_loss: 73.5180 - val_triplet_acc: 0.9874\n",
      "\n",
      "Epoch 00045: val_triplet_acc did not improve\n",
      "Epoch 46/200\n",
      "344/344 [==============================] - 361s 1s/step - loss: 57.9084 - triplet_acc: 0.9979 - val_loss: 83.1881 - val_triplet_acc: 0.9876\n",
      "\n",
      "Epoch 00046: val_triplet_acc did not improve\n",
      "Epoch 47/200\n",
      "344/344 [==============================] - 361s 1s/step - loss: 61.3956 - triplet_acc: 0.9980 - val_loss: 78.3552 - val_triplet_acc: 0.9893\n",
      "\n",
      "Epoch 00047: val_triplet_acc improved from 0.98882 to 0.98928, saving model to Weights/weights.047-0.9893.hdf5\n",
      "Epoch 48/200\n",
      "344/344 [==============================] - 361s 1s/step - loss: 56.0476 - triplet_acc: 0.9979 - val_loss: 76.8681 - val_triplet_acc: 0.9875\n",
      "\n",
      "Epoch 00048: val_triplet_acc did not improve\n",
      "Epoch 49/200\n",
      "344/344 [==============================] - 361s 1s/step - loss: 55.7891 - triplet_acc: 0.9983 - val_loss: 81.9739 - val_triplet_acc: 0.9877\n",
      "\n",
      "Epoch 00049: val_triplet_acc did not improve\n",
      "Epoch 50/200\n",
      "344/344 [==============================] - 361s 1s/step - loss: 58.9880 - triplet_acc: 0.9981 - val_loss: 75.8883 - val_triplet_acc: 0.9893\n",
      "\n",
      "Epoch 00050: val_triplet_acc improved from 0.98928 to 0.98933, saving model to Weights/weights.050-0.9893.hdf5\n",
      "Epoch 51/200\n",
      "344/344 [==============================] - 361s 1s/step - loss: 54.1317 - triplet_acc: 0.9981 - val_loss: 81.4428 - val_triplet_acc: 0.9880\n",
      "\n",
      "Epoch 00051: val_triplet_acc did not improve\n",
      "Epoch 52/200\n",
      "344/344 [==============================] - 361s 1s/step - loss: 57.5123 - triplet_acc: 0.9983 - val_loss: 77.2818 - val_triplet_acc: 0.9899\n",
      "\n",
      "Epoch 00052: val_triplet_acc improved from 0.98933 to 0.98993, saving model to Weights/weights.052-0.9899.hdf5\n",
      "Epoch 53/200\n",
      "344/344 [==============================] - 362s 1s/step - loss: 56.4723 - triplet_acc: 0.9981 - val_loss: 79.7275 - val_triplet_acc: 0.9886\n",
      "\n",
      "Epoch 00053: val_triplet_acc did not improve\n",
      "Epoch 54/200\n",
      "344/344 [==============================] - 363s 1s/step - loss: 56.4731 - triplet_acc: 0.9983 - val_loss: 83.4283 - val_triplet_acc: 0.9892\n",
      "\n",
      "Epoch 00054: val_triplet_acc did not improve\n",
      "Epoch 55/200\n",
      "344/344 [==============================] - 362s 1s/step - loss: 61.4427 - triplet_acc: 0.9978 - val_loss: 73.3668 - val_triplet_acc: 0.9891\n",
      "\n",
      "Epoch 00055: val_triplet_acc did not improve\n",
      "Epoch 56/200\n",
      "344/344 [==============================] - 362s 1s/step - loss: 55.9814 - triplet_acc: 0.9981 - val_loss: 70.9662 - val_triplet_acc: 0.9894\n",
      "\n",
      "Epoch 00056: val_triplet_acc did not improve\n",
      "Epoch 57/200\n",
      "344/344 [==============================] - 363s 1s/step - loss: 55.6726 - triplet_acc: 0.9983 - val_loss: 82.0591 - val_triplet_acc: 0.9886\n",
      "\n",
      "Epoch 00057: val_triplet_acc did not improve\n",
      "Epoch 58/200\n",
      "344/344 [==============================] - 361s 1s/step - loss: 52.4996 - triplet_acc: 0.9985 - val_loss: 73.5008 - val_triplet_acc: 0.9879\n",
      "\n",
      "Epoch 00058: val_triplet_acc did not improve\n",
      "Epoch 59/200\n",
      "344/344 [==============================] - 362s 1s/step - loss: 55.5392 - triplet_acc: 0.9982 - val_loss: 83.5193 - val_triplet_acc: 0.9887\n",
      "\n",
      "Epoch 00059: val_triplet_acc did not improve\n",
      "Epoch 60/200\n",
      "344/344 [==============================] - 363s 1s/step - loss: 56.7854 - triplet_acc: 0.9983 - val_loss: 85.4583 - val_triplet_acc: 0.9879\n",
      "\n",
      "Epoch 00060: val_triplet_acc did not improve\n",
      "Epoch 61/200\n",
      "344/344 [==============================] - 362s 1s/step - loss: 52.9217 - triplet_acc: 0.9983 - val_loss: 82.5068 - val_triplet_acc: 0.9887\n",
      "\n",
      "Epoch 00061: val_triplet_acc did not improve\n",
      "Epoch 62/200\n",
      "344/344 [==============================] - 363s 1s/step - loss: 52.9535 - triplet_acc: 0.9984 - val_loss: 85.7754 - val_triplet_acc: 0.9876\n",
      "\n",
      "Epoch 00062: val_triplet_acc did not improve\n",
      "Epoch 63/200\n",
      "344/344 [==============================] - 362s 1s/step - loss: 58.3314 - triplet_acc: 0.9981 - val_loss: 77.2746 - val_triplet_acc: 0.9874\n",
      "\n",
      "Epoch 00063: val_triplet_acc did not improve\n",
      "Epoch 64/200\n",
      " 23/344 [=>............................] - ETA: 5:04 - loss: 54.2994 - triplet_acc: 0.9985"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-96-aa7165c70102>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m                                    \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m                                    \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mA_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mP_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_test\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzeros_vect_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m                                    \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcallbacks_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m                                   )\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2222\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   2223\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2224\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   2225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2226\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1883\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1884\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1885\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2478\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2479\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1140\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1141\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1321\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1312\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[0;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1420\u001b[1;33m             status, run_metadata)\n\u001b[0m\u001b[0;32m   1421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1422\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#0.4 DROPOUT ALL\n",
    "\n",
    "classification_model, emb_model = global_model(SZ,channel_sz,drop1 = 0.4, drop2 = 0.4)\n",
    "classification_model.compile(optimizer = 'adam', loss = triplet_loss, metrics =[triplet_acc])\n",
    "\n",
    "batch_sz = 256\n",
    "\n",
    "classification_model.fit_generator(batch_generator(X_train, Y_train, batch_sz), \n",
    "                                   steps_per_epoch = 344,\n",
    "                                   epochs = 200,\n",
    "                                   verbose = 1,\n",
    "                                   validation_data = ([A_test, P_test, N_test], zeros_vect_test),\n",
    "                                   callbacks = callbacks_list\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double FC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EmbModel(input_shape, drop1=0, drop2=1/32):\n",
    "    \"\"\"\n",
    "    Define our shared embedding model\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of array of input images\n",
    "\n",
    "    \n",
    "    Returns:\n",
    "    model - Our model which transform an array of images into an array of embeddings\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Define the input placeholder as a tensor with shape input_shape.\n",
    "    X_input = Input(input_shape)\n",
    "    \n",
    "    #Padding\n",
    "    X = ZeroPadding2D((1,1))(X_input)\n",
    "    \n",
    "    #CONV\n",
    "    X = Conv2D(16,(3,3),strides =(1,1), name ='conv0', kernel_initializer='glorot_uniform') (X)\n",
    "    X = BatchNormalization(axis = 1, name = \"bn0\") (X)\n",
    "    X = Activation('relu', name='a0')(X)\n",
    "    \n",
    "    #MAXPOOL\n",
    "    X = MaxPooling2D((2,2), name='max_pool0')(X)\n",
    "    \n",
    "    X = Dropout(drop1)(X)\n",
    "    \n",
    "    #Padding\n",
    "    #X = ZeroPadding2D((1,1))(X)\n",
    "    \n",
    "     #CONV\n",
    "    X = Conv2D(32,(3,3),strides =(1,1), name ='conv1', kernel_initializer='glorot_uniform') (X)\n",
    "    X = BatchNormalization(axis = 1, name = \"bn1\") (X)\n",
    "    X = Activation('relu', name='a1')(X)\n",
    "    \n",
    "    #MAXPOOL\n",
    "    X = MaxPooling2D((2,2), name='max_pool1')(X)\n",
    "    \n",
    "    X = Dropout(drop1)(X)\n",
    "    \n",
    "    #Padding\n",
    "    #X = ZeroPadding2D((1,1))(X)\n",
    "    \n",
    "     #CONV\n",
    "    X = Conv2D(64,(3,3),strides =(1,1), name ='conv2', kernel_initializer='glorot_uniform') (X)\n",
    "    X = BatchNormalization(axis = 1, name = \"bn2\") (X)\n",
    "    X = Activation('relu', name='a2')(X)\n",
    "    \n",
    "     #MAXPOOL\n",
    "    X = MaxPooling2D((2,2), name='max_pool2')(X)\n",
    "    \n",
    "    X = Dropout(drop1)(X)\n",
    "    \n",
    "    #Padding\n",
    "    #X = ZeroPadding2D((1,1))(X)\n",
    "    \n",
    "     #CONV\n",
    "    X = Conv2D(128,(3,3),strides =(1,1), name ='conv3', kernel_initializer='glorot_uniform') (X)\n",
    "    X = BatchNormalization(axis = 1, name = \"bn3\") (X)\n",
    "    X = Activation('relu', name='a3')(X)\n",
    "    \n",
    "     #MAXPOOL\n",
    "    X = MaxPooling2D((2,2), name='max_pool3')(X)\n",
    "    \n",
    "    X = Dropout(drop1)(X)\n",
    "    \n",
    "    #FLATTEN X + FC\n",
    "    X = Flatten(name='f3')(X)\n",
    "    X = Dense (64, activation ='relu', name='fc4', kernel_initializer='glorot_uniform') (X)\n",
    "    X = Dropout(drop2)(X)\n",
    "    X = Dense (64, activation ='relu', name='fc5', kernel_initializer='glorot_uniform') (X)\n",
    "    X = Lambda(lambda  x: tf.nn.l2_normalize(x,axis=1))(X)\n",
    "    \n",
    "    ##Create model\n",
    "    model = Model(inputs = X_input, outputs = X, name='EmbModel')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We define our global model\n",
    "def global_model(size, channel_size=1, drop1=0, drop2=1/32):\n",
    "    input_size = (size, size, channel_sz)                     \n",
    "\n",
    "    A = Input(input_size)\n",
    "    P = Input(input_size)\n",
    "    N = Input(input_size)\n",
    "\n",
    "    emb_model= EmbModel(input_size)\n",
    "\n",
    "    out_A = emb_model(A)\n",
    "    out_P = emb_model(P)\n",
    "    out_N = emb_model(N)\n",
    "\n",
    "    y_pred = concatenate([out_A, out_P, out_N], axis =-1)\n",
    "\n",
    "    full_model = Model(inputs = [A, P, N], outputs = y_pred)\n",
    "    \n",
    "    return full_model, emb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit(model, b_size = 32, ep = 50):\n",
    "\n",
    "    A,P,N = triplets_list_train\n",
    "    A_test, P_test, N_test = triplets_list_test\n",
    "    zeros_vect = np.zeros(A[:,1,1].shape)\n",
    "    zeros_vect_test = np.zeros(A_test[:,1,1].shape) \n",
    "    class_model.fit(x = [A, P, N] , \n",
    "                             y = zeros_vect , \n",
    "                             batch_size = b_size, \n",
    "                             epochs = ep,\n",
    "                             validation_data = ([A_test, P_test, N_test], zeros_vect_test), \n",
    "                             shuffle = True)\n",
    "    return class_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create a checkpoint to save the best model and add an early stopping\n",
    "filepath = \"Weights/weights.{epoch:03d}-{val_triplet_acc:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_triplet_acc', verbose=1, save_best_only=True, mode='max')\n",
    "early_stop = EarlyStopping(monitor='val_triplet_acc', patience=20, verbose=1, mode='max')\n",
    "callbacks_list = [checkpoint, early_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1/32 DROPOUT ALL\n",
    "\n",
    "classification_model, emb_model = global_model(SZ,channel_sz,drop1 = 0.2, drop2 = 0.4)\n",
    "classification_model.compile(optimizer = 'adam', loss = triplet_loss, metrics =[triplet_acc])\n",
    "\n",
    "batch_sz = 256\n",
    "\n",
    "classification_model.fit_generator(batch_generator(X_train, Y_train, batch_sz), \n",
    "                                   steps_per_epoch = 344,\n",
    "                                   epochs = 200,\n",
    "                                   verbose = 1,\n",
    "                                   validation_data = ([A_test, P_test, N_test], zeros_vect_test),\n",
    "                                   callbacks = callbacks_list\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model.load_weights(\"Weights/weights.052-0.9899.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19770/19770 [==============================] - 48s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[52.97960219096221, 0.9870005057807154]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "classification_model.evaluate([A_test, P_test, N_test], zeros_vect_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison with Modified Hausdorff Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from demo_classification import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNNDistance(itemA, itemB):\n",
    "    itemA = itemA.reshape(1, itemA.shape[0], itemA.shape[1], 1)\n",
    "    itemB = itemB.reshape(1, itemB.shape[0], itemB.shape[1], 1)\n",
    "    itemA_emb = emb_model.predict_on_batch(itemA)\n",
    "    itemB_emb = emb_model.predict_on_batch(itemB)\n",
    "    dist = np.linalg.norm(itemA_emb - itemB_emb) #2-norm by default\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadImgAsArray(fn):\n",
    "\t# Load image file, return as array and resize\n",
    "    picture = mpimg.imread(fn)\n",
    "    image = resize(picture, (SZ,SZ), mode='constant')\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-shot classification with Modified Hausdorff Distance versus Siamese triplet loss Distance\n",
      " run 1 ModHausdorffDistance(error 45.0%)  -  Siamese_triplet_loss_Distance (error 20.0%)\n",
      " run 2 ModHausdorffDistance(error 35.0%)  -  Siamese_triplet_loss_Distance (error 30.0%)\n",
      " run 3 ModHausdorffDistance(error 40.0%)  -  Siamese_triplet_loss_Distance (error 15.0%)\n",
      " run 4 ModHausdorffDistance(error 25.0%)  -  Siamese_triplet_loss_Distance (error 10.0%)\n",
      " run 5 ModHausdorffDistance(error 30.0%)  -  Siamese_triplet_loss_Distance (error 0.0%)\n",
      " run 6 ModHausdorffDistance(error 15.0%)  -  Siamese_triplet_loss_Distance (error 0.0%)\n",
      " run 7 ModHausdorffDistance(error 60.0%)  -  Siamese_triplet_loss_Distance (error 10.0%)\n",
      " run 8 ModHausdorffDistance(error 35.0%)  -  Siamese_triplet_loss_Distance (error 30.0%)\n",
      " run 9 ModHausdorffDistance(error 40.0%)  -  Siamese_triplet_loss_Distance (error 15.0%)\n",
      " run 10 ModHausdorffDistance(error 55.0%)  -  Siamese_triplet_loss_Distance (error 40.0%)\n",
      " run 11 ModHausdorffDistance(error 15.0%)  -  Siamese_triplet_loss_Distance (error 5.0%)\n",
      " run 12 ModHausdorffDistance(error 70.0%)  -  Siamese_triplet_loss_Distance (error 15.0%)\n",
      " run 13 ModHausdorffDistance(error 65.0%)  -  Siamese_triplet_loss_Distance (error 30.0%)\n",
      " run 14 ModHausdorffDistance(error 35.0%)  -  Siamese_triplet_loss_Distance (error 10.0%)\n",
      " run 15 ModHausdorffDistance(error 15.0%)  -  Siamese_triplet_loss_Distance (error 10.0%)\n",
      " run 16 ModHausdorffDistance(error 25.0%)  -  Siamese_triplet_loss_Distance (error 10.0%)\n",
      " run 17 ModHausdorffDistance(error 30.0%)  -  Siamese_triplet_loss_Distance (error 10.0%)\n",
      " run 18 ModHausdorffDistance(error 40.0%)  -  Siamese_triplet_loss_Distance (error 20.0%)\n",
      " run 19 ModHausdorffDistance(error 70.0%)  -  Siamese_triplet_loss_Distance (error 15.0%)\n",
      " run 20 ModHausdorffDistance(error 30.0%)  -  Siamese_triplet_loss_Distance (error 20.0%)\n",
      " average error ModHausdorffDistance 38.75%  average error Siamese_triplet_loss_Distance 15.75%\n"
     ]
    }
   ],
   "source": [
    "print ('One-shot classification with Modified Hausdorff Distance versus Siamese triplet loss Distance')\n",
    "perror = np.zeros(nrun)\n",
    "perror_cnn =np.zeros(nrun)\n",
    "for r in range(1,nrun+1):\n",
    "\trs = str(r)\n",
    "\tif len(rs)==1:\n",
    "\t\trs = '0' + rs\t\t\n",
    "\tperror[r-1] = classification_run('one-shot-classification','/run'+rs, LoadImgAsPoints, ModHausdorffDistance, 'cost')\n",
    "\tperror_cnn[r-1] = classification_run('one-shot-classification','run'+rs, LoadImgAsArray, CNNDistance, 'cost')\n",
    "\tprint (\" run \" + str(r) + \" ModHausdorffDistance\" + \"(error \" + str(\tperror[r-1] ) + \"%)\"+ \"  -  Siamese_triplet_loss_Distance\" + \" (error \" + str(\tperror_cnn[r-1] ) + \"%)\")\t\t\n",
    "total = np.mean(perror)\n",
    "total_cnn = np.mean(perror_cnn)\n",
    "print (\" average error ModHausdorffDistance \" + str(total) + \"%\" + \"  average error Siamese_triplet_loss_Distance \" + str(total_cnn) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-shot classification with Modified Hausdorff Distance versus Siamese triplet loss Distance\n",
      " run 1 ModHausdorffDistance(error 45.0%)  -  Siamese_triplet_loss_Distance (error 20.0%)\n",
      " run 2 ModHausdorffDistance(error 35.0%)  -  Siamese_triplet_loss_Distance (error 10.0%)\n",
      " run 3 ModHausdorffDistance(error 40.0%)  -  Siamese_triplet_loss_Distance (error 10.0%)\n",
      " run 4 ModHausdorffDistance(error 25.0%)  -  Siamese_triplet_loss_Distance (error 10.0%)\n",
      " run 5 ModHausdorffDistance(error 30.0%)  -  Siamese_triplet_loss_Distance (error 10.0%)\n",
      " run 6 ModHausdorffDistance(error 15.0%)  -  Siamese_triplet_loss_Distance (error 5.0%)\n",
      " run 7 ModHausdorffDistance(error 60.0%)  -  Siamese_triplet_loss_Distance (error 15.0%)\n",
      " run 8 ModHausdorffDistance(error 35.0%)  -  Siamese_triplet_loss_Distance (error 15.0%)\n",
      " run 9 ModHausdorffDistance(error 40.0%)  -  Siamese_triplet_loss_Distance (error 25.0%)\n",
      " run 10 ModHausdorffDistance(error 55.0%)  -  Siamese_triplet_loss_Distance (error 35.0%)\n",
      " run 11 ModHausdorffDistance(error 15.0%)  -  Siamese_triplet_loss_Distance (error 5.0%)\n",
      " run 12 ModHausdorffDistance(error 70.0%)  -  Siamese_triplet_loss_Distance (error 5.0%)\n",
      " run 13 ModHausdorffDistance(error 65.0%)  -  Siamese_triplet_loss_Distance (error 15.0%)\n",
      " run 14 ModHausdorffDistance(error 35.0%)  -  Siamese_triplet_loss_Distance (error 10.0%)\n",
      " run 15 ModHausdorffDistance(error 15.0%)  -  Siamese_triplet_loss_Distance (error 5.0%)\n",
      " run 16 ModHausdorffDistance(error 25.0%)  -  Siamese_triplet_loss_Distance (error 0.0%)\n",
      " run 17 ModHausdorffDistance(error 30.0%)  -  Siamese_triplet_loss_Distance (error 25.0%)\n",
      " run 18 ModHausdorffDistance(error 40.0%)  -  Siamese_triplet_loss_Distance (error 15.0%)\n",
      " run 19 ModHausdorffDistance(error 70.0%)  -  Siamese_triplet_loss_Distance (error 15.0%)\n",
      " run 20 ModHausdorffDistance(error 30.0%)  -  Siamese_triplet_loss_Distance (error 25.0%)\n",
      " average error ModHausdorffDistance 38.75%  average error Siamese_triplet_loss_Distance 13.75%\n"
     ]
    }
   ],
   "source": [
    "##dropout 0.1 all early stop\n",
    "\n",
    "print ('One-shot classification with Modified Hausdorff Distance versus Siamese triplet loss Distance')\n",
    "perror = np.zeros(nrun)\n",
    "perror_cnn =np.zeros(nrun)\n",
    "for r in range(1,nrun+1):\n",
    "\trs = str(r)\n",
    "\tif len(rs)==1:\n",
    "\t\trs = '0' + rs\t\t\n",
    "\tperror[r-1] = classification_run('one-shot-classification','/run'+rs, LoadImgAsPoints, ModHausdorffDistance, 'cost')\n",
    "\tperror_cnn[r-1] = classification_run('one-shot-classification','run'+rs, LoadImgAsArray, CNNDistance, 'cost')\n",
    "\tprint (\" run \" + str(r) + \" ModHausdorffDistance\" + \"(error \" + str(\tperror[r-1] ) + \"%)\"+ \"  -  Siamese_triplet_loss_Distance\" + \" (error \" + str(\tperror_cnn[r-1] ) + \"%)\")\t\t\n",
    "total = np.mean(perror)\n",
    "total_cnn = np.mean(perror_cnn)\n",
    "print (\" average error ModHausdorffDistance \" + str(total) + \"%\" + \"  average error Siamese_triplet_loss_Distance \" + str(total_cnn) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-shot classification with Modified Hausdorff Distance versus Siamese triplet loss Distance\n",
      " run 1 ModHausdorffDistance(error 45.0%)  -  Siamese_triplet_loss_Distance (error 25.0%)\n",
      " run 2 ModHausdorffDistance(error 35.0%)  -  Siamese_triplet_loss_Distance (error 15.0%)\n",
      " run 3 ModHausdorffDistance(error 40.0%)  -  Siamese_triplet_loss_Distance (error 10.0%)\n",
      " run 4 ModHausdorffDistance(error 25.0%)  -  Siamese_triplet_loss_Distance (error 15.0%)\n",
      " run 5 ModHausdorffDistance(error 30.0%)  -  Siamese_triplet_loss_Distance (error 20.0%)\n",
      " run 6 ModHausdorffDistance(error 15.0%)  -  Siamese_triplet_loss_Distance (error 5.0%)\n",
      " run 7 ModHausdorffDistance(error 60.0%)  -  Siamese_triplet_loss_Distance (error 10.0%)\n",
      " run 8 ModHausdorffDistance(error 35.0%)  -  Siamese_triplet_loss_Distance (error 15.0%)\n",
      " run 9 ModHausdorffDistance(error 40.0%)  -  Siamese_triplet_loss_Distance (error 20.0%)\n",
      " run 10 ModHausdorffDistance(error 55.0%)  -  Siamese_triplet_loss_Distance (error 40.0%)\n",
      " run 11 ModHausdorffDistance(error 15.0%)  -  Siamese_triplet_loss_Distance (error 5.0%)\n",
      " run 12 ModHausdorffDistance(error 70.0%)  -  Siamese_triplet_loss_Distance (error 15.0%)\n",
      " run 13 ModHausdorffDistance(error 65.0%)  -  Siamese_triplet_loss_Distance (error 25.0%)\n",
      " run 14 ModHausdorffDistance(error 35.0%)  -  Siamese_triplet_loss_Distance (error 5.0%)\n",
      " run 15 ModHausdorffDistance(error 15.0%)  -  Siamese_triplet_loss_Distance (error 5.0%)\n",
      " run 16 ModHausdorffDistance(error 25.0%)  -  Siamese_triplet_loss_Distance (error 5.0%)\n",
      " run 17 ModHausdorffDistance(error 30.0%)  -  Siamese_triplet_loss_Distance (error 25.0%)\n",
      " run 18 ModHausdorffDistance(error 40.0%)  -  Siamese_triplet_loss_Distance (error 15.0%)\n",
      " run 19 ModHausdorffDistance(error 70.0%)  -  Siamese_triplet_loss_Distance (error 30.0%)\n",
      " run 20 ModHausdorffDistance(error 30.0%)  -  Siamese_triplet_loss_Distance (error 20.0%)\n",
      " average error ModHausdorffDistance 38.75%  average error Siamese_triplet_loss_Distance 16.25%\n"
     ]
    }
   ],
   "source": [
    "##dropout 0.1 best model\n",
    "\n",
    "print ('One-shot classification with Modified Hausdorff Distance versus Siamese triplet loss Distance')\n",
    "perror = np.zeros(nrun)\n",
    "perror_cnn =np.zeros(nrun)\n",
    "for r in range(1,nrun+1):\n",
    "\trs = str(r)\n",
    "\tif len(rs)==1:\n",
    "\t\trs = '0' + rs\t\t\n",
    "\tperror[r-1] = classification_run('one-shot-classification','/run'+rs, LoadImgAsPoints, ModHausdorffDistance, 'cost')\n",
    "\tperror_cnn[r-1] = classification_run('one-shot-classification','run'+rs, LoadImgAsArray, CNNDistance, 'cost')\n",
    "\tprint (\" run \" + str(r) + \" ModHausdorffDistance\" + \"(error \" + str(\tperror[r-1] ) + \"%)\"+ \"  -  Siamese_triplet_loss_Distance\" + \" (error \" + str(\tperror_cnn[r-1] ) + \"%)\")\t\t\n",
    "total = np.mean(perror)\n",
    "total_cnn = np.mean(perror_cnn)\n",
    "print (\" average error ModHausdorffDistance \" + str(total) + \"%\" + \"  average error Siamese_triplet_loss_Distance \" + str(total_cnn) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['images_evaluation/Glagolitic/character02/1116_09.png',\n",
       " 'images_evaluation/Glagolitic/character02/1116_03.png',\n",
       " 'images_evaluation/Glagolitic/character02/1116_15.png',\n",
       " 'images_evaluation/Glagolitic/character02/1116_08.png',\n",
       " 'images_evaluation/Glagolitic/character02/1116_17.png']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testPath = glob.glob(\"{}*/*/*.png\".format(PATH_TEST))\n",
    "testPath[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13180, 60, 60)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13180, 60, 60, 1)\n"
     ]
    }
   ],
   "source": [
    "test_array= test_array.reshape(test_array.shape[0],test_array.shape[1],test_array.shape[2],channel_sz)\n",
    "print(test_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13180,)\n"
     ]
    }
   ],
   "source": [
    "class_char_test=np.array([])\n",
    "for i in range(13180//20):\n",
    "    #As each character have 20 examples\n",
    "    class_char_test= np.concatenate((class_char_test, np.ones(20)*(i+1))) \n",
    "class_char_test = class_char_test.astype(int)\n",
    "print(class_char_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13180, 1)\n"
     ]
    }
   ],
   "source": [
    "class_char_test=class_char_test.reshape(class_char_test.shape[0],1)\n",
    "print(class_char_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, X_eval, Y_test, Y_eval = train_test_split(test_array, class_char_test, test_size=0.7, stratify= class_char_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets_list_eval = create_triplets(X_eval, Y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119938/119938 [==============================] - 145s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.8782454858518958"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_eval, P_eval, N_eval = triplets_list_eval\n",
    "zeros_vect_eval = np.zeros(A_eval[:,1,1].shape) \n",
    "\n",
    "classification_model.evaluate([A_eval, P_eval, N_eval], zeros_vect_eval, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
