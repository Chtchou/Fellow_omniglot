{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few-shot learning - Omniglot - Fellowship.ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fellowship.ai - Few-shot learning\n",
    "This is my personal project to the few-shot learning challenge from [Fellowship.ai](https://fellowship.ai/challenge/) with the following goal:\n",
    "> Omniglot, the “transpose” of MNIST, with 1623 character classes, each with 20 examples.  Build a few-shot classifier with a target of <35% error.\n",
    "\n",
    "#### Omniglot - Dataset\n",
    "*Dataset reference:* [Link](https://github.com/brendenlake/omniglot)\n",
    "> Lake, B. M., Salakhutdinov, R., and Tenenbaum, J. B. (2015). Human-level concept learning through probabilistic program induction. Science, 350(6266), 1332-1338.\n",
    "\n",
    "The Omniglot dataset is often considered as the transpose of the MNIST dataset. While the latter contains only 10 classes with a training set of 60000 examples, Omniglot contains an important number of classes (1623 different handwritten characters from 50 different alphabets) with only a low number of examples (20) for each, making it an ideal dataset for few-shot learning problems.\n",
    "\n",
    "#### Few-shot learning\n",
    "Whereas, lots of deep learning projects are based on a huge number of training examples to be trained, few-shot learning is  based only on a few one. This approach is much closer to the one experienced by humans. We are able to memorize and recognize objects we have never seen before from a few number of examples. Then for each new encounter with these types of object we can classify them in an accurate and easy way.\n",
    "\n",
    "\n",
    "#### Stategy\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/envs/goodai/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, concatenate, MaxPooling2D, Dropout, Lambda\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from PIL import Image                                                                                                                               \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import glob\n",
    "from skimage.transform import resize\n",
    "import h5py\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We indicate the PATH of the training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training images path\n",
    "PATH=\"images_background/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a look at the list of all alphabets contained in the training set, and their total number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of the different alphabets:\n",
      "\n",
      " ['Cyrillic' 'Syriac_(Estrangelo)' 'Gujarati' 'Latin' 'Burmese_(Myanmar)'\n",
      " 'Japanese_(katakana)' 'Mkhedruli_(Georgian)' 'Alphabet_of_the_Magi'\n",
      " 'Grantha' 'Braille' 'Early_Aramaic' 'Tifinagh' 'Asomtavruli_(Georgian)'\n",
      " 'Arcadian' 'Blackfoot_(Canadian_Aboriginal_Syllabics)' 'Greek'\n",
      " 'Japanese_(hiragana)' 'Malay_(Jawi_-_Arabic)'\n",
      " 'Ojibwe_(Canadian_Aboriginal_Syllabics)' 'Futurama' 'N_Ko' 'Bengali'\n",
      " 'Armenian' 'Tagalog' 'Korean' 'Anglo-Saxon_Futhorc' 'Sanskrit' 'Balinese'\n",
      " 'Inuktitut_(Canadian_Aboriginal_Syllabics)' 'Hebrew']\n",
      "\n",
      "Number of different alphabets: 30\n"
     ]
    }
   ],
   "source": [
    "alph_type = np.array(os.listdir(PATH)) #Give the different types of alphabet in our data\n",
    "print(\"List of the different alphabets:\\n\\n {}\".format(alph_type))\n",
    "print(\"\\nNumber of different alphabets: {}\".format(len(alph_type)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then check the number of character for each alphabets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of chracters corresponding to each alphabet is : \n",
      "{'Cyrillic': 33, 'Syriac_(Estrangelo)': 23, 'Gujarati': 48, 'Latin': 26, 'Burmese_(Myanmar)': 34, 'Japanese_(katakana)': 47, 'Mkhedruli_(Georgian)': 41, 'Alphabet_of_the_Magi': 20, 'Grantha': 43, 'Braille': 26, 'Early_Aramaic': 22, 'Tifinagh': 55, 'Asomtavruli_(Georgian)': 40, 'Arcadian': 26, 'Blackfoot_(Canadian_Aboriginal_Syllabics)': 14, 'Greek': 24, 'Japanese_(hiragana)': 52, 'Malay_(Jawi_-_Arabic)': 40, 'Ojibwe_(Canadian_Aboriginal_Syllabics)': 14, 'Futurama': 26, 'N_Ko': 33, 'Bengali': 46, 'Armenian': 41, 'Tagalog': 17, 'Korean': 40, 'Anglo-Saxon_Futhorc': 29, 'Sanskrit': 42, 'Balinese': 24, 'Inuktitut_(Canadian_Aboriginal_Syllabics)': 16, 'Hebrew': 22}\n",
      "\n",
      "The maximum number of different character for one alphabet is 55\n",
      "The minimum number of different character for one alphabet is 14\n",
      "The total number of different character is 964\n"
     ]
    }
   ],
   "source": [
    "alph_num_char ={}\n",
    "for alphabet in alph_type:\n",
    "    alph_num_char[alphabet]= len(os.listdir(f'{PATH}{alphabet}'))\n",
    "print(\"The number of chracters corresponding to each alphabet is : \\n{}\".format(alph_num_char))\n",
    "\n",
    "num_of_char = alph_num_char.values()\n",
    "print('\\nThe maximum number of different character for one alphabet is {}'.format(max(num_of_char)))\n",
    "print('The minimum number of different character for one alphabet is {}'.format(min(num_of_char)))\n",
    "total_char = sum(num_of_char)\n",
    "print('The total number of different character is {}'.format(total_char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum number of different examples for one character is 20\n",
      "The minimum number of different examples for one character is 20\n",
      "The total number of different pictures is 19280\n"
     ]
    }
   ],
   "source": [
    "alph_num_char_ex={}\n",
    "class_num=0\n",
    "for alphabet in alph_type:\n",
    "    char_list=os.listdir(f'{PATH}{alphabet}')\n",
    "    for char in char_list:\n",
    "        alph_num_char_ex[(alphabet,char)]= len(os.listdir(f'{PATH}{alphabet}/{char}'))\n",
    "\n",
    "num_of_example = alph_num_char_ex.values()\n",
    "print('The maximum number of different examples for one character is {}'.format(max(num_of_example)))\n",
    "print('The minimum number of different examples for one character is {}'.format(min(num_of_example)))\n",
    "total_example = sum(num_of_example) \n",
    "print('The total number of different pictures is {}'.format(total_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that each character have 20 examples(pictures)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add label for each character\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each picture in our dataset we give a corresponding label(an integer) which allow us to determine the corresponding character. Here an integer is sufficient as we are not really interested in knowing from which alphabet an image is coming from and as we don't have need to know the character name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19280,)\n"
     ]
    }
   ],
   "source": [
    "class_char=np.array([])\n",
    "for i in range(total_char):\n",
    "    #As each character have 20 examples\n",
    "    class_char= np.concatenate((class_char, np.ones(20)*(i+1))) \n",
    "class_char = class_char.astype(int)\n",
    "print(class_char.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reshape, our data to have the number of channel including (here is 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19280, 1)\n"
     ]
    }
   ],
   "source": [
    "class_char=class_char.reshape(class_char.shape[0],1)\n",
    "print(class_char.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert images to datafile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first retrieve the path for each picture in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['images_background/Cyrillic/character02/0219_04.png',\n",
       " 'images_background/Cyrillic/character02/0219_11.png',\n",
       " 'images_background/Cyrillic/character02/0219_18.png',\n",
       " 'images_background/Cyrillic/character02/0219_19.png',\n",
       " 'images_background/Cyrillic/character02/0219_17.png']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagePath = glob.glob(\"{}*/*/*.png\".format(PATH))\n",
    "imagePath[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print some random images of the dataset, convert them to arrays and resize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We decide to resize our images to the following "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imagePath = glob.glob(\"{}*/*/*.png\".format(PATH))\n",
    "\n",
    "SZ=60 #Dimension of the output image expected\n",
    "\n",
    "#Dimensions of the grill of sample pictures\n",
    "columns = 4\n",
    "rows = 5\n",
    "\n",
    "fig=plt.figure(figsize=(5, 5))\n",
    "list_example = np.random.randint(total_example, size = columns*rows)\n",
    "pos=0\n",
    "for i in list_example:\n",
    "    pos+=1\n",
    "    img = mpimg.imread(imagePath[i])\n",
    "    img = resize(img, (SZ,SZ), mode='reflect')\n",
    "    fig.add_subplot(rows, columns, pos)\n",
    "    plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All images seems clear and well centered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert all images into arrays and resize them to the 60x60 format. We concatenate all arrays into the variable im_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19280, 60, 60)\n"
     ]
    }
   ],
   "source": [
    "#Transform in array and resize all 19280 images \n",
    "im_array = np.array([resize(mpimg.imread(i), (SZ,SZ), mode='reflect') for i in imagePath] )\n",
    "print (im_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reshape the image to take into account the number of channel to pass them in our CNN, which is 1 here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19280, 60, 60, 1)\n"
     ]
    }
   ],
   "source": [
    "channel_sz = 1 #number of channel\n",
    "im_array= im_array.reshape(im_array.shape[0],im_array.shape[1],im_array.shape[2],channel_sz)\n",
    "print(im_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train/ validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the traning set into Train and Validation sets of pictures (Train : 70%, Validation : 3O%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, Y_train, Y_valid = train_test_split(im_array, class_char, test_size=0.3, stratify= class_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13496, 60, 60, 1), (5784, 60, 60, 1), (13496, 1), (5784, 1))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_valid.shape, Y_train.shape, Y_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triplet loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our loss function is determined by the following triplet loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(y_true, y_pred, alpha = 0.2):\n",
    "    \"\"\"\n",
    "    Implementation of the triplet loss\n",
    "    \n",
    "    Arguments:\n",
    "    y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.\n",
    "    y_pred -- python list containing three objects:\n",
    "            anchor -- the encodings for the anchor images, of shape (None, 64)\n",
    "            positive -- the encodings for the positive images, of shape (None, 64)\n",
    "            negative -- the encodings for the negative images, of shape (None, 64)\n",
    "    \n",
    "    Returns:\n",
    "    loss -- real number, value of the loss\n",
    "    \"\"\"\n",
    "    \n",
    "    anchor, positive, negative = y_pred[:,0:64], y_pred[:,64:128], y_pred[:,128:256]\n",
    "    \n",
    "    #Compute the (encoding) distance between the anchor and the positive, you will need to sum over axis=-1\n",
    "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), axis = -1)\n",
    "    #Compute the (encoding) distance between the anchor and the negative, you will need to sum over axis=-1\n",
    "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), axis = -1)\n",
    "    #Subtract the two previous distances and add alpha.\n",
    "    basic_loss = tf.add(tf.subtract(pos_dist, neg_dist), alpha)\n",
    "    #Take the maximum of basic_loss and 0.0. Sum over the training examples.\n",
    "    loss = tf.reduce_sum(tf.maximum(basic_loss, 0))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Triplet Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_triplets(X, Y, num=1):\n",
    "    \"\"\"\n",
    "    Create a list of valid triplets\n",
    "    \n",
    "    Arguments:\n",
    "    X -- array of images\n",
    "    Y -- array of classes corresponding to each image\n",
    "    num -- number of negative images for each valid anchor and positive images - must be positive\n",
    "           if num = 0, all possible valid couples are created\n",
    "            For example : for one valid (A,P) couple we can select 'num' random N images. \n",
    "                          Thus 'num' triplets are created for this (A,P) couple\n",
    "\n",
    "    \n",
    "    Returns:\n",
    "    (A,P,N) -- python tuple containing 3 arrays : \n",
    "            A -- the array for the anchor images, of shape (None, 64)\n",
    "            P -- the array for the positive images, of shape (None, 64)\n",
    "            N -- the array for the negative images, of shape (None, 64)\n",
    "    \"\"\"\n",
    "\n",
    "    Y = Y.reshape(Y.shape[0],)\n",
    "    A = []\n",
    "    P = []\n",
    "    N = []\n",
    "    \n",
    "    #We loop over all possible valid (A,P)\n",
    "    for i in range(X.shape[0]):  \n",
    "        list_pos = X[Y==Y[i]]\n",
    "        for j in list_pos:\n",
    "            #We provide a number 'num' of triplets for each valid (A,P)\n",
    "            if num >=1:\n",
    "                for k in range(num):\n",
    "                    rand_num = np.random.randint(X.shape[0])\n",
    "                    if np.array_equal(X[i],j) == False:\n",
    "                        A.append(X[i])\n",
    "                        P.append(j)\n",
    "                        while np.array_equal(Y[rand_num], Y[i]):\n",
    "                            rand_num = np.random.randint(X.shape[0])\n",
    "                        N.append(X[rand_num])\n",
    "            if num == 0:\n",
    "                for k in range(X.shape[0]):\n",
    "                    if np.array_equal(X[i],j) == False:\n",
    "                        if np.array_equal(Y[i],Y[k]) == False:\n",
    "                            A.append(X[i])\n",
    "                            P.append(j)\n",
    "                            N.append(X[k])\n",
    "    \n",
    "    A = np.array(A)\n",
    "    P = np.array(P)\n",
    "    N = np.array(N)\n",
    "    \n",
    "    return (A, P, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28920, 60, 60, 1)\n",
      "(28920, 60, 60, 1)\n",
      "(28920, 60, 60, 1)\n"
     ]
    }
   ],
   "source": [
    "#We create one triplet for each of the possible (A,P) couple in our validation set \n",
    "triplets_list_valid = create_triplets(X_valid,Y_valid)\n",
    "for i in range(len(triplets_list_valid)):\n",
    "    print(triplets_list_valid[i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a number of 28920 examples for the validation. Thus 1 negative image per (A,P) couple is enough for our evaluation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EmbModel(input_shape):\n",
    "    \"\"\"\n",
    "    Define our shared embedding model\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of array of input images\n",
    "\n",
    "    \n",
    "    Returns:\n",
    "    model - Our model which transform an array of images into an array of embeddings\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Define the input placeholder as a tensor with shape input_shape.\n",
    "    X_input = Input(input_shape)\n",
    "    \n",
    "    #Padding\n",
    "    X = ZeroPadding2D((1,1))(X_input)\n",
    "    \n",
    "    #CONV\n",
    "    X = Conv2D(8,(3,3),strides =(1,1), name ='conv0', kernel_initializer='glorot_uniform') (X)\n",
    "    X = BatchNormalization(axis = 1, name = \"bn0\") (X)\n",
    "    X = Activation('relu', name='a0')(X)\n",
    "    \n",
    "    #MAXPOOL\n",
    "    X = MaxPooling2D((2,2), name='max_pool0')(X)\n",
    "    \n",
    "    X = Dropout(0.15)(X)\n",
    "    \n",
    "    #Padding\n",
    "    #X = ZeroPadding2D((1,1))(X)\n",
    "    \n",
    "     #CONV\n",
    "    X = Conv2D(16,(3,3),strides =(1,1), name ='conv1', kernel_initializer='glorot_uniform') (X)\n",
    "    X = BatchNormalization(axis = 1, name = \"bn1\") (X)\n",
    "    X = Activation('relu', name='a1')(X)\n",
    "    \n",
    "    #MAXPOOL\n",
    "    X = MaxPooling2D((2,2), name='max_pool1')(X)\n",
    "    \n",
    "    X = Dropout(0.15)(X)\n",
    "    \n",
    "    #Padding\n",
    "    #X = ZeroPadding2D((1,1))(X)\n",
    "    \n",
    "     #CONV\n",
    "    X = Conv2D(32,(3,3),strides =(1,1), name ='conv2', kernel_initializer='glorot_uniform') (X)\n",
    "    X = BatchNormalization(axis = 1, name = \"bn2\") (X)\n",
    "    X = Activation('relu', name='a2')(X)\n",
    "    \n",
    "     #MAXPOOL\n",
    "    X = MaxPooling2D((2,2), name='max_pool2')(X)\n",
    "    \n",
    "    X = Dropout(0.15)(X)\n",
    "    \n",
    "    #Padding\n",
    "    #X = ZeroPadding2D((1,1))(X)\n",
    "    \n",
    "     #CONV\n",
    "    X = Conv2D(64,(3,3),strides =(1,1), name ='conv3', kernel_initializer='glorot_uniform') (X)\n",
    "    X = BatchNormalization(axis = 1, name = \"bn3\") (X)\n",
    "    X = Activation('relu', name='a3')(X)\n",
    "    \n",
    "     #MAXPOOL\n",
    "    X = MaxPooling2D((2,2), name='max_pool3')(X)\n",
    "    \n",
    "    X = Dropout(0.2)(X)\n",
    "    \n",
    "    #FLATTEN X + FC\n",
    "    X = Flatten(name='f3')(X)\n",
    "    X = Dense (256, activation ='relu', name='fc4', kernel_initializer='glorot_uniform') (X)\n",
    "    X = Dropout(0.3)(X)\n",
    "    X = Dense (64, activation ='relu', name='fc5', kernel_initializer='glorot_uniform') (X)\n",
    "    X = Lambda(lambda  x: tf.nn.l2_normalize(x,axis=1))(X)\n",
    "    \n",
    "    ##Create model\n",
    "    model = Model(inputs = X_input, outputs = X, name='EmbModel')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We define our global model\n",
    "input_size = (SZ, SZ, channel_sz)                     \n",
    "\n",
    "A = Input(input_size)\n",
    "P = Input(input_size)\n",
    "N = Input(input_size)\n",
    "\n",
    "emb_model= EmbModel(input_size)\n",
    "\n",
    "out_A = emb_model(A)\n",
    "out_P = emb_model(P)\n",
    "out_N = emb_model(N)\n",
    "\n",
    "y_pred = concatenate([out_A, out_P, out_N], axis =-1)\n",
    "\n",
    "classification_model = Model(inputs = [A, P, N], outputs = y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 60, 60, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 60, 60, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 60, 60, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "EmbModel (Model)                (None, 64)           107040      input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 192)          0           EmbModel[1][0]                   \n",
      "                                                                 EmbModel[2][0]                   \n",
      "                                                                 EmbModel[3][0]                   \n",
      "==================================================================================================\n",
      "Total params: 107,040\n",
      "Trainable params: 106,832\n",
      "Non-trainable params: 208\n",
      "__________________________________________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 60, 60, 1)         0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 62, 62, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv0 (Conv2D)               (None, 60, 60, 8)         80        \n",
      "_________________________________________________________________\n",
      "bn0 (BatchNormalization)     (None, 60, 60, 8)         240       \n",
      "_________________________________________________________________\n",
      "a0 (Activation)              (None, 60, 60, 8)         0         \n",
      "_________________________________________________________________\n",
      "max_pool0 (MaxPooling2D)     (None, 30, 30, 8)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 30, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 28, 28, 16)        1168      \n",
      "_________________________________________________________________\n",
      "bn1 (BatchNormalization)     (None, 28, 28, 16)        112       \n",
      "_________________________________________________________________\n",
      "a1 (Activation)              (None, 28, 28, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pool1 (MaxPooling2D)     (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 12, 12, 32)        4640      \n",
      "_________________________________________________________________\n",
      "bn2 (BatchNormalization)     (None, 12, 12, 32)        48        \n",
      "_________________________________________________________________\n",
      "a2 (Activation)              (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pool2 (MaxPooling2D)     (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv3 (Conv2D)               (None, 4, 4, 64)          18496     \n",
      "_________________________________________________________________\n",
      "bn3 (BatchNormalization)     (None, 4, 4, 64)          16        \n",
      "_________________________________________________________________\n",
      "a3 (Activation)              (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pool3 (MaxPooling2D)     (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "f3 (Flatten)                 (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "fc4 (Dense)                  (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "fc5 (Dense)                  (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 64)                0         \n",
      "=================================================================\n",
      "Total params: 107,040\n",
      "Trainable params: 106,832\n",
      "Non-trainable params: 208\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classification_model.summary()\n",
    "emb_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X,Y, bs=32):\n",
    "    \"\"\"\n",
    "    Create a mini-batch generator\n",
    "    \n",
    "    Arguments:\n",
    "    X -- array of images\n",
    "    Y -- array of classes corresponding to each image\n",
    "    bs -- size of the minibatch\n",
    "\n",
    "    \n",
    "    Returns:\n",
    "    [A_batch, P_batch, N_batch], y_dummie) -- a mini-batch of size bs\n",
    "    \"\"\"\n",
    "    \n",
    "    Y = Y.reshape(Y.shape[0],)\n",
    "    while True:\n",
    "        #0. Initialize Anchor,Postive, Negative\n",
    "        A_batch = []\n",
    "        P_batch = []\n",
    "        N_batch = []\n",
    "        for i in range(bs):      \n",
    "            #1.Choose a random Anchor Image\n",
    "            rand_A_num = np.random.randint(X.shape[0])\n",
    "            A_batch.append(X[rand_A_num])\n",
    "            \n",
    "            #2.Choose a random Positive Image\n",
    "            list_pos = X[Y==Y[rand_A_num]]                            #List of positive images\n",
    "            rand_P_num = np.random.randint(len(list_pos))\n",
    "            P_batch.append(list_pos[rand_P_num])\n",
    "            \n",
    "            #3.Choose a random Negative Image\n",
    "            rand_N_num = np.random.randint(X.shape[0])\n",
    "            while np.array_equal(Y[rand_N_num], Y[rand_A_num]):\n",
    "                rand_A_num = np.random.randint(X.shape[0])\n",
    "            N_batch.append(X[rand_N_num])\n",
    "            \n",
    "        A_batch = np.array(A_batch)\n",
    "        P_batch = np.array(P_batch)\n",
    "        N_batch = np.array(N_batch)\n",
    "        \n",
    "        y_dummie = np.zeros((len(A_batch),))\n",
    "        \n",
    "        yield ([A_batch, P_batch, N_batch], y_dummie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real shit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We compile our model with the custom made triplet_loss\n",
    "classification_model.compile(optimizer = 'adam', loss = triplet_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint and early stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create a checkpoint to save the best model and add an early stopping\n",
    "filepath = \"Weights/weights.{epoch:02d}-{val_loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "callbacks_list = [checkpoint, early_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "344/344 [==============================] - 167s 486ms/step - loss: 169.8978 - val_loss: 42.9049\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f09a00a1668>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_valid, P_valid, N_valid = triplets_list_valid\n",
    "zeros_vect_valid = np.zeros(A_valid[:,1,1].shape) \n",
    "\n",
    "batch_sz = 2048\n",
    "\n",
    "classification_model.fit_generator(batch_generator(X_train, Y_train, batch_sz), \n",
    "                                   steps_per_epoch = 344,\n",
    "                                   epochs = 1,\n",
    "                                   verbose = 1,\n",
    "                                   validation_data = ([A_valid, P_valid, N_valid], zeros_vect_valid),\n",
    "                                  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "55040/55040 [==============================] - 1925s 35ms/step - loss: 0.3610 - val_loss: 0.1258\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.12578, saving model to Weights/weights.01-0.1258.hdf5\n",
      "Epoch 2/200\n",
      "55040/55040 [==============================] - 1766s 32ms/step - loss: 0.1620 - val_loss: 0.1036\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.12578 to 0.10357, saving model to Weights/weights.02-0.1036.hdf5\n",
      "Epoch 3/200\n",
      "55040/55040 [==============================] - 1739s 32ms/step - loss: 0.1255 - val_loss: 0.0835\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.10357 to 0.08346, saving model to Weights/weights.03-0.0835.hdf5\n",
      "Epoch 4/200\n",
      "55040/55040 [==============================] - 1739s 32ms/step - loss: 0.1077 - val_loss: 0.0801\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.08346 to 0.08014, saving model to Weights/weights.04-0.0801.hdf5\n",
      "Epoch 5/200\n",
      "55040/55040 [==============================] - 1741s 32ms/step - loss: 0.0963 - val_loss: 0.0676\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.08014 to 0.06759, saving model to Weights/weights.05-0.0676.hdf5\n",
      "Epoch 6/200\n",
      "55040/55040 [==============================] - 1739s 32ms/step - loss: 0.0884 - val_loss: 0.0655\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.06759 to 0.06550, saving model to Weights/weights.06-0.0655.hdf5\n",
      "Epoch 7/200\n",
      "55040/55040 [==============================] - 1737s 32ms/step - loss: 0.0838 - val_loss: 0.0629\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.06550 to 0.06286, saving model to Weights/weights.07-0.0629.hdf5\n",
      "Epoch 8/200\n",
      "55040/55040 [==============================] - 1740s 32ms/step - loss: 0.0785 - val_loss: 0.0602\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.06286 to 0.06018, saving model to Weights/weights.08-0.0602.hdf5\n",
      "Epoch 9/200\n",
      "55040/55040 [==============================] - 1739s 32ms/step - loss: 0.0744 - val_loss: 0.0631\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/200\n",
      "55040/55040 [==============================] - 1741s 32ms/step - loss: 0.0703 - val_loss: 0.0624\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/200\n",
      "20065/55040 [=========>....................] - ETA: 18:20 - loss: 0.0694"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-df154718d530>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                                    \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                    \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mA_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mP_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_valid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeros_vect_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                                    \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m                                   )\n",
      "\u001b[0;32m~/anaconda3/envs/goodai/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/goodai/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2222\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2223\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2224\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2226\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/goodai/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1883\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/goodai/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/goodai/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/goodai/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/goodai/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/goodai/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/goodai/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/goodai/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_sz = 32\n",
    "\n",
    "classification_model.fit_generator(batch_generator(X_train, Y_train, batch_sz), \n",
    "                                   steps_per_epoch = 55040,\n",
    "                                   epochs = 200,\n",
    "                                   verbose = 1,\n",
    "                                   validation_data = ([A_valid, P_valid, N_valid], zeros_vect_valid),\n",
    "                                   callbacks = callbacks_list\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model.load_weights(\"Weights/weights.08-0.0602.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison with Modified Hausdorff Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from demo_classification import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNNDistance(itemA, itemB):\n",
    "    itemA = itemA.reshape(1, itemA.shape[0], itemA.shape[1], 1)\n",
    "    itemB = itemB.reshape(1, itemB.shape[0], itemB.shape[1], 1)\n",
    "    itemA_emb = emb_model.predict_on_batch(itemA)\n",
    "    itemB_emb = emb_model.predict_on_batch(itemB)\n",
    "    dist = np.linalg.norm(itemA_emb - itemB_emb)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadImgAsArray(fn):\n",
    "\t# Load image file, return as array and resize\n",
    "    picture = mpimg.imread(fn)\n",
    "    image = resize(picture, (SZ,SZ), mode='constant')\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-shot classification with Modified Hausdorff Distance versus Siamese triplet loss Distance\n",
      " run 1 ModHausdorffDistance(error 45.0%)  -  Siamese_triplet_loss_Distance (error 25.0%)\n",
      " run 2 ModHausdorffDistance(error 35.0%)  -  Siamese_triplet_loss_Distance (error 35.0%)\n",
      " run 3 ModHausdorffDistance(error 40.0%)  -  Siamese_triplet_loss_Distance (error 5.0%)\n",
      " run 4 ModHausdorffDistance(error 25.0%)  -  Siamese_triplet_loss_Distance (error 15.0%)\n",
      " run 5 ModHausdorffDistance(error 30.0%)  -  Siamese_triplet_loss_Distance (error 30.0%)\n",
      " run 6 ModHausdorffDistance(error 15.0%)  -  Siamese_triplet_loss_Distance (error 0.0%)\n",
      " run 7 ModHausdorffDistance(error 60.0%)  -  Siamese_triplet_loss_Distance (error 10.0%)\n",
      " run 8 ModHausdorffDistance(error 35.0%)  -  Siamese_triplet_loss_Distance (error 15.0%)\n",
      " run 9 ModHausdorffDistance(error 40.0%)  -  Siamese_triplet_loss_Distance (error 25.0%)\n",
      " run 10 ModHausdorffDistance(error 55.0%)  -  Siamese_triplet_loss_Distance (error 40.0%)\n",
      " run 11 ModHausdorffDistance(error 15.0%)  -  Siamese_triplet_loss_Distance (error 10.0%)\n",
      " run 12 ModHausdorffDistance(error 70.0%)  -  Siamese_triplet_loss_Distance (error 20.0%)\n",
      " run 13 ModHausdorffDistance(error 65.0%)  -  Siamese_triplet_loss_Distance (error 20.0%)\n",
      " run 14 ModHausdorffDistance(error 35.0%)  -  Siamese_triplet_loss_Distance (error 25.0%)\n",
      " run 15 ModHausdorffDistance(error 15.0%)  -  Siamese_triplet_loss_Distance (error 10.0%)\n",
      " run 16 ModHausdorffDistance(error 25.0%)  -  Siamese_triplet_loss_Distance (error 10.0%)\n",
      " run 17 ModHausdorffDistance(error 30.0%)  -  Siamese_triplet_loss_Distance (error 15.0%)\n",
      " run 18 ModHausdorffDistance(error 40.0%)  -  Siamese_triplet_loss_Distance (error 25.0%)\n",
      " run 19 ModHausdorffDistance(error 70.0%)  -  Siamese_triplet_loss_Distance (error 30.0%)\n",
      " run 20 ModHausdorffDistance(error 30.0%)  -  Siamese_triplet_loss_Distance (error 25.0%)\n",
      " average error ModHausdorffDistance 38.75%  average error Siamese_triplet_loss_Distance 19.5%\n"
     ]
    }
   ],
   "source": [
    "print ('One-shot classification with Modified Hausdorff Distance versus Siamese triplet loss Distance')\n",
    "perror = np.zeros(nrun)\n",
    "perror_cnn =np.zeros(nrun)\n",
    "for r in range(1,nrun+1):\n",
    "\trs = str(r)\n",
    "\tif len(rs)==1:\n",
    "\t\trs = '0' + rs\t\t\n",
    "\tperror[r-1] = classification_run('one-shot-classification','/run'+rs, LoadImgAsPoints, ModHausdorffDistance, 'cost')\n",
    "\tperror_cnn[r-1] = classification_run('one-shot-classification','run'+rs, LoadImgAsArray, CNNDistance, 'cost')\n",
    "\tprint (\" run \" + str(r) + \" ModHausdorffDistance\" + \"(error \" + str(\tperror[r-1] ) + \"%)\"+ \"  -  Siamese_triplet_loss_Distance\" + \" (error \" + str(\tperror_cnn[r-1] ) + \"%)\")\t\t\n",
    "total = np.mean(perror)\n",
    "total_cnn = np.mean(perror_cnn)\n",
    "print (\" average error ModHausdorffDistance \" + str(total) + \"%\" + \"  average error Siamese_triplet_loss_Distance \" + str(total_cnn) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TEST = \"images_evaluation/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['images_evaluation/Glagolitic/character02/1116_09.png',\n",
       " 'images_evaluation/Glagolitic/character02/1116_03.png',\n",
       " 'images_evaluation/Glagolitic/character02/1116_15.png',\n",
       " 'images_evaluation/Glagolitic/character02/1116_08.png',\n",
       " 'images_evaluation/Glagolitic/character02/1116_17.png']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testPath = glob.glob(\"{}*/*/*.png\".format(PATH_TEST))\n",
    "testPath[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13180, 60, 60)\n"
     ]
    }
   ],
   "source": [
    "test_array = np.array([resize(mpimg.imread(i), (SZ,SZ), mode='reflect') for i in testPath] )\n",
    "print (test_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13180, 60, 60, 1)\n"
     ]
    }
   ],
   "source": [
    "test_array= test_array.reshape(test_array.shape[0],test_array.shape[1],test_array.shape[2],channel_sz)\n",
    "print(test_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13180,)\n"
     ]
    }
   ],
   "source": [
    "class_char_test=np.array([])\n",
    "for i in range(13180//20):\n",
    "    #As each character have 20 examples\n",
    "    class_char_test= np.concatenate((class_char_test, np.ones(20)*(i+1))) \n",
    "class_char_test = class_char_test.astype(int)\n",
    "print(class_char_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13180, 1)\n"
     ]
    }
   ],
   "source": [
    "class_char_test=class_char_test.reshape(class_char_test.shape[0],1)\n",
    "print(class_char_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, X_eval, Y_test, Y_eval = train_test_split(test_array, class_char_test, test_size=0.5, stratify= class_char_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59310, 60, 60, 1)\n",
      "(59310, 60, 60, 1)\n",
      "(59310, 60, 60, 1)\n"
     ]
    }
   ],
   "source": [
    "triplets_list_test = create_triplets(X_test, Y_test)\n",
    "for i in range(len(triplets_list_test)):\n",
    "    print(triplets_list_test[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "del triplets_list_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__name__ 57\n",
      "__doc__ 113\n",
      "__package__ 16\n",
      "__loader__ 16\n",
      "__spec__ 16\n",
      "__builtin__ 80\n",
      "__builtins__ 80\n",
      "_ih 528\n",
      "_oh 368\n",
      "_dh 72\n",
      "In 528\n",
      "Out 368\n",
      "get_ipython 64\n",
      "exit 56\n",
      "quit 56\n",
      "_ 28\n",
      "__ 28\n",
      "___ 28\n",
      "_i 141\n",
      "_ii 139\n",
      "_iii 116\n",
      "_i1 818\n",
      "os 80\n",
      "tf 80\n",
      "np 80\n",
      "Model 888\n",
      "load_model 136\n",
      "Input 136\n",
      "Dense 2000\n",
      "Activation 1464\n",
      "ZeroPadding2D 1464\n",
      "BatchNormalization 2000\n",
      "Flatten 1464\n",
      "Conv2D 2000\n",
      "concatenate 136\n",
      "MaxPooling2D 1464\n",
      "Dropout 1464\n",
      "Lambda 1464\n",
      "Adam 1184\n",
      "ModelCheckpoint 1056\n",
      "EarlyStopping 1056\n",
      "train_test_split 136\n",
      "Image 80\n",
      "plt 80\n",
      "mpimg 80\n",
      "glob 80\n",
      "resize 136\n",
      "h5py 80\n",
      "_i2 96\n",
      "PATH 67\n",
      "_i3 272\n",
      "alph_type 5016\n",
      "_i4 597\n",
      "alph_num_char 1184\n",
      "alphabet 115\n",
      "num_of_char 48\n",
      "total_char 28\n",
      "_i5 628\n",
      "alph_num_char_ex 36968\n",
      "class_num 24\n",
      "char_list 264\n",
      "char 60\n",
      "num_of_example 48\n",
      "total_example 28\n",
      "_i6 265\n",
      "class_char 112\n",
      "i 57\n",
      "_i7 125\n",
      "_i8 113\n",
      "imagePath 168856\n",
      "_8 104\n",
      "_i9 98\n",
      "_i10 507\n",
      "SZ 28\n",
      "columns 28\n",
      "rows 28\n",
      "fig 56\n",
      "list_example 256\n",
      "pos 28\n",
      "img 112\n",
      "_i11 212\n",
      "im_array 144\n",
      "_i12 197\n",
      "channel_sz 28\n",
      "_i13 161\n",
      "X_valid 166579344\n",
      "Y_train 108080\n",
      "Y_valid 46384\n",
      "_i14 107\n",
      "_14 80\n",
      "_i15 1378\n",
      "triplet_loss 136\n",
      "_i16 1962\n",
      "create_triplets 136\n",
      "_i17 269\n",
      "_i18 2280\n",
      "EmbModel 136\n",
      "_i19 418\n",
      "input_size 72\n",
      "A 56\n",
      "P 56\n",
      "N 56\n",
      "emb_model 56\n",
      "out_A 56\n",
      "out_P 56\n",
      "out_N 56\n",
      "y_pred 56\n",
      "classification_model 56\n",
      "_i20 99\n",
      "_i21 1467\n",
      "batch_generator 136\n",
      "_i22 174\n",
      "_i23 116\n",
      "_i24 116\n",
      "_i25 82\n",
      "copy 80\n",
      "imread 136\n",
      "cdist 136\n",
      "nrun 28\n",
      "fname_label 65\n",
      "classification_run 136\n",
      "ModHausdorffDistance 136\n",
      "LoadImgAsPoints 136\n",
      "_i26 372\n",
      "CNNDistance 136\n",
      "_i27 221\n",
      "LoadImgAsArray 136\n",
      "_i28 878\n",
      "perror 256\n",
      "perror_cnn 256\n",
      "r 28\n",
      "rs 51\n",
      "total 32\n",
      "total_cnn 32\n",
      "_i29 81\n",
      "PATH_TEST 67\n",
      "_i30 116\n",
      "testPath 118488\n",
      "_30 104\n",
      "_i31 72\n",
      "_i32 166\n",
      "test_array 144\n",
      "_i33 175\n",
      "_i34 294\n",
      "class_char_test 112\n",
      "_i35 145\n",
      "_i36 194\n",
      "_i37 169\n",
      "X_test 189792144\n",
      "X_eval 189792144\n",
      "Y_test 52832\n",
      "Y_eval 52832\n",
      "_i38 181\n",
      "_i39 93\n",
      "getsizeof 72\n",
      "_39 28\n",
      "_i40 105\n",
      "_i41 93\n",
      "_41 28\n",
      "_i42 60\n",
      "_i43 181\n",
      "_i44 93\n",
      "_44 28\n",
      "_i45 95\n",
      "_45 28\n",
      "_i46 119\n",
      "_i47 120\n",
      "_i48 70\n",
      "_48 28\n",
      "_i49 130\n",
      "a 28\n",
      "_i50 116\n",
      "_i51 139\n",
      "_i52 141\n",
      "var 53\n",
      "obj 53\n",
      "_i53 162\n",
      "temp_var 48\n"
     ]
    }
   ],
   "source": [
    "from sys import getsizeof\n",
    "\n",
    "temp_var = locals().items()\n",
    "\n",
    "for var, obj in temp_var:\n",
    "    print (var, getsizeof(obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getsizeof(test_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59310/59310 [==============================] - 14s 243us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2777600132822086"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_test, P_test, N_test = triplets_list_test\n",
    "zeros_vect_test = np.zeros(A_test[:,1,1].shape) \n",
    "\n",
    "classification_model.evaluate([A_test, P_test, N_test], zeros_vect_test, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
